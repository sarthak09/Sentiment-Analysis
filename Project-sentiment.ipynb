{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xT7MKZuMRaCg"
   },
   "source": [
    "# Sentiment Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: I have used following steps in the project\n",
    "1. Import libs: This is I just imported libruary used in the project\n",
    "2. Loading the dataset: This step is just data loading IMDB dataset step only with max vocabulary size and splitting it in train and test set which include dependent and independent variable\n",
    "3. EDA: This step is used for exploring the dataset. Here I have crossed various dateset like vaocab size, unique values of the target variable and also used visualization techniques, seen the average sequence length of the dataset, Standard deviation as well. These step helped me to decide max sequence padding length for padding  \n",
    "4. word embedding: In this step I have prepared word to index dictionary, index to word dictionary and words list. Also In this I have made a function so to translate the sequence back to words using above dictionary.\n",
    "5. Model building: Here I have made various model starting with normal neural network model with sequence length of 20 but this model was overfitting, Then I made the same model with sequence length of 300, Then I have done grid search on this model and made final model with improved parameter with sequence length of 300 but here also it was overfitting. Then I have made an LSTM model without pretrained embeddings as this model was underperforming from before models thus I decided to make an LSTM model with pretrained GLOVE embedding, Here I have used GLOVE 100 which encodes each words with 100 embedding dimension.\n",
    "\n",
    "NOTE I have shown predictions, performance evaluation, performance graph and AUC curve for LSTM with pretrained glove and for normal NN with imporved hyperparameter\n",
    "6. Output: Here based on above model I have shown the output of each layer and the shape of each layer for 2 different model i.e Final normal model after grid search and LSTM with GLove vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Importing libs\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Flatten\n",
    "from keras.layers import GRU, LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import  EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score,precision_score,recall_score,f1_score\n",
    "from sklearn import metrics\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import imdb\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score, GridSearchCV\n",
    "import os\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import GRU, LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wq4RCyyPSYRp"
   },
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize variables\n",
    "MAX_VOCAB=10000\n",
    "EMBEDDING_DIM=50\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NGCtiXUhSWss"
   },
   "outputs": [],
   "source": [
    "#Loading of preprocessed movie review dataset with vocal size 10000 \n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=MAX_VOCAB) # vocab_size is no.of words to consider from the dataset, ordering based on frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length =  25000\n",
      "Train label length =  25000\n",
      "Test data length =  25000\n",
      "Test label length =  25000\n",
      "\n",
      "\n",
      "Train data type =  <class 'numpy.ndarray'>\n",
      "Train label type =  <class 'numpy.ndarray'>\n",
      "Test data type =  <class 'numpy.ndarray'>\n",
      "Test label type =  <class 'numpy.ndarray'>\n",
      "\n",
      "\n",
      "Train data shape =  (25000,)\n",
      "Train label shape =  (25000,)\n",
      "Test data shape =  (25000,)\n",
      "Test label shape =  (25000,)\n"
     ]
    }
   ],
   "source": [
    "#Printing important information about the loaded dataset into train and test\n",
    "print(\"Train data length = \",len(x_train))\n",
    "print(\"Train label length = \",len(y_train))\n",
    "print(\"Test data length = \",len(x_test))\n",
    "print(\"Test label length = \",len(y_test))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Train data type = \",type(x_train))\n",
    "print(\"Train label type = \",type(y_train))\n",
    "print(\"Test data type = \",type(x_test))\n",
    "print(\"Test label type = \",type(y_test))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Train data shape = \",x_train.shape)\n",
    "print(\"Train label shape = \",y_train.shape)\n",
    "print(\"Test data shape = \",x_test.shape)\n",
    "print(\"Test label shape = \",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "#Seeing value in train set to see if everything is correct\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 40, 49, 85, 84, 1040, 146, 6, 783, 254, 4386, 337, 5, 13, 447, 14, 500, 10, 10, 14, 500, 517, 1076, 357, 21, 1684, 72, 45, 290, 12, 17, 515, 17, 25, 380, 129, 3305, 4, 2191, 26, 253, 5, 2, 36, 80, 4357, 25, 2, 129, 330, 505, 8, 2, 146, 24, 3988, 14, 500, 9, 82, 2, 5, 9, 1293, 224, 10, 10, 8, 401, 14, 1361, 879, 13, 28, 8, 401, 61, 1642, 2925, 44, 1373, 21, 591, 353, 14, 500, 4092, 30, 290, 12, 10, 10, 65, 790, 790, 206, 158, 300, 45, 15, 52, 2, 158, 692, 2, 158, 856, 158]\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "#Seeing value in test set to see if everything is correct\n",
    "print(x_test[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value of a word index \n",
      "9999\n",
      "Maximum length num words of review in train \n",
      "2494\n"
     ]
    }
   ],
   "source": [
    "#print max value( max vocabulary value not vocalbulary size) in train set and printing max seq that any review can have\n",
    "print(\"Maximum value of a word index \")\n",
    "print(max([max(sequence) for sequence in x_train]))\n",
    "print(\"Maximum length num words of review in train \")\n",
    "print(max([len(sequence) for sequence in x_train]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value of a word index \n",
      "9999\n",
      "Maximum length num words of review in train \n",
      "2315\n"
     ]
    }
   ],
   "source": [
    "#print max value( max vocabulary value not vocalbulary size) in test set and printing max seq that any review can have\n",
    "print(\"Maximum value of a word index \")\n",
    "print(max([max(sequence) for sequence in x_test]))\n",
    "print(\"Maximum length num words of review in train \")\n",
    "print(max([len(sequence) for sequence in x_test]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total classes in test = [0 1]\n",
      "Total classes in train =  [0 1]\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "#print number of classes in target variable for both train and test set\n",
    "print(\"Total classes in test =\",np.unique(y_test))\n",
    "print(\"Total classes in train = \",np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in train =  9998\n",
      "Total number of words in test = 9951\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "#To see number of words in both test and train set\n",
    "print(\"Total number of words in train = \",len(np.unique(np.hstack(x_train))))\n",
    "print(\"Total number of words in test =\",len(np.unique(np.hstack(x_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize step "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review length: \n",
      "Mean 238.71 words (176.493674)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize step\n",
    "# Summarize review length for train\n",
    "#This cell shows how long each review is and based on that it give mean and std of lenght of each sequence\n",
    "#For train set\n",
    "print(\"Review length: \")\n",
    "result = [len(x) for x in x_train]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# plot review length\n",
    "plt.boxplot(result)\n",
    "plt.show()\n",
    "\n",
    "#Here you can see mean is 238.71 and std 176.49\n",
    "#This helps us to decide best sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean 230.80 words (169.161087)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize step\n",
    "# Summarize review length for train\n",
    "#This cell shows how long each review is and based on that it give mean and std of lenght of each sequence\n",
    "#For train set\n",
    "result=[len(x) for x in x_test]\n",
    "print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "plt.boxplot(result)\n",
    "plt.show()\n",
    "\n",
    "#Here you can see mean is 230.80 and std 169.16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#Load original word to index dictionary\n",
    "voca = imdb.get_word_index()\n",
    "print(type(voca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting words into list this gives index to word mapping where each index has a word in that position\n",
    "#This list has all the words that we want\n",
    "#This has all the words in the dictionary\n",
    "#words=['<PAD>','<START>','<UNK>','<UNUSED>']\n",
    "count=0\n",
    "words=[]\n",
    "for w in sorted(voca, key=voca.get):\n",
    "    if count<=MAX_VOCAB-1:\n",
    "        words.append(w)\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "and\n",
      "a\n",
      "of\n",
      "to\n",
      "beaver\n",
      "approved\n",
      "\n",
      "\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "#Cross check steps\n",
    "#print words in that index\n",
    "print(words[0])\n",
    "print(words[1])\n",
    "print(words[2])\n",
    "print(words[3])\n",
    "print(words[4])\n",
    "print(words[-2])\n",
    "print(words[-1])\n",
    "print(\"\\n\")\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This steps has all the word to index original mapping but only till vocab size i.e 10000 and they are stored in variable word2idx\n",
    "#This step also stores index values i.e numbers from 1 to 10000\n",
    "count=0\n",
    "word2idx={}\n",
    "index=[]\n",
    "for i , j in voca.items():\n",
    "    if j<=MAX_VOCAB:  #Here it is leass or equal because index starts from 1 as 0 is a special character \n",
    "        word2idx[i]=j+3   #Here it is storing dict based on above vocab size condition but with plus 3 as from 0 to 3 are storing special character\n",
    "        index.append(j+3)\n",
    "\n",
    "word2idx[\"<PAD>\"] = 0 # This is special character for padding\n",
    "word2idx[\"<START>\"] = 1 #Start od sequence\n",
    "word2idx[\"<UNK>\"] = 2  # unknown\n",
    "word2idx[\"<UNUSED>\"] = 3\n",
    "    \n",
    "#Note first 4 are used for special chacter and it starts from 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is reverse mapping of word to index\n",
    "#This is dictionary off index to word where key = value and value = word\n",
    "\n",
    "idx2word={j:i for i,j in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10003\n",
      "approved\n"
     ]
    }
   ],
   "source": [
    "#Cross check step where 1st statement i.e word2idx returns value of 2nd dict i.e idx2word and 2nd statement idx2word return 1st dict i.e word2idx value\n",
    "print(word2idx[\"approved\"])\n",
    "print(idx2word[10003])\n",
    "\n",
    "#value of second should match index of 1st\n",
    "\n",
    "#Note here len is 10003 because first few are speacial character in these dictionay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10004\n"
     ]
    }
   ],
   "source": [
    "#Cross check steps which tell if we are doing things right\n",
    "print(len(word2idx)) #This tells us that our max value is 10004 where first 4 are special words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross check step to see index value of \"and\" word\n",
    "word2idx[\"and\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word corresponds to 6 index =  a\n",
      "We got same word with value =  6\n",
      "\n",
      "\n",
      "Word corresponds to 10003 index =  approved\n",
      "We got same word with value =  10003\n",
      "\n",
      "\n",
      "Word corresponds to 3 index =  the\n",
      "We got same word with value =  4\n",
      "\n",
      "\n",
      "Word corresponds to 5 index =  and\n",
      "We got same word with value =  5\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Cross check step which compare data from word(list) and word2idx(dict)\n",
    "#Here I am seeing when i provide i to specific condition word2idx should return some value and on next step if I supply that word it should return the same index\n",
    "for i , j in voca.items():\n",
    "    if j==0:\n",
    "        print(\"Word corresponds to 0 index = \",i) #This should be 0 as 0 corresponds to special word\n",
    "        print(\"We got same word with value = \",word2idx[i])\n",
    "        print(\"\\n\")\n",
    "    if j==1:\n",
    "        print(\"Word corresponds to 3 index = \",i) #Most common word\n",
    "        print(\"We got same word with value = \",word2idx[i])\n",
    "        print(\"\\n\")\n",
    "    if j==2:\n",
    "        print(\"Word corresponds to 5 index = \",i) #2nd most common word\n",
    "        print(\"We got same word with value = \",word2idx[i])\n",
    "        print(\"\\n\")\n",
    "    if j==3:\n",
    "        print(\"Word corresponds to 6 index = \",i) #3rd most common word\n",
    "        print(\"We got same word with value = \",word2idx[i])\n",
    "        print(\"\\n\")\n",
    "    if j==10000:\n",
    "        print(\"Word corresponds to 10003 index = \",i) #Last word\n",
    "        print(\"We got same word with value = \",word2idx[i])\n",
    "        print(\"\\n\")\n",
    "        \n",
    "#Note first 4 values are special words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'woods': 1411,\n",
       " 'hanging': 2348,\n",
       " 'woody': 2292,\n",
       " 'arranged': 6749,\n",
       " 'bringing': 2341,\n",
       " 'wooden': 1639,\n",
       " 'errors': 4013,\n",
       " 'dialogs': 3233,\n",
       " 'kids': 362,\n",
       " 'uplifting': 5037,\n",
       " 'controversy': 7096,\n",
       " 'projection': 9881,\n",
       " 'stern': 7183,\n",
       " 'morally': 5624,\n",
       " 'wang': 5286,\n",
       " 'want': 181,\n",
       " 'travel': 2106,\n",
       " 'barbra': 6705,\n",
       " 'dinosaurs': 3933,\n",
       " 'wrong': 355,\n",
       " 'subplots': 4763,\n",
       " 'welcomed': 9095,\n",
       " 'butcher': 6706,\n",
       " 'fit': 1183,\n",
       " 'screaming': 1930,\n",
       " 'fix': 4290,\n",
       " 'hurting': 9657,\n",
       " 'effects': 302,\n",
       " 'barton': 8778,\n",
       " 'ingrid': 6195,\n",
       " 'adapt': 7919,\n",
       " 'disturbed': 4014,\n",
       " 'purpose': 1288,\n",
       " 'olds': 6795,\n",
       " 'needed': 887,\n",
       " 'master': 1300,\n",
       " 'positively': 5402,\n",
       " 'zatoichi': 9839,\n",
       " 'feeling': 547,\n",
       " 'affairs': 5625,\n",
       " 'wholesome': 7802,\n",
       " 'cinematic': 1360,\n",
       " 'tech': 4991,\n",
       " 'saying': 660,\n",
       " 'padded': 8425,\n",
       " 'tempted': 5748,\n",
       " 'plate': 7479,\n",
       " 'altogether': 3902,\n",
       " 'lds': 8779,\n",
       " 'nicely': 1780,\n",
       " 'mummy': 4591,\n",
       " 'lots': 775,\n",
       " 'lotr': 9262,\n",
       " 'irs': 9882,\n",
       " 'ira': 6890,\n",
       " 'discipline': 7696,\n",
       " 'nature': 876,\n",
       " 'superficial': 3903,\n",
       " 'extent': 2826,\n",
       " 'bothers': 8780,\n",
       " 'much': 76,\n",
       " 'spit': 6262,\n",
       " 'arkin': 7367,\n",
       " 'doubts': 4992,\n",
       " 'spin': 3087,\n",
       " 'hong': 2579,\n",
       " 'academic': 9097,\n",
       " 'corporate': 4452,\n",
       " 'hal': 4314,\n",
       " 'ham': 4895,\n",
       " 'had': 69,\n",
       " 'has': 47,\n",
       " 'hat': 2404,\n",
       " 'crowd': 2293,\n",
       " 'crown': 6630,\n",
       " 'bottom': 1324,\n",
       " 'starring': 1184,\n",
       " 'marshall': 5343,\n",
       " 'honeymoon': 9098,\n",
       " 'shoots': 3234,\n",
       " 'fabric': 8292,\n",
       " 'raped': 3540,\n",
       " 'rapes': 8929,\n",
       " \"else's\": 5927,\n",
       " 'martian': 7256,\n",
       " 'passenger': 9460,\n",
       " 'disgrace': 6044,\n",
       " 'barrymore': 5123,\n",
       " 'cambodia': 9883,\n",
       " 'palma': 5561,\n",
       " 'explosions': 3978,\n",
       " 'loren': 8062,\n",
       " 'shootout': 6707,\n",
       " 'chain': 3629,\n",
       " 'whoever': 2500,\n",
       " 'chair': 3039,\n",
       " 'ballet': 4520,\n",
       " 'macho': 5626,\n",
       " 'jerk': 3399,\n",
       " 'gloomy': 7368,\n",
       " 'locked': 2897,\n",
       " 'exact': 2591,\n",
       " 'minute': 786,\n",
       " 'celebrated': 6541,\n",
       " 'unintentionally': 3386,\n",
       " 'climbs': 9099,\n",
       " 'honour': 9658,\n",
       " 'address': 5517,\n",
       " 'benson': 9461,\n",
       " 'cusack': 3843,\n",
       " 'opposed': 3630,\n",
       " 'following': 1045,\n",
       " 'convincingly': 4521,\n",
       " 'surfing': 4255,\n",
       " 'jim': 1240,\n",
       " 'quarter': 6542,\n",
       " 'entering': 6263,\n",
       " 'seriously': 615,\n",
       " 'raunchy': 7820,\n",
       " 'grandma': 7097,\n",
       " 'modest': 6111,\n",
       " 'spoken': 2850,\n",
       " 'concert': 3252,\n",
       " 'lingering': 8427,\n",
       " 'snatch': 9100,\n",
       " \"miyazaki's\": 9263,\n",
       " 'wandering': 4635,\n",
       " 'turned': 679,\n",
       " 'turner': 3768,\n",
       " 'opposite': 1961,\n",
       " 'grateful': 6089,\n",
       " 'inconsistent': 5562,\n",
       " 'imagined': 3792,\n",
       " 'enthralled': 9884,\n",
       " 'tsui': 8599,\n",
       " 'menacing': 3541,\n",
       " 'convoluted': 3660,\n",
       " 'millionaire': 5288,\n",
       " 'west': 1263,\n",
       " 'motives': 4204,\n",
       " 'photos': 4453,\n",
       " 'unlikeable': 6112,\n",
       " 'technology': 2131,\n",
       " 'otto': 6988,\n",
       " 'visually': 2009,\n",
       " 'being': 112,\n",
       " 'grounded': 9355,\n",
       " 'excuses': 6878,\n",
       " 'sums': 5289,\n",
       " 'traffic': 5847,\n",
       " 'sensational': 8428,\n",
       " 'satisfactory': 8781,\n",
       " 'substance': 2327,\n",
       " 'thailand': 7697,\n",
       " 'hopkins': 5677,\n",
       " 'sealed': 9659,\n",
       " 'brazilian': 8063,\n",
       " 'bubble': 6708,\n",
       " 'wits': 9264,\n",
       " 'societal': 9660,\n",
       " 'with': 19,\n",
       " 'abused': 5233,\n",
       " 'rage': 3980,\n",
       " 'tripe': 5174,\n",
       " 'dirty': 1641,\n",
       " 'watches': 3631,\n",
       " 'watcher': 7579,\n",
       " 'ensuing': 9885,\n",
       " 'watched': 296,\n",
       " 'cream': 5234,\n",
       " 'waving': 9265,\n",
       " 'natalie': 6264,\n",
       " 'tricks': 3351,\n",
       " 'caused': 2164,\n",
       " 'beware': 5518,\n",
       " 'causes': 2906,\n",
       " 'nora': 8782,\n",
       " 'norm': 5797,\n",
       " 'sans': 9462,\n",
       " 'sang': 6796,\n",
       " 'sand': 6439,\n",
       " 'sane': 7921,\n",
       " 'leia': 9886,\n",
       " 'portrays': 2235,\n",
       " 'traumatized': 9266,\n",
       " 'more': 53,\n",
       " 'company': 1169,\n",
       " 'learn': 850,\n",
       " 'knocked': 5290,\n",
       " 'huge': 666,\n",
       " 'hugo': 7922,\n",
       " 'hugh': 3934,\n",
       " 'scifi': 6196,\n",
       " 'brett': 7580,\n",
       " 'resemble': 5124,\n",
       " 'paper': 2300,\n",
       " 'scott': 1091,\n",
       " 'colleague': 7698,\n",
       " 'gadget': 4315,\n",
       " \"victoria's\": 9463,\n",
       " 'shocking': 1621,\n",
       " 'ernie': 7581,\n",
       " 'research': 2301,\n",
       " '1990s': 5974,\n",
       " 'saint': 5563,\n",
       " \"paul's\": 9981,\n",
       " 'word': 681,\n",
       " 'blond': 4384,\n",
       " 'understands': 5627,\n",
       " 'overwhelmed': 9101,\n",
       " 'indifference': 9661,\n",
       " 'lombard': 9662,\n",
       " 'pleasures': 8794,\n",
       " 'exercise': 3435,\n",
       " 'insane': 2140,\n",
       " 'callahan': 9464,\n",
       " 'objects': 5344,\n",
       " 'retarded': 3000,\n",
       " 'illiterate': 9267,\n",
       " 'bell': 4015,\n",
       " 'bela': 4679,\n",
       " 'adaptation': 1253,\n",
       " 'luis': 6045,\n",
       " 'belt': 5848,\n",
       " 'satire': 2006,\n",
       " 'geoffrey': 8453,\n",
       " 'treatment': 2199,\n",
       " 'awake': 4141,\n",
       " '33': 9633,\n",
       " 'pressed': 7369,\n",
       " '30': 1088,\n",
       " 'tin': 8172,\n",
       " 'parents': 846,\n",
       " 'couple': 378,\n",
       " 'pounds': 5678,\n",
       " 'chorus': 4316,\n",
       " 'behave': 4486,\n",
       " 'mouth': 1642,\n",
       " 'terrorists': 4847,\n",
       " 'into': 83,\n",
       " 'katie': 8367,\n",
       " 'atlantis': 4016,\n",
       " 'singer': 1950,\n",
       " 'atlantic': 8064,\n",
       " 'paired': 8600,\n",
       " \"chan's\": 9268,\n",
       " 'haunt': 7370,\n",
       " 'puzzling': 9269,\n",
       " 'creasy': 7923,\n",
       " 'detectives': 5928,\n",
       " 'turkish': 7481,\n",
       " 'dickinson': 8931,\n",
       " \"show's\": 3769,\n",
       " 'cannibals': 8601,\n",
       " 'video': 374,\n",
       " 'dynamics': 6891,\n",
       " 'victor': 2273,\n",
       " 'flowing': 8065,\n",
       " 'orleans': 5097,\n",
       " 'makes': 166,\n",
       " 'maker': 3011,\n",
       " 'confidence': 4454,\n",
       " 'comedy': 212,\n",
       " 'intelligent': 1089,\n",
       " 'democracy': 9431,\n",
       " 'insight': 2618,\n",
       " \"wife's\": 5078,\n",
       " 'derivative': 6277,\n",
       " 'snake': 4017,\n",
       " 'denzel': 3387,\n",
       " 'books': 1151,\n",
       " 'bigfoot': 8430,\n",
       " 'witness': 2413,\n",
       " \"'\": 758,\n",
       " 'greedy': 4636,\n",
       " 'prepare': 5403,\n",
       " 'could': 100,\n",
       " 'lumet': 4848,\n",
       " 'interests': 4993,\n",
       " 'gays': 9102,\n",
       " 'false': 2556,\n",
       " 'tonight': 4487,\n",
       " 'depict': 6357,\n",
       " 'sinatra': 2390,\n",
       " 'placement': 8293,\n",
       " 'tape': 2214,\n",
       " 'riding': 3053,\n",
       " 'stuff': 538,\n",
       " 'guessing': 3099,\n",
       " 'frame': 2122,\n",
       " 'destiny': 4205,\n",
       " 'nuclear': 3478,\n",
       " 'preminger': 6892,\n",
       " 'staring': 4488,\n",
       " 'marty': 4897,\n",
       " 'boyer': 6543,\n",
       " 'english': 631,\n",
       " 'genetic': 8783,\n",
       " 'hateful': 6798,\n",
       " 'greater': 2798,\n",
       " 'off': 125,\n",
       " 'crack': 4080,\n",
       " 'become': 413,\n",
       " 'recognition': 4637,\n",
       " 'morris': 4849,\n",
       " 'passion': 1797,\n",
       " 'imaginary': 7265,\n",
       " 'union': 3617,\n",
       " 'swimming': 4419,\n",
       " 'letters': 4455,\n",
       " 'pairing': 8173,\n",
       " 'peters': 5038,\n",
       " 'stopping': 5564,\n",
       " 'moonstruck': 8784,\n",
       " 'tossed': 6265,\n",
       " 'evident': 3523,\n",
       " 'excitement': 2318,\n",
       " 'garbo': 4942,\n",
       " 'problem': 439,\n",
       " 'nonetheless': 2935,\n",
       " 'details': 1373,\n",
       " 'exposure': 4994,\n",
       " 'dave': 3833,\n",
       " 'strings': 5849,\n",
       " 'compete': 6113,\n",
       " 'villainous': 7582,\n",
       " 'madsen': 7683,\n",
       " 'integrity': 5039,\n",
       " 'stinks': 4385,\n",
       " 'porno': 4522,\n",
       " 'worth': 290,\n",
       " 'progression': 8066,\n",
       " 'samurai': 3632,\n",
       " 'machines': 3869,\n",
       " 'viewings': 4719,\n",
       " 'equals': 9538,\n",
       " 'achieve': 2715,\n",
       " '1991': 6047,\n",
       " '1990': 4523,\n",
       " '1993': 5040,\n",
       " '1992': 7371,\n",
       " '1995': 5125,\n",
       " '1994': 6114,\n",
       " 'divorced': 6284,\n",
       " '1996': 4142,\n",
       " '1999': 4206,\n",
       " '1998': 6631,\n",
       " 'era': 999,\n",
       " 'nuts': 4765,\n",
       " 'ladder': 5404,\n",
       " 'vera': 7109,\n",
       " 'schneider': 7266,\n",
       " 'davies': 3770,\n",
       " 'innovative': 3972,\n",
       " 'production': 365,\n",
       " 'understated': 4638,\n",
       " 'reasonably': 3716,\n",
       " 'routines': 6709,\n",
       " 'reasonable': 3793,\n",
       " 'daniel': 2274,\n",
       " \"character's\": 1730,\n",
       " 'flawless': 3562,\n",
       " 'another': 160,\n",
       " 'illustrate': 8785,\n",
       " \"o'toole\": 9887,\n",
       " 'seduction': 9272,\n",
       " 'dogs': 2515,\n",
       " 'cabin': 2816,\n",
       " 'historical': 1379,\n",
       " 'enchanted': 7372,\n",
       " 'convenient': 7699,\n",
       " 'subjects': 4046,\n",
       " 'swamp': 9466,\n",
       " 'aunt': 2965,\n",
       " 'haunted': 2368,\n",
       " 'runs': 1129,\n",
       " 'horrendous': 3388,\n",
       " 'draws': 3771,\n",
       " 'drawn': 1309,\n",
       " 'encounters': 3267,\n",
       " 'handful': 3516,\n",
       " 'kitchen': 3904,\n",
       " 'essentially': 2027,\n",
       " 'han': 8061,\n",
       " 'tone': 1163,\n",
       " 'imaginative': 3268,\n",
       " 'condescending': 9273,\n",
       " 'tons': 3400,\n",
       " 'massive': 2557,\n",
       " 'tony': 1223,\n",
       " 'unlikely': 2388,\n",
       " 'brilliantly': 2105,\n",
       " 'apparently': 684,\n",
       " 'survival': 4098,\n",
       " 'fuss': 8602,\n",
       " 'quentin': 6632,\n",
       " 'humble': 4489,\n",
       " 'mid': 1696,\n",
       " 'thanks': 1216,\n",
       " 'similarities': 4351,\n",
       " 'sparse': 9467,\n",
       " 'night': 314,\n",
       " 'jacques': 6990,\n",
       " 'attorney': 4817,\n",
       " 'rendering': 7700,\n",
       " 'czech': 8426,\n",
       " 'captive': 8431,\n",
       " 'test': 2181,\n",
       " 'tess': 8067,\n",
       " 'songs': 690,\n",
       " 'concept': 1120,\n",
       " 'battle': 985,\n",
       " 'hurry': 8605,\n",
       " 'rebecca': 9274,\n",
       " 'cunningham': 7098,\n",
       " 'turns': 505,\n",
       " 'gun': 1056,\n",
       " 'gus': 7267,\n",
       " 'gut': 5565,\n",
       " 'guy': 232,\n",
       " 'rapist': 5749,\n",
       " 'shares': 5519,\n",
       " 'shared': 5345,\n",
       " \"hadn't\": 1869,\n",
       " 'teaches': 5291,\n",
       " 'teacher': 1750,\n",
       " 'sending': 5679,\n",
       " 'franklin': 6893,\n",
       " 'plotted': 8432,\n",
       " 'regardless': 3563,\n",
       " 'extra': 1727,\n",
       " 'woefully': 8174,\n",
       " 'chip': 8175,\n",
       " 'lacks': 1503,\n",
       " 'discussion': 3772,\n",
       " 'kurt': 3199,\n",
       " 'chops': 7924,\n",
       " 'brain': 1224,\n",
       " 'still': 131,\n",
       " 'drop': 2440,\n",
       " 'challenged': 5235,\n",
       " 'yeah': 1243,\n",
       " 'challenges': 5459,\n",
       " 'year': 291,\n",
       " 'gibson': 8294,\n",
       " 'transition': 4592,\n",
       " 'suffice': 4917,\n",
       " 'romania': 8606,\n",
       " 'flipping': 8954,\n",
       " 'tomorrow': 5520,\n",
       " 'seymour': 6759,\n",
       " 'brains': 4081,\n",
       " 'professionals': 7099,\n",
       " 'transferred': 9888,\n",
       " 'importantly': 3517,\n",
       " 'premiered': 8433,\n",
       " 'teamed': 9663,\n",
       " 'burst': 5566,\n",
       " 'colours': 6267,\n",
       " 'madness': 3001,\n",
       " 'foreboding': 8607,\n",
       " 'inexplicable': 5750,\n",
       " 'exploit': 6471,\n",
       " 'charismatic': 3389,\n",
       " 'dictator': 8434,\n",
       " 'elvis': 3479,\n",
       " 'offbeat': 6358,\n",
       " 'develop': 2061,\n",
       " 'food': 1644,\n",
       " 'death': 341,\n",
       " 'earnest': 6359,\n",
       " 'fortune': 3200,\n",
       " 'fully': 1314,\n",
       " 'verbal': 6894,\n",
       " 'exposed': 3773,\n",
       " 'exposes': 9889,\n",
       " 'francis': 4804,\n",
       " \"isn't\": 218,\n",
       " 'pitched': 7925,\n",
       " 'freddy': 2281,\n",
       " 'fools': 6799,\n",
       " 'poor': 338,\n",
       " 'pool': 3073,\n",
       " 'corey': 7184,\n",
       " 'overseas': 9469,\n",
       " 'robert': 670,\n",
       " 'thoughtful': 4353,\n",
       " 'religious': 1736,\n",
       " 'decide': 1197,\n",
       " 'ass': 1995,\n",
       " 'streets': 1986,\n",
       " 'bass': 9470,\n",
       " 'excess': 5850,\n",
       " 'advertising': 4593,\n",
       " 'cathy': 9664,\n",
       " 'heroic': 3818,\n",
       " 'budgets': 6441,\n",
       " 'reject': 7970,\n",
       " 'surpasses': 9471,\n",
       " 'criticize': 7100,\n",
       " 'anytime': 6800,\n",
       " 'roommates': 9103,\n",
       " 'absence': 3819,\n",
       " \"haven't\": 774,\n",
       " 'ninja': 4850,\n",
       " 'bless': 8295,\n",
       " 'fairy': 2447,\n",
       " 'heavy': 1185,\n",
       " 'jolly': 9104,\n",
       " 'lord': 1635,\n",
       " 'earns': 8609,\n",
       " 'hapless': 5680,\n",
       " 'american': 298,\n",
       " 'visions': 5567,\n",
       " 'trapped': 2605,\n",
       " 'toward': 1841,\n",
       " 'randomly': 4851,\n",
       " 'organs': 9472,\n",
       " 'caliber': 4898,\n",
       " 'physics': 5681,\n",
       " 'stalked': 7185,\n",
       " 'phenomenon': 5682,\n",
       " 'stalker': 5851,\n",
       " 'heavens': 9105,\n",
       " 'competing': 9890,\n",
       " 'imitating': 9156,\n",
       " 'fluff': 5628,\n",
       " 'hype': 3401,\n",
       " 'locale': 8435,\n",
       " 'portrait': 3213,\n",
       " 'locals': 5751,\n",
       " 'abruptly': 6197,\n",
       " 'league': 2756,\n",
       " \"wouldn't\": 586,\n",
       " 'empty': 1896,\n",
       " 'juice': 7926,\n",
       " 'match': 1014,\n",
       " 'grant': 2107,\n",
       " 'sensual': 8077,\n",
       " 'grand': 1758,\n",
       " 'composition': 7269,\n",
       " 'classmates': 8176,\n",
       " 'obviously': 540,\n",
       " 'synopsis': 3935,\n",
       " 'reviewed': 6801,\n",
       " 'reviewer': 2215,\n",
       " 'showing': 800,\n",
       " 'sketch': 5405,\n",
       " 'lips': 4047,\n",
       " 'towards': 949,\n",
       " 'silence': 3542,\n",
       " 'alison': 5406,\n",
       " 'placing': 9106,\n",
       " 'ideals': 7843,\n",
       " 'similar': 729,\n",
       " 'ordered': 5175,\n",
       " 'fears': 3564,\n",
       " 'department': 2550,\n",
       " 'smiles': 5852,\n",
       " 'unfunny': 1960,\n",
       " 'riders': 8234,\n",
       " 'telling': 979,\n",
       " 'yourselves': 9891,\n",
       " 'watered': 9107,\n",
       " 'jump': 1783,\n",
       " 'notwithstanding': 8786,\n",
       " 'vampire': 1362,\n",
       " 'lugosi': 2784,\n",
       " 'clark': 2595,\n",
       " 'manage': 1921,\n",
       " 'clara': 6544,\n",
       " 'camera': 370,\n",
       " 'boards': 8296,\n",
       " 'meek': 8068,\n",
       " 'servants': 6360,\n",
       " 'meet': 909,\n",
       " 'pulling': 3661,\n",
       " 'sought': 6442,\n",
       " 'orson': 4420,\n",
       " 'rohmer': 9473,\n",
       " 'sentiments': 9892,\n",
       " 'ronald': 6268,\n",
       " 'scoop': 6443,\n",
       " 'favourites': 7482,\n",
       " \"its'\": 9665,\n",
       " 'university': 3436,\n",
       " 'slide': 6444,\n",
       " 'special': 318,\n",
       " 'butch': 6445,\n",
       " 'obsessive': 6633,\n",
       " 'darkly': 8069,\n",
       " 'jill': 6361,\n",
       " 'times': 211,\n",
       " 'timed': 8177,\n",
       " 'bitch': 5460,\n",
       " 'wrapped': 4561,\n",
       " 'hines': 7701,\n",
       " 'bastard': 8436,\n",
       " 'battles': 3353,\n",
       " 'mansion': 3025,\n",
       " 'repeated': 2448,\n",
       " 'manga': 8437,\n",
       " 'unfinished': 9666,\n",
       " 'sheriff': 2249,\n",
       " 'hector': 9164,\n",
       " 'won': 1199,\n",
       " 'cameos': 3201,\n",
       " 'inherited': 9275,\n",
       " 'episodes': 672,\n",
       " 'ken': 3662,\n",
       " 'kicking': 4562,\n",
       " 'key': 1317,\n",
       " 'limits': 4317,\n",
       " 'cena': 7702,\n",
       " 'troopers': 8297,\n",
       " 'controlled': 5407,\n",
       " 'surface': 2558,\n",
       " 'examined': 8932,\n",
       " 'http': 5752,\n",
       " 'riff': 7927,\n",
       " 'montages': 9474,\n",
       " 'increasingly': 3437,\n",
       " 'distant': 3609,\n",
       " 'gamut': 9835,\n",
       " 'disappearance': 8070,\n",
       " 'demonstrates': 5629,\n",
       " 'cradle': 8933,\n",
       " 'demonstrated': 6710,\n",
       " 'limitations': 6198,\n",
       " 'unhinged': 7619,\n",
       " 'nightclub': 6446,\n",
       " 'pointless': 1149,\n",
       " 'additional': 5461,\n",
       " 'gain': 3235,\n",
       " 'highest': 4082,\n",
       " 'kisses': 9893,\n",
       " 'beats': 3936,\n",
       " 'education': 4421,\n",
       " 'disasters': 8930,\n",
       " 'foil': 7186,\n",
       " 'consists': 3202,\n",
       " 'sorry': 806,\n",
       " 'void': 6991,\n",
       " 'suspenseful': 2570,\n",
       " 'herbert': 6992,\n",
       " 'unrelated': 5684,\n",
       " 'enhance': 6802,\n",
       " 'iturbi': 7373,\n",
       " 'kidnap': 7101,\n",
       " 'blandings': 7928,\n",
       " 'meg': 5799,\n",
       " 'mel': 3774,\n",
       " 'men': 349,\n",
       " 'mitchell': 3717,\n",
       " 'robertson': 7270,\n",
       " 'berlin': 4456,\n",
       " 'shahid': 7733,\n",
       " 'room': 673,\n",
       " 'roof': 5370,\n",
       " 'movies': 102,\n",
       " 'exceptions': 5630,\n",
       " 'root': 3663,\n",
       " 'titular': 7803,\n",
       " 'gordon': 2236,\n",
       " 'vicious': 3839,\n",
       " 'third': 840,\n",
       " 'fable': 9108,\n",
       " 'budding': 8178,\n",
       " 'personal': 965,\n",
       " 'crew': 1051,\n",
       " 'anil': 6301,\n",
       " 'combination': 2221,\n",
       " 'one': 31,\n",
       " 'forgot': 2738,\n",
       " 'aids': 4422,\n",
       " 'comedies': 1290,\n",
       " 'mandy': 7703,\n",
       " 'uses': 1077,\n",
       " 'floors': 9667,\n",
       " 'downside': 9668,\n",
       " 'gandolfini': 8179,\n",
       " 'begins': 778,\n",
       " 'mario': 4386,\n",
       " 'maria': 2907,\n",
       " 'zealand': 9109,\n",
       " 'mildred': 4049,\n",
       " 'testing': 7704,\n",
       " 'narrated': 6993,\n",
       " 'guaranteed': 5523,\n",
       " 'represented': 4359,\n",
       " 'quinn': 5800,\n",
       " 'mathieu': 6269,\n",
       " 'asks': 1643,\n",
       " 'entered': 5978,\n",
       " 'lovely': 1334,\n",
       " 'locations': 1979,\n",
       " 'lionel': 6994,\n",
       " 'ugly': 1558,\n",
       " 'cant': 2488,\n",
       " 'realizing': 4354,\n",
       " 'ella': 9476,\n",
       " 'programs': 5872,\n",
       " 'failing': 3718,\n",
       " 'reese': 8298,\n",
       " 'yours': 6447,\n",
       " 'assigned': 4943,\n",
       " 'fighters': 8438,\n",
       " 'goodman': 8299,\n",
       " 'stunts': 3289,\n",
       " 'nude': 2516,\n",
       " 'dean': 2643,\n",
       " 'deal': 855,\n",
       " 'deaf': 5080,\n",
       " 'dear': 3214,\n",
       " 'shakespeare': 2282,\n",
       " 'confrontation': 5127,\n",
       " 'afternoon': 2655,\n",
       " 'automatically': 5347,\n",
       " 'down': 180,\n",
       " 'narration': 2559,\n",
       " 'editor': 3794,\n",
       " 'creation': 3565,\n",
       " 'batman': 1354,\n",
       " 'landing': 5128,\n",
       " 'feminine': 6895,\n",
       " 'awhile': 5236,\n",
       " 'happening': 1448,\n",
       " 'pseudo': 3905,\n",
       " 'restored': 4639,\n",
       " 'father': 336,\n",
       " 'turgid': 9895,\n",
       " 'talked': 3543,\n",
       " 'targets': 6995,\n",
       " 'suspect': 1781,\n",
       " 'box': 953,\n",
       " 'boy': 430,\n",
       " 'maguire': 9477,\n",
       " 'bow': 5631,\n",
       " 'bon': 8610,\n",
       " 'boo': 7804,\n",
       " 'bob': 2046,\n",
       " 'teenage': 1667,\n",
       " 'transplant': 8300,\n",
       " 'drags': 3465,\n",
       " 'dennis': 2801,\n",
       " 'snl': 4490,\n",
       " 'blooded': 6448,\n",
       " 'police': 568,\n",
       " 'policy': 6896,\n",
       " 'tucker': 9669,\n",
       " 'lunch': 6803,\n",
       " 'elephants': 8787,\n",
       " 'ajay': 7483,\n",
       " 'stanwyck': 3334,\n",
       " 'carnival': 9110,\n",
       " 'frequent': 4852,\n",
       " 'first': 86,\n",
       " 'fleeing': 8934,\n",
       " 'speaking': 1386,\n",
       " 'kevin': 1842,\n",
       " 'complexity': 4640,\n",
       " 'shocked': 2414,\n",
       " 'shocker': 8301,\n",
       " '200': 6305,\n",
       " 'arguing': 6545,\n",
       " 'angst': 5568,\n",
       " 'harvey': 4355,\n",
       " 'russian': 1766,\n",
       " 'treasure': 2528,\n",
       " 'travesty': 4995,\n",
       " 'enthusiasm': 4805,\n",
       " 'get': 79,\n",
       " 'gee': 9111,\n",
       " 'gen': 8611,\n",
       " 'gem': 1528,\n",
       " 'london': 1316,\n",
       " 'seat': 2224,\n",
       " 'declares': 9896,\n",
       " 'seal': 9276,\n",
       " 'wonder': 594,\n",
       " 'satisfying': 2349,\n",
       " 'label': 6049,\n",
       " 'boundaries': 7484,\n",
       " 'across': 638,\n",
       " 'august': 6996,\n",
       " 'considering': 1069,\n",
       " 'capable': 2250,\n",
       " 'sort': 432,\n",
       " 'wake': 3290,\n",
       " 'hardcore': 4208,\n",
       " 'promising': 2428,\n",
       " 'rupert': 7285,\n",
       " \"o'brien\": 7805,\n",
       " 'extended': 3844,\n",
       " 'concentrates': 9366,\n",
       " 'northam': 7485,\n",
       " 'consisted': 9277,\n",
       " 'flavor': 6897,\n",
       " 'clueless': 5753,\n",
       " 'adams': 4601,\n",
       " 'passionate': 4387,\n",
       " 'each': 257,\n",
       " 'demonic': 5979,\n",
       " 'distracted': 7136,\n",
       " 'spice': 6711,\n",
       " 'vhs': 1856,\n",
       " 'examine': 8302,\n",
       " \"she'd\": 6449,\n",
       " 'hey': 1400,\n",
       " \"she's\": 442,\n",
       " 'u': 1206,\n",
       " 'motel': 7705,\n",
       " 'plodding': 7187,\n",
       " 'former': 1138,\n",
       " 'paperhouse': 9670,\n",
       " 'strung': 7583,\n",
       " 'zero': 1456,\n",
       " 'newspaper': 3994,\n",
       " 'masterpiece': 991,\n",
       " 'mentions': 4899,\n",
       " 'africa': 2415,\n",
       " 'tacky': 5237,\n",
       " 'engaged': 3953,\n",
       " 'mill': 4209,\n",
       " 'hour': 534,\n",
       " 'recall': 2283,\n",
       " 'sucks': 1870,\n",
       " 'remain': 2416,\n",
       " 'stubborn': 9278,\n",
       " 'ford': 2108,\n",
       " 'colman': 8071,\n",
       " 'biography': 5041,\n",
       " 'homicide': 6071,\n",
       " 'needs': 738,\n",
       " 'acts': 1421,\n",
       " 'sacred': 9788,\n",
       " 'kitty': 5685,\n",
       " 'sophistication': 8439,\n",
       " 'brady': 4143,\n",
       " 'dragon': 2785,\n",
       " 'heartfelt': 5348,\n",
       " 'appeals': 6450,\n",
       " 'comedic': 1717,\n",
       " 'compound': 9671,\n",
       " 'viewers': 797,\n",
       " 'mystery': 736,\n",
       " 'repeating': 5686,\n",
       " 'rhys': 9478,\n",
       " 'engaging': 1728,\n",
       " 'edged': 9999,\n",
       " 'perry': 4050,\n",
       " \"they'll\": 3664,\n",
       " 'extraordinary': 2802,\n",
       " 'backed': 6712,\n",
       " 'top': 350,\n",
       " 'razor': 6997,\n",
       " 'mercilessly': 9898,\n",
       " 'ton': 5853,\n",
       " 'tom': 827,\n",
       " 'lifts': 8440,\n",
       " 'godfather': 3518,\n",
       " 'eggs': 9672,\n",
       " 'charm': 1382,\n",
       " 'services': 7272,\n",
       " 'rebels': 7584,\n",
       " 'chock': 8303,\n",
       " 'ebert': 6451,\n",
       " 'premise': 863,\n",
       " 'foreign': 2189,\n",
       " 'point': 213,\n",
       " 'expensive': 3269,\n",
       " 'screened': 7930,\n",
       " 'variation': 8022,\n",
       " 'politician': 5854,\n",
       " 'widescreen': 5633,\n",
       " 'century': 1117,\n",
       " 'tashan': 7931,\n",
       " '1965': 8912,\n",
       " '1967': 7170,\n",
       " 'knock': 3296,\n",
       " 'foolish': 6393,\n",
       " 'candle': 7585,\n",
       " 'though': 151,\n",
       " 'manipulate': 8803,\n",
       " 'abusive': 4595,\n",
       " 'underused': 9673,\n",
       " 'murphy': 2693,\n",
       " 'stripped': 9480,\n",
       " 'scarlet': 8304,\n",
       " 'cure': 4318,\n",
       " 'stripper': 8788,\n",
       " 'utterly': 1254,\n",
       " 'implied': 6270,\n",
       " 'portraying': 2265,\n",
       " 'literary': 5292,\n",
       " 'pleasing': 5754,\n",
       " 'entire': 436,\n",
       " 'rivers': 6998,\n",
       " 'fat': 1922,\n",
       " 'archer': 9112,\n",
       " 'healing': 8935,\n",
       " 'escaped': 3954,\n",
       " 'closing': 2722,\n",
       " 'didnt': 9279,\n",
       " 'continuity': 2386,\n",
       " 'varied': 7188,\n",
       " 'holds': 1777,\n",
       " 'suspend': 4984,\n",
       " 'profile': 7486,\n",
       " 'watch': 106,\n",
       " 'incompetence': 9369,\n",
       " 'chasing': 3187,\n",
       " 'subsequently': 7706,\n",
       " 'grendel': 8789,\n",
       " 'grayson': 5081,\n",
       " 'blatant': 4144,\n",
       " 'lively': 4764,\n",
       " 'sexual': 861,\n",
       " 'yard': 5042,\n",
       " 'yarn': 8441,\n",
       " 'reaches': 4231,\n",
       " 'reached': 3820,\n",
       " 'spelling': 8442,\n",
       " 'sweden': 7189,\n",
       " 'have': 28,\n",
       " 'prisoner': 4766,\n",
       " 'disease': 3497,\n",
       " 'occasion': 4083,\n",
       " 'hamlet': 3420,\n",
       " 'knowledge': 1857,\n",
       " 'perfection': 3203,\n",
       " 'teams': 6271,\n",
       " 'showdown': 4901,\n",
       " 'bruno': 6362,\n",
       " 'incarnation': 9674,\n",
       " 'antics': 3871,\n",
       " 'russ': 7487,\n",
       " 'joke': 975,\n",
       " 'equal': 3215,\n",
       " 'fassbinder': 7707,\n",
       " \"it's\": 45,\n",
       " \"it'd\": 9899,\n",
       " 'locales': 9113,\n",
       " 'meredith': 7932,\n",
       " 'frustrating': 5129,\n",
       " 'powell': 2592,\n",
       " 'exceedingly': 9900,\n",
       " 'stores': 5687,\n",
       " 'griffith': 5178,\n",
       " 'resolved': 6272,\n",
       " 'doyle': 7933,\n",
       " 'like': 40,\n",
       " 'vibrant': 5755,\n",
       " 'admitted': 6999,\n",
       " 'chick': 2284,\n",
       " 'hair': 1153,\n",
       " 'recommendation': 5492,\n",
       " 'hysterically': 7273,\n",
       " 'lieutenant': 9280,\n",
       " 'uptight': 8443,\n",
       " 'introduces': 4319,\n",
       " 'rushed': 3312,\n",
       " 'rushes': 9481,\n",
       " \"'80s\": 7423,\n",
       " 'coke': 6804,\n",
       " 'flip': 7808,\n",
       " 'thorn': 9114,\n",
       " 'circus': 5601,\n",
       " 'dressed': 1810,\n",
       " 'detail': 1589,\n",
       " 'dresses': 5350,\n",
       " \"daughter's\": 5569,\n",
       " \"ted's\": 9901,\n",
       " 'harriet': 6452,\n",
       " 'direct': 1504,\n",
       " 'nail': 4423,\n",
       " 'doubt': 824,\n",
       " 'selected': 6546,\n",
       " 'revolves': 3054,\n",
       " 'revolver': 8180,\n",
       " 'liberty': 8181,\n",
       " 'leaves': 889,\n",
       " 'excellent': 321,\n",
       " 'salvage': 7809,\n",
       " 'estate': 3519,\n",
       " 'attract': 5688,\n",
       " 'ceremony': 8471,\n",
       " 'keen': 5855,\n",
       " 'drummer': 8445,\n",
       " 'description': 2786,\n",
       " 'insecure': 9902,\n",
       " 'parallel': 4680,\n",
       " 'amid': 8446,\n",
       " 'upside': 6805,\n",
       " 'succeeds': 2880,\n",
       " 'detroit': 7190,\n",
       " 'newly': 4700,\n",
       " 'independence': 5756,\n",
       " 'associate': 8182,\n",
       " 'days': 504,\n",
       " ...}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross check step to see word to index mapping\n",
    "word2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1411: 'woods',\n",
       " 2348: 'hanging',\n",
       " 2292: 'woody',\n",
       " 6749: 'arranged',\n",
       " 2341: 'bringing',\n",
       " 1639: 'wooden',\n",
       " 4013: 'errors',\n",
       " 3233: 'dialogs',\n",
       " 362: 'kids',\n",
       " 5037: 'uplifting',\n",
       " 7096: 'controversy',\n",
       " 9881: 'projection',\n",
       " 7183: 'stern',\n",
       " 5624: 'morally',\n",
       " 5286: 'wang',\n",
       " 181: 'want',\n",
       " 2106: 'travel',\n",
       " 6705: 'barbra',\n",
       " 3933: 'dinosaurs',\n",
       " 355: 'wrong',\n",
       " 4763: 'subplots',\n",
       " 9095: 'welcomed',\n",
       " 6706: 'butcher',\n",
       " 1183: 'fit',\n",
       " 1930: 'screaming',\n",
       " 4290: 'fix',\n",
       " 9657: 'hurting',\n",
       " 302: 'effects',\n",
       " 8778: 'barton',\n",
       " 6195: 'ingrid',\n",
       " 7919: 'adapt',\n",
       " 4014: 'disturbed',\n",
       " 1288: 'purpose',\n",
       " 6795: 'olds',\n",
       " 887: 'needed',\n",
       " 1300: 'master',\n",
       " 5402: 'positively',\n",
       " 9839: 'zatoichi',\n",
       " 547: 'feeling',\n",
       " 5625: 'affairs',\n",
       " 7802: 'wholesome',\n",
       " 1360: 'cinematic',\n",
       " 4991: 'tech',\n",
       " 660: 'saying',\n",
       " 8425: 'padded',\n",
       " 5748: 'tempted',\n",
       " 7479: 'plate',\n",
       " 3902: 'altogether',\n",
       " 8779: 'lds',\n",
       " 1780: 'nicely',\n",
       " 4591: 'mummy',\n",
       " 775: 'lots',\n",
       " 9262: 'lotr',\n",
       " 9882: 'irs',\n",
       " 6890: 'ira',\n",
       " 7696: 'discipline',\n",
       " 876: 'nature',\n",
       " 3903: 'superficial',\n",
       " 2826: 'extent',\n",
       " 8780: 'bothers',\n",
       " 76: 'much',\n",
       " 6262: 'spit',\n",
       " 7367: 'arkin',\n",
       " 4992: 'doubts',\n",
       " 3087: 'spin',\n",
       " 2579: 'hong',\n",
       " 9097: 'academic',\n",
       " 4452: 'corporate',\n",
       " 4314: 'hal',\n",
       " 4895: 'ham',\n",
       " 69: 'had',\n",
       " 47: 'has',\n",
       " 2404: 'hat',\n",
       " 2293: 'crowd',\n",
       " 6630: 'crown',\n",
       " 1324: 'bottom',\n",
       " 1184: 'starring',\n",
       " 5343: 'marshall',\n",
       " 9098: 'honeymoon',\n",
       " 3234: 'shoots',\n",
       " 8292: 'fabric',\n",
       " 3540: 'raped',\n",
       " 8929: 'rapes',\n",
       " 5927: \"else's\",\n",
       " 7256: 'martian',\n",
       " 9460: 'passenger',\n",
       " 6044: 'disgrace',\n",
       " 5123: 'barrymore',\n",
       " 9883: 'cambodia',\n",
       " 5561: 'palma',\n",
       " 3978: 'explosions',\n",
       " 8062: 'loren',\n",
       " 6707: 'shootout',\n",
       " 3629: 'chain',\n",
       " 2500: 'whoever',\n",
       " 3039: 'chair',\n",
       " 4520: 'ballet',\n",
       " 5626: 'macho',\n",
       " 3399: 'jerk',\n",
       " 7368: 'gloomy',\n",
       " 2897: 'locked',\n",
       " 2591: 'exact',\n",
       " 786: 'minute',\n",
       " 6541: 'celebrated',\n",
       " 3386: 'unintentionally',\n",
       " 9099: 'climbs',\n",
       " 9658: 'honour',\n",
       " 5517: 'address',\n",
       " 9461: 'benson',\n",
       " 3843: 'cusack',\n",
       " 3630: 'opposed',\n",
       " 1045: 'following',\n",
       " 4521: 'convincingly',\n",
       " 4255: 'surfing',\n",
       " 1240: 'jim',\n",
       " 6542: 'quarter',\n",
       " 6263: 'entering',\n",
       " 615: 'seriously',\n",
       " 7820: 'raunchy',\n",
       " 7097: 'grandma',\n",
       " 6111: 'modest',\n",
       " 2850: 'spoken',\n",
       " 3252: 'concert',\n",
       " 8427: 'lingering',\n",
       " 9100: 'snatch',\n",
       " 9263: \"miyazaki's\",\n",
       " 4635: 'wandering',\n",
       " 679: 'turned',\n",
       " 3768: 'turner',\n",
       " 1961: 'opposite',\n",
       " 6089: 'grateful',\n",
       " 5562: 'inconsistent',\n",
       " 3792: 'imagined',\n",
       " 9884: 'enthralled',\n",
       " 8599: 'tsui',\n",
       " 3541: 'menacing',\n",
       " 3660: 'convoluted',\n",
       " 5288: 'millionaire',\n",
       " 1263: 'west',\n",
       " 4204: 'motives',\n",
       " 4453: 'photos',\n",
       " 6112: 'unlikeable',\n",
       " 2131: 'technology',\n",
       " 6988: 'otto',\n",
       " 2009: 'visually',\n",
       " 112: 'being',\n",
       " 9355: 'grounded',\n",
       " 6878: 'excuses',\n",
       " 5289: 'sums',\n",
       " 5847: 'traffic',\n",
       " 8428: 'sensational',\n",
       " 8781: 'satisfactory',\n",
       " 2327: 'substance',\n",
       " 7697: 'thailand',\n",
       " 5677: 'hopkins',\n",
       " 9659: 'sealed',\n",
       " 8063: 'brazilian',\n",
       " 6708: 'bubble',\n",
       " 9264: 'wits',\n",
       " 9660: 'societal',\n",
       " 19: 'with',\n",
       " 5233: 'abused',\n",
       " 3980: 'rage',\n",
       " 5174: 'tripe',\n",
       " 1641: 'dirty',\n",
       " 3631: 'watches',\n",
       " 7579: 'watcher',\n",
       " 9885: 'ensuing',\n",
       " 296: 'watched',\n",
       " 5234: 'cream',\n",
       " 9265: 'waving',\n",
       " 6264: 'natalie',\n",
       " 3351: 'tricks',\n",
       " 2164: 'caused',\n",
       " 5518: 'beware',\n",
       " 2906: 'causes',\n",
       " 8782: 'nora',\n",
       " 5797: 'norm',\n",
       " 9462: 'sans',\n",
       " 6796: 'sang',\n",
       " 6439: 'sand',\n",
       " 7921: 'sane',\n",
       " 9886: 'leia',\n",
       " 2235: 'portrays',\n",
       " 9266: 'traumatized',\n",
       " 53: 'more',\n",
       " 1169: 'company',\n",
       " 850: 'learn',\n",
       " 5290: 'knocked',\n",
       " 666: 'huge',\n",
       " 7922: 'hugo',\n",
       " 3934: 'hugh',\n",
       " 6196: 'scifi',\n",
       " 7580: 'brett',\n",
       " 5124: 'resemble',\n",
       " 2300: 'paper',\n",
       " 1091: 'scott',\n",
       " 7698: 'colleague',\n",
       " 4315: 'gadget',\n",
       " 9463: \"victoria's\",\n",
       " 1621: 'shocking',\n",
       " 7581: 'ernie',\n",
       " 2301: 'research',\n",
       " 5974: '1990s',\n",
       " 5563: 'saint',\n",
       " 9981: \"paul's\",\n",
       " 681: 'word',\n",
       " 4384: 'blond',\n",
       " 5627: 'understands',\n",
       " 9101: 'overwhelmed',\n",
       " 9661: 'indifference',\n",
       " 9662: 'lombard',\n",
       " 8794: 'pleasures',\n",
       " 3435: 'exercise',\n",
       " 2140: 'insane',\n",
       " 9464: 'callahan',\n",
       " 5344: 'objects',\n",
       " 3000: 'retarded',\n",
       " 9267: 'illiterate',\n",
       " 4015: 'bell',\n",
       " 4679: 'bela',\n",
       " 1253: 'adaptation',\n",
       " 6045: 'luis',\n",
       " 5848: 'belt',\n",
       " 2006: 'satire',\n",
       " 8453: 'geoffrey',\n",
       " 2199: 'treatment',\n",
       " 4141: 'awake',\n",
       " 9633: '33',\n",
       " 7369: 'pressed',\n",
       " 1088: '30',\n",
       " 8172: 'tin',\n",
       " 846: 'parents',\n",
       " 378: 'couple',\n",
       " 5678: 'pounds',\n",
       " 4316: 'chorus',\n",
       " 4486: 'behave',\n",
       " 1642: 'mouth',\n",
       " 4847: 'terrorists',\n",
       " 83: 'into',\n",
       " 8367: 'katie',\n",
       " 4016: 'atlantis',\n",
       " 1950: 'singer',\n",
       " 8064: 'atlantic',\n",
       " 8600: 'paired',\n",
       " 9268: \"chan's\",\n",
       " 7370: 'haunt',\n",
       " 9269: 'puzzling',\n",
       " 7923: 'creasy',\n",
       " 5928: 'detectives',\n",
       " 7481: 'turkish',\n",
       " 8931: 'dickinson',\n",
       " 3769: \"show's\",\n",
       " 8601: 'cannibals',\n",
       " 374: 'video',\n",
       " 6891: 'dynamics',\n",
       " 2273: 'victor',\n",
       " 8065: 'flowing',\n",
       " 5097: 'orleans',\n",
       " 166: 'makes',\n",
       " 3011: 'maker',\n",
       " 4454: 'confidence',\n",
       " 212: 'comedy',\n",
       " 1089: 'intelligent',\n",
       " 9431: 'democracy',\n",
       " 2618: 'insight',\n",
       " 5078: \"wife's\",\n",
       " 6277: 'derivative',\n",
       " 4017: 'snake',\n",
       " 3387: 'denzel',\n",
       " 1151: 'books',\n",
       " 8430: 'bigfoot',\n",
       " 2413: 'witness',\n",
       " 758: \"'\",\n",
       " 4636: 'greedy',\n",
       " 5403: 'prepare',\n",
       " 100: 'could',\n",
       " 4848: 'lumet',\n",
       " 4993: 'interests',\n",
       " 9102: 'gays',\n",
       " 2556: 'false',\n",
       " 4487: 'tonight',\n",
       " 6357: 'depict',\n",
       " 2390: 'sinatra',\n",
       " 8293: 'placement',\n",
       " 2214: 'tape',\n",
       " 3053: 'riding',\n",
       " 538: 'stuff',\n",
       " 3099: 'guessing',\n",
       " 2122: 'frame',\n",
       " 4205: 'destiny',\n",
       " 3478: 'nuclear',\n",
       " 6892: 'preminger',\n",
       " 4488: 'staring',\n",
       " 4897: 'marty',\n",
       " 6543: 'boyer',\n",
       " 631: 'english',\n",
       " 8783: 'genetic',\n",
       " 6798: 'hateful',\n",
       " 2798: 'greater',\n",
       " 125: 'off',\n",
       " 4080: 'crack',\n",
       " 413: 'become',\n",
       " 4637: 'recognition',\n",
       " 4849: 'morris',\n",
       " 1797: 'passion',\n",
       " 7265: 'imaginary',\n",
       " 3617: 'union',\n",
       " 4419: 'swimming',\n",
       " 4455: 'letters',\n",
       " 8173: 'pairing',\n",
       " 5038: 'peters',\n",
       " 5564: 'stopping',\n",
       " 8784: 'moonstruck',\n",
       " 6265: 'tossed',\n",
       " 3523: 'evident',\n",
       " 2318: 'excitement',\n",
       " 4942: 'garbo',\n",
       " 439: 'problem',\n",
       " 2935: 'nonetheless',\n",
       " 1373: 'details',\n",
       " 4994: 'exposure',\n",
       " 3833: 'dave',\n",
       " 5849: 'strings',\n",
       " 6113: 'compete',\n",
       " 7582: 'villainous',\n",
       " 7683: 'madsen',\n",
       " 5039: 'integrity',\n",
       " 4385: 'stinks',\n",
       " 4522: 'porno',\n",
       " 290: 'worth',\n",
       " 8066: 'progression',\n",
       " 3632: 'samurai',\n",
       " 3869: 'machines',\n",
       " 4719: 'viewings',\n",
       " 9538: 'equals',\n",
       " 2715: 'achieve',\n",
       " 6047: '1991',\n",
       " 4523: '1990',\n",
       " 5040: '1993',\n",
       " 7371: '1992',\n",
       " 5125: '1995',\n",
       " 6114: '1994',\n",
       " 6284: 'divorced',\n",
       " 4142: '1996',\n",
       " 4206: '1999',\n",
       " 6631: '1998',\n",
       " 999: 'era',\n",
       " 4765: 'nuts',\n",
       " 5404: 'ladder',\n",
       " 7109: 'vera',\n",
       " 7266: 'schneider',\n",
       " 3770: 'davies',\n",
       " 3972: 'innovative',\n",
       " 365: 'production',\n",
       " 4638: 'understated',\n",
       " 3716: 'reasonably',\n",
       " 6709: 'routines',\n",
       " 3793: 'reasonable',\n",
       " 2274: 'daniel',\n",
       " 1730: \"character's\",\n",
       " 3562: 'flawless',\n",
       " 160: 'another',\n",
       " 8785: 'illustrate',\n",
       " 9887: \"o'toole\",\n",
       " 9272: 'seduction',\n",
       " 2515: 'dogs',\n",
       " 2816: 'cabin',\n",
       " 1379: 'historical',\n",
       " 7372: 'enchanted',\n",
       " 7699: 'convenient',\n",
       " 4046: 'subjects',\n",
       " 9466: 'swamp',\n",
       " 2965: 'aunt',\n",
       " 2368: 'haunted',\n",
       " 1129: 'runs',\n",
       " 3388: 'horrendous',\n",
       " 3771: 'draws',\n",
       " 1309: 'drawn',\n",
       " 3267: 'encounters',\n",
       " 3516: 'handful',\n",
       " 3904: 'kitchen',\n",
       " 2027: 'essentially',\n",
       " 8061: 'han',\n",
       " 1163: 'tone',\n",
       " 3268: 'imaginative',\n",
       " 9273: 'condescending',\n",
       " 3400: 'tons',\n",
       " 2557: 'massive',\n",
       " 1223: 'tony',\n",
       " 2388: 'unlikely',\n",
       " 2105: 'brilliantly',\n",
       " 684: 'apparently',\n",
       " 4098: 'survival',\n",
       " 8602: 'fuss',\n",
       " 6632: 'quentin',\n",
       " 4489: 'humble',\n",
       " 1696: 'mid',\n",
       " 1216: 'thanks',\n",
       " 4351: 'similarities',\n",
       " 9467: 'sparse',\n",
       " 314: 'night',\n",
       " 6990: 'jacques',\n",
       " 4817: 'attorney',\n",
       " 7700: 'rendering',\n",
       " 8426: 'czech',\n",
       " 8431: 'captive',\n",
       " 2181: 'test',\n",
       " 8067: 'tess',\n",
       " 690: 'songs',\n",
       " 1120: 'concept',\n",
       " 985: 'battle',\n",
       " 8605: 'hurry',\n",
       " 9274: 'rebecca',\n",
       " 7098: 'cunningham',\n",
       " 505: 'turns',\n",
       " 1056: 'gun',\n",
       " 7267: 'gus',\n",
       " 5565: 'gut',\n",
       " 232: 'guy',\n",
       " 5749: 'rapist',\n",
       " 5519: 'shares',\n",
       " 5345: 'shared',\n",
       " 1869: \"hadn't\",\n",
       " 5291: 'teaches',\n",
       " 1750: 'teacher',\n",
       " 5679: 'sending',\n",
       " 6893: 'franklin',\n",
       " 8432: 'plotted',\n",
       " 3563: 'regardless',\n",
       " 1727: 'extra',\n",
       " 8174: 'woefully',\n",
       " 8175: 'chip',\n",
       " 1503: 'lacks',\n",
       " 3772: 'discussion',\n",
       " 3199: 'kurt',\n",
       " 7924: 'chops',\n",
       " 1224: 'brain',\n",
       " 131: 'still',\n",
       " 2440: 'drop',\n",
       " 5235: 'challenged',\n",
       " 1243: 'yeah',\n",
       " 5459: 'challenges',\n",
       " 291: 'year',\n",
       " 8294: 'gibson',\n",
       " 4592: 'transition',\n",
       " 4917: 'suffice',\n",
       " 8606: 'romania',\n",
       " 8954: 'flipping',\n",
       " 5520: 'tomorrow',\n",
       " 6759: 'seymour',\n",
       " 4081: 'brains',\n",
       " 7099: 'professionals',\n",
       " 9888: 'transferred',\n",
       " 3517: 'importantly',\n",
       " 8433: 'premiered',\n",
       " 9663: 'teamed',\n",
       " 5566: 'burst',\n",
       " 6267: 'colours',\n",
       " 3001: 'madness',\n",
       " 8607: 'foreboding',\n",
       " 5750: 'inexplicable',\n",
       " 6471: 'exploit',\n",
       " 3389: 'charismatic',\n",
       " 8434: 'dictator',\n",
       " 3479: 'elvis',\n",
       " 6358: 'offbeat',\n",
       " 2061: 'develop',\n",
       " 1644: 'food',\n",
       " 341: 'death',\n",
       " 6359: 'earnest',\n",
       " 3200: 'fortune',\n",
       " 1314: 'fully',\n",
       " 6894: 'verbal',\n",
       " 3773: 'exposed',\n",
       " 9889: 'exposes',\n",
       " 4804: 'francis',\n",
       " 218: \"isn't\",\n",
       " 7925: 'pitched',\n",
       " 2281: 'freddy',\n",
       " 6799: 'fools',\n",
       " 338: 'poor',\n",
       " 3073: 'pool',\n",
       " 7184: 'corey',\n",
       " 9469: 'overseas',\n",
       " 670: 'robert',\n",
       " 4353: 'thoughtful',\n",
       " 1736: 'religious',\n",
       " 1197: 'decide',\n",
       " 1995: 'ass',\n",
       " 1986: 'streets',\n",
       " 9470: 'bass',\n",
       " 5850: 'excess',\n",
       " 4593: 'advertising',\n",
       " 9664: 'cathy',\n",
       " 3818: 'heroic',\n",
       " 6441: 'budgets',\n",
       " 7970: 'reject',\n",
       " 9471: 'surpasses',\n",
       " 7100: 'criticize',\n",
       " 6800: 'anytime',\n",
       " 9103: 'roommates',\n",
       " 3819: 'absence',\n",
       " 774: \"haven't\",\n",
       " 4850: 'ninja',\n",
       " 8295: 'bless',\n",
       " 2447: 'fairy',\n",
       " 1185: 'heavy',\n",
       " 9104: 'jolly',\n",
       " 1635: 'lord',\n",
       " 8609: 'earns',\n",
       " 5680: 'hapless',\n",
       " 298: 'american',\n",
       " 5567: 'visions',\n",
       " 2605: 'trapped',\n",
       " 1841: 'toward',\n",
       " 4851: 'randomly',\n",
       " 9472: 'organs',\n",
       " 4898: 'caliber',\n",
       " 5681: 'physics',\n",
       " 7185: 'stalked',\n",
       " 5682: 'phenomenon',\n",
       " 5851: 'stalker',\n",
       " 9105: 'heavens',\n",
       " 9890: 'competing',\n",
       " 9156: 'imitating',\n",
       " 5628: 'fluff',\n",
       " 3401: 'hype',\n",
       " 8435: 'locale',\n",
       " 3213: 'portrait',\n",
       " 5751: 'locals',\n",
       " 6197: 'abruptly',\n",
       " 2756: 'league',\n",
       " 586: \"wouldn't\",\n",
       " 1896: 'empty',\n",
       " 7926: 'juice',\n",
       " 1014: 'match',\n",
       " 2107: 'grant',\n",
       " 8077: 'sensual',\n",
       " 1758: 'grand',\n",
       " 7269: 'composition',\n",
       " 8176: 'classmates',\n",
       " 540: 'obviously',\n",
       " 3935: 'synopsis',\n",
       " 6801: 'reviewed',\n",
       " 2215: 'reviewer',\n",
       " 800: 'showing',\n",
       " 5405: 'sketch',\n",
       " 4047: 'lips',\n",
       " 949: 'towards',\n",
       " 3542: 'silence',\n",
       " 5406: 'alison',\n",
       " 9106: 'placing',\n",
       " 7843: 'ideals',\n",
       " 729: 'similar',\n",
       " 5175: 'ordered',\n",
       " 3564: 'fears',\n",
       " 2550: 'department',\n",
       " 5852: 'smiles',\n",
       " 1960: 'unfunny',\n",
       " 8234: 'riders',\n",
       " 979: 'telling',\n",
       " 9891: 'yourselves',\n",
       " 9107: 'watered',\n",
       " 1783: 'jump',\n",
       " 8786: 'notwithstanding',\n",
       " 1362: 'vampire',\n",
       " 2784: 'lugosi',\n",
       " 2595: 'clark',\n",
       " 1921: 'manage',\n",
       " 6544: 'clara',\n",
       " 370: 'camera',\n",
       " 8296: 'boards',\n",
       " 8068: 'meek',\n",
       " 6360: 'servants',\n",
       " 909: 'meet',\n",
       " 3661: 'pulling',\n",
       " 6442: 'sought',\n",
       " 4420: 'orson',\n",
       " 9473: 'rohmer',\n",
       " 9892: 'sentiments',\n",
       " 6268: 'ronald',\n",
       " 6443: 'scoop',\n",
       " 7482: 'favourites',\n",
       " 9665: \"its'\",\n",
       " 3436: 'university',\n",
       " 6444: 'slide',\n",
       " 318: 'special',\n",
       " 6445: 'butch',\n",
       " 6633: 'obsessive',\n",
       " 8069: 'darkly',\n",
       " 6361: 'jill',\n",
       " 211: 'times',\n",
       " 8177: 'timed',\n",
       " 5460: 'bitch',\n",
       " 4561: 'wrapped',\n",
       " 7701: 'hines',\n",
       " 8436: 'bastard',\n",
       " 3353: 'battles',\n",
       " 3025: 'mansion',\n",
       " 2448: 'repeated',\n",
       " 8437: 'manga',\n",
       " 9666: 'unfinished',\n",
       " 2249: 'sheriff',\n",
       " 9164: 'hector',\n",
       " 1199: 'won',\n",
       " 3201: 'cameos',\n",
       " 9275: 'inherited',\n",
       " 672: 'episodes',\n",
       " 3662: 'ken',\n",
       " 4562: 'kicking',\n",
       " 1317: 'key',\n",
       " 4317: 'limits',\n",
       " 7702: 'cena',\n",
       " 8297: 'troopers',\n",
       " 5407: 'controlled',\n",
       " 2558: 'surface',\n",
       " 8932: 'examined',\n",
       " 5752: 'http',\n",
       " 7927: 'riff',\n",
       " 9474: 'montages',\n",
       " 3437: 'increasingly',\n",
       " 3609: 'distant',\n",
       " 9835: 'gamut',\n",
       " 8070: 'disappearance',\n",
       " 5629: 'demonstrates',\n",
       " 8933: 'cradle',\n",
       " 6710: 'demonstrated',\n",
       " 6198: 'limitations',\n",
       " 7619: 'unhinged',\n",
       " 6446: 'nightclub',\n",
       " 1149: 'pointless',\n",
       " 5461: 'additional',\n",
       " 3235: 'gain',\n",
       " 4082: 'highest',\n",
       " 9893: 'kisses',\n",
       " 3936: 'beats',\n",
       " 4421: 'education',\n",
       " 8930: 'disasters',\n",
       " 7186: 'foil',\n",
       " 3202: 'consists',\n",
       " 806: 'sorry',\n",
       " 6991: 'void',\n",
       " 2570: 'suspenseful',\n",
       " 6992: 'herbert',\n",
       " 5684: 'unrelated',\n",
       " 6802: 'enhance',\n",
       " 7373: 'iturbi',\n",
       " 7101: 'kidnap',\n",
       " 7928: 'blandings',\n",
       " 5799: 'meg',\n",
       " 3774: 'mel',\n",
       " 349: 'men',\n",
       " 3717: 'mitchell',\n",
       " 7270: 'robertson',\n",
       " 4456: 'berlin',\n",
       " 7733: 'shahid',\n",
       " 673: 'room',\n",
       " 5370: 'roof',\n",
       " 102: 'movies',\n",
       " 5630: 'exceptions',\n",
       " 3663: 'root',\n",
       " 7803: 'titular',\n",
       " 2236: 'gordon',\n",
       " 3839: 'vicious',\n",
       " 840: 'third',\n",
       " 9108: 'fable',\n",
       " 8178: 'budding',\n",
       " 965: 'personal',\n",
       " 1051: 'crew',\n",
       " 6301: 'anil',\n",
       " 2221: 'combination',\n",
       " 31: 'one',\n",
       " 2738: 'forgot',\n",
       " 4422: 'aids',\n",
       " 1290: 'comedies',\n",
       " 7703: 'mandy',\n",
       " 1077: 'uses',\n",
       " 9667: 'floors',\n",
       " 9668: 'downside',\n",
       " 8179: 'gandolfini',\n",
       " 778: 'begins',\n",
       " 4386: 'mario',\n",
       " 2907: 'maria',\n",
       " 9109: 'zealand',\n",
       " 4049: 'mildred',\n",
       " 7704: 'testing',\n",
       " 6993: 'narrated',\n",
       " 5523: 'guaranteed',\n",
       " 4359: 'represented',\n",
       " 5800: 'quinn',\n",
       " 6269: 'mathieu',\n",
       " 1643: 'asks',\n",
       " 5978: 'entered',\n",
       " 1334: 'lovely',\n",
       " 1979: 'locations',\n",
       " 6994: 'lionel',\n",
       " 1558: 'ugly',\n",
       " 2488: 'cant',\n",
       " 4354: 'realizing',\n",
       " 9476: 'ella',\n",
       " 5872: 'programs',\n",
       " 3718: 'failing',\n",
       " 8298: 'reese',\n",
       " 6447: 'yours',\n",
       " 4943: 'assigned',\n",
       " 8438: 'fighters',\n",
       " 8299: 'goodman',\n",
       " 3289: 'stunts',\n",
       " 2516: 'nude',\n",
       " 2643: 'dean',\n",
       " 855: 'deal',\n",
       " 5080: 'deaf',\n",
       " 3214: 'dear',\n",
       " 2282: 'shakespeare',\n",
       " 5127: 'confrontation',\n",
       " 2655: 'afternoon',\n",
       " 5347: 'automatically',\n",
       " 180: 'down',\n",
       " 2559: 'narration',\n",
       " 3794: 'editor',\n",
       " 3565: 'creation',\n",
       " 1354: 'batman',\n",
       " 5128: 'landing',\n",
       " 6895: 'feminine',\n",
       " 5236: 'awhile',\n",
       " 1448: 'happening',\n",
       " 3905: 'pseudo',\n",
       " 4639: 'restored',\n",
       " 336: 'father',\n",
       " 9895: 'turgid',\n",
       " 3543: 'talked',\n",
       " 6995: 'targets',\n",
       " 1781: 'suspect',\n",
       " 953: 'box',\n",
       " 430: 'boy',\n",
       " 9477: 'maguire',\n",
       " 5631: 'bow',\n",
       " 8610: 'bon',\n",
       " 7804: 'boo',\n",
       " 2046: 'bob',\n",
       " 1667: 'teenage',\n",
       " 8300: 'transplant',\n",
       " 3465: 'drags',\n",
       " 2801: 'dennis',\n",
       " 4490: 'snl',\n",
       " 6448: 'blooded',\n",
       " 568: 'police',\n",
       " 6896: 'policy',\n",
       " 9669: 'tucker',\n",
       " 6803: 'lunch',\n",
       " 8787: 'elephants',\n",
       " 7483: 'ajay',\n",
       " 3334: 'stanwyck',\n",
       " 9110: 'carnival',\n",
       " 4852: 'frequent',\n",
       " 86: 'first',\n",
       " 8934: 'fleeing',\n",
       " 1386: 'speaking',\n",
       " 1842: 'kevin',\n",
       " 4640: 'complexity',\n",
       " 2414: 'shocked',\n",
       " 8301: 'shocker',\n",
       " 6305: '200',\n",
       " 6545: 'arguing',\n",
       " 5568: 'angst',\n",
       " 4355: 'harvey',\n",
       " 1766: 'russian',\n",
       " 2528: 'treasure',\n",
       " 4995: 'travesty',\n",
       " 4805: 'enthusiasm',\n",
       " 79: 'get',\n",
       " 9111: 'gee',\n",
       " 8611: 'gen',\n",
       " 1528: 'gem',\n",
       " 1316: 'london',\n",
       " 2224: 'seat',\n",
       " 9896: 'declares',\n",
       " 9276: 'seal',\n",
       " 594: 'wonder',\n",
       " 2349: 'satisfying',\n",
       " 6049: 'label',\n",
       " 7484: 'boundaries',\n",
       " 638: 'across',\n",
       " 6996: 'august',\n",
       " 1069: 'considering',\n",
       " 2250: 'capable',\n",
       " 432: 'sort',\n",
       " 3290: 'wake',\n",
       " 4208: 'hardcore',\n",
       " 2428: 'promising',\n",
       " 7285: 'rupert',\n",
       " 7805: \"o'brien\",\n",
       " 3844: 'extended',\n",
       " 9366: 'concentrates',\n",
       " 7485: 'northam',\n",
       " 9277: 'consisted',\n",
       " 6897: 'flavor',\n",
       " 5753: 'clueless',\n",
       " 4601: 'adams',\n",
       " 4387: 'passionate',\n",
       " 257: 'each',\n",
       " 5979: 'demonic',\n",
       " 7136: 'distracted',\n",
       " 6711: 'spice',\n",
       " 1856: 'vhs',\n",
       " 8302: 'examine',\n",
       " 6449: \"she'd\",\n",
       " 1400: 'hey',\n",
       " 442: \"she's\",\n",
       " 1206: 'u',\n",
       " 7705: 'motel',\n",
       " 7187: 'plodding',\n",
       " 1138: 'former',\n",
       " 9670: 'paperhouse',\n",
       " 7583: 'strung',\n",
       " 1456: 'zero',\n",
       " 3994: 'newspaper',\n",
       " 991: 'masterpiece',\n",
       " 4899: 'mentions',\n",
       " 2415: 'africa',\n",
       " 5237: 'tacky',\n",
       " 3953: 'engaged',\n",
       " 4209: 'mill',\n",
       " 534: 'hour',\n",
       " 2283: 'recall',\n",
       " 1870: 'sucks',\n",
       " 2416: 'remain',\n",
       " 9278: 'stubborn',\n",
       " 2108: 'ford',\n",
       " 8071: 'colman',\n",
       " 5041: 'biography',\n",
       " 6071: 'homicide',\n",
       " 738: 'needs',\n",
       " 1421: 'acts',\n",
       " 9788: 'sacred',\n",
       " 5685: 'kitty',\n",
       " 8439: 'sophistication',\n",
       " 4143: 'brady',\n",
       " 2785: 'dragon',\n",
       " 5348: 'heartfelt',\n",
       " 6450: 'appeals',\n",
       " 1717: 'comedic',\n",
       " 9671: 'compound',\n",
       " 797: 'viewers',\n",
       " 736: 'mystery',\n",
       " 5686: 'repeating',\n",
       " 9478: 'rhys',\n",
       " 1728: 'engaging',\n",
       " 9999: 'edged',\n",
       " 4050: 'perry',\n",
       " 3664: \"they'll\",\n",
       " 2802: 'extraordinary',\n",
       " 6712: 'backed',\n",
       " 350: 'top',\n",
       " 6997: 'razor',\n",
       " 9898: 'mercilessly',\n",
       " 5853: 'ton',\n",
       " 827: 'tom',\n",
       " 8440: 'lifts',\n",
       " 3518: 'godfather',\n",
       " 9672: 'eggs',\n",
       " 1382: 'charm',\n",
       " 7272: 'services',\n",
       " 7584: 'rebels',\n",
       " 8303: 'chock',\n",
       " 6451: 'ebert',\n",
       " 863: 'premise',\n",
       " 2189: 'foreign',\n",
       " 213: 'point',\n",
       " 3269: 'expensive',\n",
       " 7930: 'screened',\n",
       " 8022: 'variation',\n",
       " 5854: 'politician',\n",
       " 5633: 'widescreen',\n",
       " 1117: 'century',\n",
       " 7931: 'tashan',\n",
       " 8912: '1965',\n",
       " 7170: '1967',\n",
       " 3296: 'knock',\n",
       " 6393: 'foolish',\n",
       " 7585: 'candle',\n",
       " 151: 'though',\n",
       " 8803: 'manipulate',\n",
       " 4595: 'abusive',\n",
       " 9673: 'underused',\n",
       " 2693: 'murphy',\n",
       " 9480: 'stripped',\n",
       " 8304: 'scarlet',\n",
       " 4318: 'cure',\n",
       " 8788: 'stripper',\n",
       " 1254: 'utterly',\n",
       " 6270: 'implied',\n",
       " 2265: 'portraying',\n",
       " 5292: 'literary',\n",
       " 5754: 'pleasing',\n",
       " 436: 'entire',\n",
       " 6998: 'rivers',\n",
       " 1922: 'fat',\n",
       " 9112: 'archer',\n",
       " 8935: 'healing',\n",
       " 3954: 'escaped',\n",
       " 2722: 'closing',\n",
       " 9279: 'didnt',\n",
       " 2386: 'continuity',\n",
       " 7188: 'varied',\n",
       " 1777: 'holds',\n",
       " 4984: 'suspend',\n",
       " 7486: 'profile',\n",
       " 106: 'watch',\n",
       " 9369: 'incompetence',\n",
       " 3187: 'chasing',\n",
       " 7706: 'subsequently',\n",
       " 8789: 'grendel',\n",
       " 5081: 'grayson',\n",
       " 4144: 'blatant',\n",
       " 4764: 'lively',\n",
       " 861: 'sexual',\n",
       " 5042: 'yard',\n",
       " 8441: 'yarn',\n",
       " 4231: 'reaches',\n",
       " 3820: 'reached',\n",
       " 8442: 'spelling',\n",
       " 7189: 'sweden',\n",
       " 28: 'have',\n",
       " 4766: 'prisoner',\n",
       " 3497: 'disease',\n",
       " 4083: 'occasion',\n",
       " 3420: 'hamlet',\n",
       " 1857: 'knowledge',\n",
       " 3203: 'perfection',\n",
       " 6271: 'teams',\n",
       " 4901: 'showdown',\n",
       " 6362: 'bruno',\n",
       " 9674: 'incarnation',\n",
       " 3871: 'antics',\n",
       " 7487: 'russ',\n",
       " 975: 'joke',\n",
       " 3215: 'equal',\n",
       " 7707: 'fassbinder',\n",
       " 45: \"it's\",\n",
       " 9899: \"it'd\",\n",
       " 9113: 'locales',\n",
       " 7932: 'meredith',\n",
       " 5129: 'frustrating',\n",
       " 2592: 'powell',\n",
       " 9900: 'exceedingly',\n",
       " 5687: 'stores',\n",
       " 5178: 'griffith',\n",
       " 6272: 'resolved',\n",
       " 7933: 'doyle',\n",
       " 40: 'like',\n",
       " 5755: 'vibrant',\n",
       " 6999: 'admitted',\n",
       " 2284: 'chick',\n",
       " 1153: 'hair',\n",
       " 5492: 'recommendation',\n",
       " 7273: 'hysterically',\n",
       " 9280: 'lieutenant',\n",
       " 8443: 'uptight',\n",
       " 4319: 'introduces',\n",
       " 3312: 'rushed',\n",
       " 9481: 'rushes',\n",
       " 7423: \"'80s\",\n",
       " 6804: 'coke',\n",
       " 7808: 'flip',\n",
       " 9114: 'thorn',\n",
       " 5601: 'circus',\n",
       " 1810: 'dressed',\n",
       " 1589: 'detail',\n",
       " 5350: 'dresses',\n",
       " 5569: \"daughter's\",\n",
       " 9901: \"ted's\",\n",
       " 6452: 'harriet',\n",
       " 1504: 'direct',\n",
       " 4423: 'nail',\n",
       " 824: 'doubt',\n",
       " 6546: 'selected',\n",
       " 3054: 'revolves',\n",
       " 8180: 'revolver',\n",
       " 8181: 'liberty',\n",
       " 889: 'leaves',\n",
       " 321: 'excellent',\n",
       " 7809: 'salvage',\n",
       " 3519: 'estate',\n",
       " 5688: 'attract',\n",
       " 8471: 'ceremony',\n",
       " 5855: 'keen',\n",
       " 8445: 'drummer',\n",
       " 2786: 'description',\n",
       " 9902: 'insecure',\n",
       " 4680: 'parallel',\n",
       " 8446: 'amid',\n",
       " 6805: 'upside',\n",
       " 2880: 'succeeds',\n",
       " 7190: 'detroit',\n",
       " 4700: 'newly',\n",
       " 5756: 'independence',\n",
       " 8182: 'associate',\n",
       " 504: 'days',\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross check step to see index to word mapping\n",
    "idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This step is visual step where encoding in the dataset is convrted back to readable or alphabetic form \n",
    "#Here each integer in the text is matched with the index, and replaced by the corresponding word\n",
    "\n",
    "def decode(text):\n",
    "    return ' '.join([idx2word.get(i,\"?\") for i in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "#Provide values and it return decoded output based on decode function\n",
    "k=decode(x_train[0])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL BUILDING:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NORMAL NN WITH SEQ LEN 20 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding train and test set with max sequence length of 20 \n",
    "SEQ_LEN=20\n",
    "X_train = pad_sequences(x_train, maxlen=SEQ_LEN, padding='post', truncating=\"post\")\n",
    "X_test =  pad_sequences(x_test, maxlen=SEQ_LEN, padding='post', truncating=\"post\")\n",
    "\n",
    "#Note Padding is post padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 20)\n",
      "(25000, 20)\n"
     ]
    }
   ],
   "source": [
    "#Cross check step to see shape of both test and train set\n",
    "#here it should be 20 in shape[1]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "print(type(X_train))\n",
    "print(type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,  591,  202, ..., 5760,  394,  354],\n",
       "       [   1,   14,   22, ...,  114,    9,   55],\n",
       "       [   1,  111,  748, ...,  498, 5076,  748],\n",
       "       ...,\n",
       "       [   1,   13, 1408, ...,   45,  184,   78],\n",
       "       [   1,   11,  119, ...,   86,  107,    8],\n",
       "       [   1,    6,   52, ...,   47,    6, 3482]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross check step to see test values\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   14,   22, ...,  256,    5,   25],\n",
       "       [   1,  194, 1153, ...,  118, 1634,   14],\n",
       "       [   1,   14,   47, ...,   71,  149,   14],\n",
       "       ...,\n",
       "       [   1,   11,    6, ...,    4,  912,   84],\n",
       "       [   1, 1446, 7079, ...,   54,  349,   11],\n",
       "       [   1,   17,    6, ...,  270,    2,    5]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross check step to see test values\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "#Provide values and it return decoded output based on decode function\n",
    "k=decode(X_train[0])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> big hair big boobs bad music and a giant safety pin these are the words to best describe this\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "#Provide values and it return decoded output based on decode function\n",
    "k=decode(X_train[1])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this has to be one of the worst films of the 1990s when my friends i were watching this\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "#Provide values and it return decoded output based on decode function\n",
    "k=decode(X_train[2])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL NN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sarth\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "#Model buliding\n",
    "model = Sequential()  #Sequential model\n",
    "\n",
    "model.add(Embedding(MAX_VOCAB, EMBEDDING_DIM, input_length=SEQ_LEN)) #Init embedding layer with no pretrained wts\n",
    "#embedding layer takes input of vocabulary size i.e 10000, embedding dimension i.e 50 and input sequence i.e number of columns of dataset i.e 20\n",
    "model.add(Flatten()) #Use flatten layer\n",
    "model.add(Dropout(0.5))#use dropout for regularization\n",
    "model.add(Dense(10)) #Hidden layer\n",
    "model.add(Dropout(0.3))  #use dropout for regularization\n",
    "model.add(Dense(1, activation='sigmoid')) #Output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxNDNhrseCzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 20, 50)            500000    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                10010     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 510,021\n",
      "Trainable params: 510,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#To see what our model have, number of layer , output shape ,etc\n",
    "#model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3CSVVPPeCzD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sarth\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#Compile model with optimizer adam, loss as binary cross entropy and metric is accuracy\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 20)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross check step to see shape of input to our model\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sarth\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 7s 281us/step - loss: 0.6678 - accuracy: 0.5866 - val_loss: 0.6025 - val_accuracy: 0.6713\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 7s 280us/step - loss: 0.5222 - accuracy: 0.7432 - val_loss: 0.5621 - val_accuracy: 0.6970\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 7s 273us/step - loss: 0.3994 - accuracy: 0.8264 - val_loss: 0.6042 - val_accuracy: 0.6898\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 7s 288us/step - loss: 0.2912 - accuracy: 0.8842 - val_loss: 0.6765 - val_accuracy: 0.6783\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 7s 262us/step - loss: 0.2089 - accuracy: 0.9213 - val_loss: 0.7787 - val_accuracy: 0.6711\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 7s 270us/step - loss: 0.1584 - accuracy: 0.9412 - val_loss: 0.8733 - val_accuracy: 0.6695\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 6s 250us/step - loss: 0.1282 - accuracy: 0.9523 - val_loss: 0.9589 - val_accuracy: 0.6641\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 9s 351us/step - loss: 0.1035 - accuracy: 0.9617 - val_loss: 1.0516 - val_accuracy: 0.6654\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 7s 265us/step - loss: 0.0902 - accuracy: 0.9670 - val_loss: 1.1338 - val_accuracy: 0.6593\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 7s 283us/step - loss: 0.0813 - accuracy: 0.9705 - val_loss: 1.2030 - val_accuracy: 0.6603\n"
     ]
    }
   ],
   "source": [
    "#Fitting model to xtrain and ytrain with defined epochs and batch size\n",
    "r = model.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_data=(X_test,y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 45us/step\n",
      "Test accuracy:  0.6602799892425537\n"
     ]
    }
   ],
   "source": [
    "#Evaluate test set and then print accuracy\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 41us/step\n",
      "Test accuracy:  0.9976400136947632\n"
     ]
    }
   ],
   "source": [
    "#Evaluate train set and then print accuracy\n",
    "results = model.evaluate(X_train, y_train)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "After seeing model performance on dataset with sequence length of 20(which is very less as you cannot get all details in review). The model under performed, the accuarcy is very low and the model was going into overfit zone.\n",
    "Now lets increase sequence length based on the gaph in visualize steps and some calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN WITH SEQ LEN 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To create list which contain all the sequence length of each review\n",
    "k=[len(i) for i in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[218, 189, 141, 550, 147, 43, 123, 562, 233, 130, 450, 99, 117, 238, 109, 129, 163, 752, 212, 177, 129, 140, 256, 888, 93, 142, 220, 193, 171, 221, 174, 647, 233, 162, 597, 234, 51, 336, 139, 231, 704, 142, 861, 132, 122, 570, 55, 214, 103, 186, 113, 169, 469, 138, 302, 766, 351, 146, 59, 206, 107, 152, 186, 431, 147, 684, 383, 324, 252, 263, 787, 211, 314, 118, 390, 132, 710, 306, 167, 115, 95, 158, 156, 82, 502, 314, 190, 174, 60, 145, 214, 659, 408, 515, 461, 202, 238, 170, 107, 171, 158, 145, 790, 258, 287, 67, 123, 975, 775, 236, 195, 274, 214, 91, 1038, 815, 183, 206, 50, 118, 147, 141, 60, 56, 439, 439, 213, 144, 533, 303, 203, 563, 129, 153, 55, 92, 174, 187, 183, 165, 78, 198, 156, 223, 127, 61, 362, 84, 57, 176, 159, 57, 159, 165, 213, 194, 149, 130, 203, 19, 98, 466, 525, 130, 322, 153, 408, 215, 472, 143, 136, 354, 260, 319, 125, 209, 282, 810, 142, 240, 148, 198, 193, 123, 128, 103, 479, 345, 263, 165, 205, 333, 184, 92, 177, 335, 120, 121, 259, 180, 160, 114, 59, 343, 513, 133, 206, 152, 206, 572, 153, 139, 151, 129, 129, 196, 433, 199, 140, 311, 151, 200, 584, 127, 513, 781, 932, 526, 161, 646, 135, 52, 267, 174, 185, 219, 81, 219, 131, 153, 270, 644, 155, 546, 284, 85, 293, 155, 358, 45, 231, 124, 178, 118, 260, 393, 127, 157, 107, 322, 188, 126, 155, 294, 249, 177, 138, 215, 263, 132, 150, 217, 188, 385, 199, 127, 325, 161, 140, 215, 240, 230, 327, 129, 113, 225, 87, 496, 234, 311, 215, 111, 102, 110, 165, 839, 296, 130, 104, 274, 229, 235, 653, 468, 578, 139, 315, 65, 178, 836, 164, 239, 212, 297, 258, 157, 78, 544, 152, 120, 208, 163, 226, 304, 195, 454, 121, 175, 617, 320, 121, 245, 655, 114, 131, 104, 238, 138, 164, 52, 215, 87, 471, 142, 289, 106, 141, 239, 412, 154, 175, 828, 41, 144, 525, 176, 551, 251, 621, 159, 75, 207, 80, 94, 78, 449, 622, 157, 85, 260, 1011, 444, 326, 586, 118, 270, 360, 95, 640, 315, 138, 573, 434, 313, 128, 1007, 130, 257, 209, 159, 602, 109, 250, 117, 149, 199, 55, 578, 158, 323, 486, 177, 73, 127, 138, 130, 110, 283, 244, 375, 137, 156, 153, 112, 94, 137, 195, 137, 112, 25, 106, 105, 272, 857, 116, 58, 114, 71, 57, 292, 56, 157, 283, 87, 327, 481, 918, 129, 181, 214, 601, 215, 117, 112, 401, 165, 154, 587, 417, 345, 233, 496, 403, 317, 189, 340, 195, 370, 194, 150, 559, 48, 129, 110, 45, 173, 674, 416, 233, 146, 73, 146, 190, 130, 127, 217, 785, 227, 119, 149, 150, 175, 588, 130, 414, 172, 523, 119, 130, 122, 219, 264, 202, 218, 367, 230, 429, 985, 144, 128, 60, 78, 125, 131, 186, 283, 121, 226, 82, 57, 468, 336, 218, 110, 535, 603, 147, 117, 156, 72, 72, 69, 529, 148, 56, 263, 202, 348, 172, 104, 212, 146, 191, 251, 179, 722, 156, 141, 235, 97, 69, 222, 228, 233, 46, 130, 599, 146, 71, 144, 132, 89, 115, 267, 100, 148, 197, 627, 161, 263, 447, 97, 132, 357, 52, 329, 149, 253, 330, 549, 166, 190, 165, 202, 351, 633, 942, 123, 121, 184, 270, 139, 248, 382, 292, 215, 439, 177, 42, 173, 173, 233, 480, 377, 48, 177, 192, 84, 176, 150, 467, 210, 687, 631, 279, 136, 67, 167, 170, 359, 451, 135, 197, 1009, 87, 241, 492, 336, 88, 170, 425, 459, 331, 199, 158, 69, 121, 116, 120, 297, 173, 293, 103, 477, 723, 133, 413, 109, 101, 227, 141, 939, 199, 162, 377, 172, 532, 68, 41, 130, 83, 136, 787, 117, 165, 111, 124, 552, 172, 130, 96, 153, 181, 49, 119, 88, 100, 297, 363, 632, 218, 168, 100, 212, 120, 84, 135, 268, 127, 746, 127, 236, 335, 193, 843, 251, 165, 210, 239, 311, 214, 29, 334, 168, 127, 169, 110, 164, 147, 205, 133, 188, 125, 183, 326, 150, 290, 214, 328, 51, 343, 212, 454, 96, 471, 82, 523, 114, 147, 200, 387, 174, 150, 103, 187, 183, 115, 144, 131, 241, 57, 46, 23, 419, 227, 110, 91, 66, 137, 104, 164, 417, 127, 78, 181, 179, 181, 261, 243, 205, 155, 166, 180, 77, 440, 284, 328, 258, 61, 193, 39, 156, 124, 108, 322, 376, 162, 359, 139, 302, 646, 44, 253, 244, 438, 432, 226, 236, 336, 166, 444, 195, 74, 127, 213, 195, 148, 174, 148, 149, 136, 77, 53, 151, 215, 129, 196, 135, 146, 235, 166, 359, 132, 167, 141, 188, 365, 138, 118, 110, 309, 261, 127, 76, 900, 211, 273, 233, 149, 212, 302, 268, 295, 212, 143, 467, 256, 230, 129, 145, 119, 83, 322, 208, 132, 149, 278, 136, 198, 164, 349, 200, 278, 439, 130, 161, 223, 561, 397, 302, 282, 187, 192, 107, 361, 126, 201, 79, 415, 133, 300, 124, 355, 978, 323, 114, 85, 143, 221, 190, 215, 244, 626, 64, 127, 180, 128, 206, 221, 261, 122, 201, 221, 139, 315, 356, 254, 131, 128, 234, 129, 66, 134, 121, 470, 373, 129, 127, 92, 127, 223, 179, 651, 452, 935, 146, 170, 438, 94, 159, 312, 302, 128, 121, 280, 622, 166, 126, 157, 353, 45, 132, 324, 65, 304, 149, 243, 117, 719, 164, 116, 61, 368, 407, 73, 571, 170, 303, 159, 42, 400, 295, 183, 75, 215, 255, 229, 445, 1000, 494, 128, 580, 130, 59, 189, 526, 116, 114, 544, 118, 111, 420, 250, 258, 188, 139, 89, 135, 58, 173, 84, 150, 170, 159, 152, 170, 74, 213, 41, 250, 124, 533, 84, 186, 126, 183, 125, 153, 282, 146, 149, 215, 122, 68, 139, 128, 113, 252, 168, 434, 298, 181, 216, 152, 99, 182, 135, 169, 103, 109, 218, 62, 1022, 546, 313, 261, 248, 57, 100, 544, 290, 197, 132, 170, 151, 707, 263, 142, 164, 141, 371, 298, 324, 142, 237, 244, 40, 230, 158, 142, 379, 124, 178, 108, 149, 741, 125, 410, 128, 243, 179, 138, 610, 542, 90, 187, 172, 186, 106, 27, 237, 98, 140, 125, 245, 147, 137, 84, 264, 147, 257, 220, 82, 159, 239, 149, 157, 160, 183, 185, 253, 230, 167, 105, 218, 85, 367, 209, 66, 180, 253, 214, 219, 320, 200, 216, 112, 184, 173, 499, 196, 340, 308, 144, 240, 72, 657, 753, 164, 925, 169, 694, 395, 144, 152, 142, 596, 431, 160, 985, 143, 138, 171, 215, 110, 140, 486, 267, 344, 129, 162, 88, 116, 150, 232, 104, 39, 932, 209, 207, 344, 153, 220, 183, 310, 328, 196, 284, 232, 203, 135, 122, 271, 132, 133, 242, 593, 319, 331, 152, 160, 284, 191, 409, 198, 171, 159, 297, 164, 124, 184, 139, 356, 296, 184, 633, 656, 156, 170, 223, 98, 273, 37, 60, 166, 679, 123, 89, 168, 189, 247, 110, 633, 642, 133, 182, 134, 125, 133, 84, 111, 179, 185, 149, 324, 517, 174, 186, 716, 959, 129, 130, 147, 122, 180, 167, 142, 240, 151, 117, 196, 137, 171, 199, 135, 153, 197, 209, 253, 132, 133, 137, 232, 138, 212, 171, 790, 51, 97, 71, 118, 49, 1009, 149, 255, 229, 123, 204, 469, 201, 110, 228, 133, 467, 318, 211, 107, 132, 179, 304, 239, 114, 186, 187, 261, 137, 263, 132, 154, 159, 254, 147, 502, 202, 117, 150, 117, 204, 144, 135, 215, 478, 919, 141, 129, 125, 341, 189, 496, 107, 278, 54, 55, 270, 129, 145, 254, 278, 216, 168, 340, 193, 360, 421, 339, 138, 132, 143, 356, 137, 270, 199, 540, 240, 131, 183, 250, 131, 32, 190, 161, 71, 324, 383, 622, 364, 238, 196, 98, 76, 119, 128, 169, 148, 154, 143, 60, 178, 305, 208, 93, 197, 180, 101, 227, 204, 278, 278, 191, 429, 153, 192, 811, 208, 137, 529, 147, 67, 141, 230, 49, 137, 117, 235, 134, 128, 205, 92, 504, 135, 52, 189, 262, 96, 345, 244, 194, 702, 256, 857, 186, 200, 66, 263, 155, 52, 872, 116, 315, 536, 119, 40, 745, 222, 139, 278, 271, 730, 118, 131, 158, 121, 426, 144, 112, 266, 311, 831, 425, 83, 269, 256, 191, 166, 798, 255, 186, 167, 141, 174, 142, 73, 105, 175, 144, 130, 988, 101, 163, 107, 144, 298, 214, 118, 130, 124, 135, 558, 292, 173, 119, 282, 534, 205, 42, 99, 50, 146, 67, 206, 145, 255, 223, 425, 198, 727, 254, 204, 155, 174, 117, 160, 321, 146, 105, 123, 183, 189, 145, 209, 120, 193, 158, 163, 131, 109, 267, 209, 116, 543, 466, 134, 57, 171, 358, 172, 124, 58, 950, 323, 205, 174, 368, 290, 156, 184, 465, 58, 373, 97, 366, 140, 177, 127, 57, 295, 608, 488, 599, 338, 140, 211, 384, 281, 130, 172, 89, 129, 736, 142, 158, 357, 211, 382, 113, 121, 248, 145, 639, 96, 83, 324, 183, 559, 335, 634, 341, 198, 291, 323, 306, 99, 193, 193, 177, 81, 252, 277, 424, 155, 315, 445, 113, 68, 431, 173, 188, 125, 177, 261, 144, 439, 277, 52, 128, 254, 69, 275, 198, 277, 317, 321, 100, 173, 110, 242, 453, 679, 576, 206, 475, 272, 334, 207, 82, 289, 126, 122, 122, 59, 63, 125, 414, 131, 261, 184, 210, 127, 167, 134, 414, 141, 175, 263, 118, 154, 96, 206, 166, 151, 119, 133, 184, 430, 409, 153, 211, 228, 402, 61, 346, 185, 186, 149, 197, 141, 142, 233, 124, 201, 252, 152, 128, 297, 129, 357, 142, 252, 125, 155, 165, 306, 125, 314, 61, 189, 164, 92, 225, 240, 288, 146, 373, 543, 153, 117, 418, 190, 151, 723, 43, 52, 236, 163, 318, 170, 65, 405, 233, 144, 148, 190, 170, 201, 631, 127, 80, 74, 369, 127, 140, 133, 542, 133, 184, 136, 699, 231, 265, 214, 347, 302, 209, 59, 108, 143, 760, 147, 909, 224, 114, 134, 254, 112, 104, 319, 621, 182, 232, 166, 122, 138, 400, 365, 186, 153, 114, 151, 450, 702, 155, 230, 214, 182, 100, 469, 202, 335, 131, 358, 124, 156, 319, 689, 171, 291, 48, 261, 54, 314, 234, 156, 316, 962, 430, 211, 253, 256, 135, 365, 325, 115, 125, 171, 228, 127, 182, 105, 235, 329, 113, 174, 150, 162, 123, 218, 333, 140, 402, 81, 293, 205, 190, 148, 292, 112, 664, 411, 161, 347, 424, 160, 181, 210, 114, 359, 113, 220, 300, 248, 636, 178, 259, 178, 1015, 378, 152, 924, 475, 997, 279, 145, 148, 125, 136, 65, 107, 124, 139, 184, 135, 410, 256, 148, 122, 83, 56, 462, 126, 61, 101, 121, 271, 1010, 178, 110, 41, 159, 447, 185, 183, 90, 150, 524, 172, 184, 149, 121, 215, 82, 245, 139, 55, 184, 115, 134, 335, 627, 203, 267, 228, 173, 196, 130, 271, 179, 49, 67, 138, 112, 61, 88, 152, 64, 124, 360, 127, 240, 158, 346, 133, 872, 73, 212, 206, 141, 191, 325, 207, 155, 175, 423, 269, 128, 645, 310, 161, 110, 114, 331, 525, 125, 50, 139, 243, 168, 260, 119, 65, 795, 158, 124, 235, 116, 281, 116, 177, 838, 96, 68, 103, 189, 242, 135, 201, 137, 384, 198, 278, 89, 123, 227, 854, 129, 488, 115, 63, 80, 50, 128, 179, 285, 214, 173, 325, 147, 228, 495, 125, 223, 216, 541, 708, 42, 165, 730, 132, 128, 347, 233, 247, 446, 304, 337, 438, 130, 511, 175, 101, 198, 114, 160, 128, 91, 148, 563, 43, 159, 115, 56, 391, 149, 340, 363, 152, 187, 204, 133, 624, 136, 116, 375, 169, 156, 477, 134, 165, 138, 129, 201, 790, 165, 182, 114, 178, 124, 141, 417, 379, 55, 109, 229, 53, 166, 135, 200, 183, 169, 202, 630, 137, 80, 136, 151, 596, 624, 196, 475, 356, 308, 203, 91, 151, 127, 124, 89, 125, 487, 221, 410, 183, 299, 354, 238, 167, 161, 150, 339, 415, 152, 330, 207, 123, 212, 124, 136, 134, 350, 125, 370, 150, 438, 121, 156, 283, 132, 140, 229, 125, 113, 191, 245, 135, 751, 54, 240, 230, 148, 174, 389, 390, 1011, 146, 535, 245, 65, 160, 141, 197, 245, 166, 146, 128, 84, 110, 118, 157, 318, 50, 263, 258, 73, 120, 371, 197, 146, 226, 207, 76, 169, 426, 312, 652, 121, 138, 156, 397, 116, 57, 645, 203, 16, 353, 70, 125, 164, 258, 396, 185, 110, 148, 127, 163, 117, 439, 138, 386, 114, 37, 175, 1034, 238, 125, 52, 406, 640, 131, 526, 180, 75, 190, 182, 267, 126, 191, 697, 162, 371, 155, 276, 257, 152, 351, 80, 139, 133, 143, 75, 112, 226, 112, 186, 145, 209, 315, 120, 758, 93, 220, 128, 137, 129, 328, 648, 443, 159, 125, 39, 160, 456, 402, 48, 112, 91, 142, 125, 758, 835, 228, 188, 185, 70, 667, 128, 300, 157, 392, 229, 442, 64, 123, 49, 47, 680, 139, 92, 166, 221, 234, 724, 119, 263, 462, 159, 120, 105, 188, 115, 184, 419, 155, 292, 1002, 447, 113, 130, 240, 147, 216, 136, 150, 119, 305, 250, 64, 547, 142, 186, 269, 45, 592, 50, 167, 141, 345, 158, 162, 541, 314, 349, 148, 143, 317, 152, 89, 383, 468, 238, 131, 160, 325, 91, 122, 111, 317, 85, 213, 193, 141, 46, 161, 147, 300, 322, 149, 90, 235, 591, 334, 126, 239, 136, 153, 173, 182, 113, 150, 161, 229, 180, 395, 125, 436, 569, 302, 168, 129, 495, 406, 297, 279, 205, 197, 122, 244, 112, 466, 139, 128, 136, 184, 122, 200, 78, 122, 268, 151, 124, 162, 479, 155, 400, 201, 131, 148, 155, 343, 135, 337, 137, 95, 436, 247, 158, 179, 259, 120, 693, 144, 196, 105, 406, 804, 142, 611, 414, 269, 172, 133, 142, 645, 475, 262, 132, 167, 128, 167, 108, 504, 70, 150, 216, 129, 90, 48, 493, 459, 635, 233, 486, 301, 256, 205, 178, 120, 240, 317, 120, 157, 141, 140, 27, 538, 47, 73, 203, 124, 100, 167, 173, 83, 117, 128, 39, 997, 167, 141, 193, 127, 353, 122, 150, 735, 501, 396, 160, 160, 124, 104, 119, 50, 179, 314, 77, 195, 348, 119, 373, 172, 140, 155, 532, 247, 246, 454, 255, 483, 125, 362, 142, 117, 49, 106, 120, 169, 526, 514, 241, 124, 67, 510, 312, 105, 273, 131, 145, 822, 249, 596, 166, 151, 436, 161, 1629, 192, 139, 125, 179, 114, 265, 166, 219, 353, 130, 159, 124, 210, 321, 407, 59, 102, 260, 313, 262, 166, 124, 279, 113, 123, 76, 350, 163, 51, 152, 261, 90, 205, 518, 98, 146, 194, 106, 502, 567, 138, 188, 123, 290, 485, 507, 150, 139, 152, 354, 65, 62, 300, 139, 718, 153, 848, 130, 131, 259, 178, 631, 149, 141, 130, 319, 241, 84, 190, 242, 1036, 133, 86, 202, 173, 497, 104, 229, 483, 102, 249, 327, 436, 56, 236, 328, 454, 49, 232, 382, 475, 180, 125, 323, 350, 106, 175, 272, 227, 146, 117, 134, 123, 182, 395, 413, 103, 245, 259, 188, 207, 117, 382, 112, 244, 148, 174, 77, 222, 107, 605, 74, 295, 158, 498, 469, 117, 51, 274, 149, 61, 118, 457, 265, 209, 45, 229, 349, 595, 445, 158, 284, 111, 146, 158, 468, 54, 538, 237, 164, 237, 197, 593, 115, 111, 122, 250, 163, 224, 275, 659, 336, 201, 197, 601, 104, 185, 435, 538, 128, 167, 234, 220, 335, 398, 46, 491, 809, 163, 108, 746, 126, 175, 116, 433, 220, 823, 153, 135, 110, 52, 644, 231, 485, 53, 338, 334, 176, 173, 144, 165, 61, 226, 481, 122, 159, 164, 470, 829, 143, 105, 416, 315, 197, 408, 362, 254, 331, 68, 107, 54, 534, 282, 162, 71, 688, 133, 71, 122, 116, 178, 659, 223, 140, 303, 423, 191, 989, 333, 136, 832, 1021, 385, 139, 164, 277, 232, 999, 170, 201, 173, 147, 158, 167, 55, 116, 44, 708, 195, 214, 123, 123, 328, 402, 51, 110, 243, 223, 204, 211, 150, 240, 146, 211, 134, 370, 475, 43, 188, 149, 292, 350, 539, 156, 249, 189, 162, 155, 102, 51, 130, 168, 588, 295, 146, 265, 231, 67, 695, 193, 155, 302, 137, 422, 309, 58, 184, 133, 133, 148, 126, 436, 103, 154, 95, 127, 111, 782, 279, 362, 119, 140, 171, 820, 151, 64, 253, 161, 171, 370, 199, 120, 96, 307, 169, 145, 342, 210, 344, 193, 294, 52, 660, 56, 271, 760, 85, 129, 124, 467, 110, 495, 502, 223, 256, 47, 121, 149, 375, 118, 850, 247, 109, 132, 130, 176, 300, 200, 121, 147, 319, 727, 176, 177, 202, 187, 128, 220, 126, 161, 49, 176, 144, 1000, 218, 108, 706, 186, 151, 210, 191, 118, 142, 114, 202, 84, 132, 121, 133, 120, 694, 124, 72, 257, 297, 162, 121, 186, 133, 633, 136, 186, 34, 125, 107, 143, 143, 325, 949, 207, 41, 194, 607, 641, 497, 91, 201, 553, 174, 306, 123, 514, 238, 171, 168, 751, 518, 286, 137, 421, 457, 118, 542, 116, 69, 348, 412, 241, 352, 139, 53, 487, 356, 85, 575, 615, 157, 86, 247, 625, 696, 236, 253, 361, 123, 201, 293, 239, 167, 53, 64, 364, 103, 431, 475, 47, 352, 106, 158, 149, 162, 773, 115, 137, 101, 148, 222, 175, 227, 164, 193, 121, 147, 501, 300, 208, 276, 277, 131, 134, 371, 40, 391, 403, 74, 146, 912, 135, 266, 163, 149, 100, 522, 89, 190, 140, 428, 99, 365, 98, 525, 96, 127, 117, 160, 351, 118, 126, 247, 284, 598, 211, 130, 241, 135, 368, 323, 220, 79, 153, 177, 301, 103, 115, 139, 263, 314, 121, 140, 388, 194, 484, 162, 217, 324, 168, 154, 748, 515, 186, 287, 287, 255, 512, 331, 80, 203, 138, 192, 578, 468, 361, 117, 275, 156, 285, 138, 122, 203, 135, 153, 160, 329, 161, 229, 117, 65, 430, 143, 606, 143, 118, 232, 221, 335, 155, 206, 121, 136, 295, 221, 674, 506, 120, 166, 111, 182, 161, 116, 355, 168, 143, 292, 320, 543, 610, 160, 390, 242, 236, 56, 957, 264, 69, 180, 154, 173, 563, 173, 750, 658, 137, 303, 101, 127, 164, 135, 783, 61, 195, 287, 679, 172, 110, 154, 111, 473, 245, 484, 263, 258, 280, 186, 108, 137, 250, 73, 137, 119, 91, 344, 283, 760, 259, 415, 520, 317, 164, 317, 499, 218, 292, 154, 165, 443, 202, 129, 347, 64, 150, 138, 80, 403, 149, 116, 223, 158, 137, 161, 155, 285, 180, 124, 424, 199, 266, 275, 143, 174, 193, 660, 69, 171, 321, 232, 125, 119, 473, 158, 244, 49, 125, 47, 333, 186, 141, 203, 198, 456, 119, 280, 50, 496, 248, 309, 106, 126, 871, 226, 80, 511, 420, 592, 262, 126, 166, 217, 377, 415, 143, 63, 530, 186, 109, 192, 499, 443, 429, 262, 150, 128, 128, 205, 80, 838, 307, 184, 1030, 565, 220, 54, 127, 208, 110, 569, 112, 251, 140, 155, 187, 165, 266, 255, 135, 168, 220, 187, 785, 128, 513, 171, 136, 622, 63, 73, 267, 125, 587, 67, 138, 197, 174, 686, 141, 260, 240, 167, 133, 424, 148, 270, 366, 72, 319, 195, 119, 151, 517, 268, 120, 111, 126, 178, 48, 146, 185, 210, 254, 62, 241, 30, 532, 214, 81, 222, 94, 50, 163, 305, 343, 691, 267, 208, 125, 142, 70, 180, 255, 229, 349, 294, 125, 158, 100, 266, 171, 596, 163, 169, 467, 406, 89, 310, 125, 387, 208, 117, 488, 1004, 63, 194, 154, 435, 233, 92, 187, 127, 203, 306, 202, 567, 150, 142, 127, 199, 174, 108, 246, 640, 522, 379, 799, 157, 354, 440, 351, 403, 357, 62, 434, 167, 100, 462, 589, 707, 117, 321, 355, 203, 123, 63, 492, 181, 121, 308, 213, 618, 271, 158, 1034, 113, 163, 413, 71, 178, 241, 142, 252, 408, 192, 271, 188, 265, 164, 404, 152, 229, 1112, 136, 116, 279, 339, 166, 317, 259, 436, 188, 81, 504, 224, 126, 1050, 339, 126, 126, 263, 119, 457, 320, 171, 169, 117, 73, 77, 715, 243, 329, 158, 48, 470, 46, 137, 128, 128, 201, 274, 50, 120, 198, 551, 323, 66, 424, 105, 65, 217, 113, 147, 187, 257, 148, 237, 112, 152, 185, 71, 128, 126, 83, 171, 203, 132, 120, 324, 464, 287, 173, 206, 136, 522, 175, 77, 153, 132, 123, 128, 227, 118, 129, 211, 552, 378, 309, 58, 702, 183, 284, 207, 128, 313, 151, 168, 297, 397, 111, 741, 130, 204, 136, 404, 962, 465, 544, 152, 443, 496, 314, 165, 200, 297, 127, 251, 155, 272, 208, 409, 344, 129, 71, 85, 140, 519, 263, 123, 235, 190, 126, 255, 409, 52, 144, 131, 122, 357, 96, 199, 280, 229, 163, 263, 319, 378, 476, 279, 210, 251, 134, 124, 216, 148, 116, 181, 178, 109, 427, 65, 130, 313, 139, 173, 152, 210, 201, 398, 137, 65, 303, 107, 126, 317, 521, 849, 343, 164, 320, 361, 168, 390, 48, 702, 119, 117, 132, 388, 660, 256, 57, 204, 216, 116, 168, 114, 251, 495, 142, 326, 96, 374, 199, 163, 184, 63, 731, 269, 499, 142, 53, 86, 238, 212, 315, 254, 162, 116, 100, 101, 119, 126, 257, 137, 410, 134, 397, 239, 757, 237, 183, 352, 437, 159, 300, 177, 294, 179, 218, 231, 149, 336, 135, 318, 114, 125, 163, 223, 397, 553, 332, 55, 270, 343, 175, 343, 138, 361, 131, 215, 265, 171, 494, 182, 497, 59, 71, 850, 512, 178, 740, 598, 62, 195, 168, 137, 270, 219, 183, 348, 203, 57, 178, 222, 257, 170, 152, 128, 142, 87, 118, 51, 355, 134, 316, 208, 220, 58, 181, 159, 63, 89, 172, 235, 462, 605, 128, 171, 171, 132, 184, 116, 67, 623, 374, 683, 155, 165, 426, 117, 612, 156, 144, 390, 227, 134, 484, 189, 139, 131, 44, 191, 180, 540, 138, 247, 237, 104, 130, 327, 233, 125, 149, 133, 158, 190, 160, 144, 246, 497, 249, 123, 173, 45, 342, 158, 168, 140, 133, 31, 287, 146, 613, 318, 154, 390, 146, 551, 349, 153, 168, 225, 452, 191, 219, 35, 60, 412, 125, 237, 140, 131, 116, 94, 124, 280, 125, 285, 201, 594, 142, 98, 130, 101, 256, 105, 146, 110, 244, 144, 132, 169, 341, 444, 127, 470, 338, 294, 69, 248, 246, 142, 133, 126, 88, 137, 143, 171, 438, 252, 136, 69, 45, 155, 955, 234, 132, 68, 767, 287, 119, 178, 184, 86, 113, 642, 139, 47, 314, 307, 196, 184, 386, 110, 299, 152, 164, 126, 174, 126, 136, 206, 56, 156, 231, 105, 242, 108, 882, 109, 180, 214, 323, 148, 123, 190, 176, 494, 326, 132, 78, 194, 294, 262, 219, 159, 138, 133, 209, 166, 57, 365, 110, 193, 112, 344, 45, 118, 351, 270, 69, 44, 102, 47, 310, 130, 128, 69, 351, 119, 607, 597, 112, 148, 173, 118, 424, 141, 142, 254, 287, 298, 399, 387, 310, 52, 246, 367, 242, 256, 987, 135, 256, 146, 285, 109, 270, 161, 231, 210, 89, 236, 229, 460, 162, 498, 233, 71, 142, 435, 184, 442, 190, 387, 85, 389, 52, 102, 174, 355, 133, 125, 365, 609, 308, 864, 654, 108, 301, 103, 223, 169, 355, 493, 97, 51, 186, 103, 153, 142, 89, 883, 155, 121, 491, 763, 133, 65, 192, 169, 171, 126, 215, 215, 138, 184, 650, 180, 194, 309, 148, 276, 1851, 502, 57, 464, 45, 234, 172, 139, 318, 205, 158, 167, 236, 352, 277, 916, 105, 179, 141, 183, 261, 281, 140, 413, 157, 200, 360, 471, 306, 590, 104, 485, 163, 136, 59, 99, 219, 148, 686, 311, 161, 151, 73, 122, 317, 235, 127, 168, 207, 987, 187, 285, 372, 243, 197, 271, 138, 154, 407, 190, 149, 128, 130, 300, 152, 136, 119, 194, 137, 701, 119, 193, 173, 130, 230, 136, 114, 162, 116, 240, 145, 382, 290, 129, 128, 122, 216, 232, 300, 137, 172, 508, 151, 124, 383, 364, 171, 504, 135, 210, 172, 726, 161, 130, 138, 141, 440, 306, 113, 123, 164, 161, 154, 253, 161, 47, 118, 124, 209, 275, 134, 67, 80, 123, 153, 208, 171, 270, 196, 107, 134, 130, 129, 149, 101, 150, 404, 146, 148, 599, 130, 194, 548, 247, 366, 764, 119, 123, 225, 217, 78, 354, 122, 397, 250, 128, 132, 312, 192, 467, 108, 115, 144, 156, 310, 241, 92, 216, 407, 131, 154, 119, 144, 205, 475, 205, 104, 715, 230, 104, 317, 41, 210, 565, 154, 62, 146, 383, 301, 349, 134, 239, 230, 223, 154, 172, 198, 141, 385, 279, 250, 200, 117, 385, 145, 396, 895, 172, 495, 148, 216, 262, 68, 481, 191, 217, 197, 177, 693, 70, 272, 134, 57, 307, 198, 142, 679, 99, 155, 171, 318, 167, 201, 173, 186, 256, 129, 345, 353, 141, 118, 591, 537, 166, 201, 117, 213, 177, 57, 496, 75, 135, 94, 108, 130, 227, 160, 121, 146, 329, 87, 180, 119, 219, 255, 349, 292, 584, 115, 64, 183, 103, 275, 371, 117, 124, 122, 102, 539, 111, 370, 299, 270, 356, 155, 141, 203, 167, 103, 189, 62, 358, 83, 84, 124, 230, 141, 205, 147, 49, 69, 318, 146, 134, 136, 118, 183, 604, 152, 424, 236, 174, 191, 214, 64, 405, 137, 686, 268, 817, 148, 65, 306, 211, 183, 55, 112, 285, 153, 293, 77, 347, 557, 62, 223, 876, 412, 139, 138, 203, 123, 123, 330, 926, 170, 530, 62, 83, 237, 660, 280, 124, 174, 158, 180, 117, 148, 733, 381, 412, 138, 101, 205, 228, 311, 156, 137, 83, 190, 521, 68, 327, 93, 341, 119, 118, 110, 275, 165, 386, 123, 492, 138, 550, 243, 291, 372, 133, 140, 94, 183, 410, 280, 352, 150, 333, 128, 71, 106, 228, 238, 115, 404, 63, 182, 296, 317, 129, 152, 89, 104, 111, 542, 136, 209, 114, 158, 440, 487, 118, 191, 187, 262, 284, 219, 116, 152, 312, 91, 145, 126, 804, 269, 174, 147, 158, 166, 250, 116, 260, 192, 157, 159, 204, 217, 96, 138, 128, 196, 184, 338, 554, 765, 75, 122, 321, 318, 581, 400, 112, 142, 135, 295, 166, 378, 236, 356, 178, 322, 40, 134, 146, 785, 655, 254, 374, 337, 137, 583, 411, 591, 145, 370, 171, 848, 177, 122, 143, 319, 159, 156, 132, 147, 268, 320, 145, 161, 120, 323, 84, 85, 146, 648, 554, 311, 162, 535, 398, 360, 104, 197, 112, 446, 244, 91, 443, 189, 128, 252, 274, 431, 140, 187, 285, 437, 118, 128, 40, 213, 274, 1014, 153, 275, 135, 114, 114, 142, 229, 136, 374, 328, 118, 285, 153, 87, 124, 176, 130, 243, 148, 189, 108, 222, 99, 247, 136, 108, 217, 416, 143, 432, 132, 868, 123, 128, 103, 69, 551, 279, 107, 363, 127, 124, 168, 679, 78, 241, 220, 69, 536, 218, 311, 206, 317, 133, 87, 132, 125, 453, 393, 79, 154, 96, 486, 43, 237, 115, 593, 119, 134, 193, 321, 298, 118, 428, 167, 354, 424, 108, 159, 185, 173, 102, 454, 497, 87, 35, 340, 856, 135, 133, 135, 177, 397, 206, 178, 105, 643, 206, 145, 160, 225, 201, 729, 125, 216, 160, 195, 695, 127, 99, 219, 150, 190, 305, 337, 179, 245, 162, 96, 134, 309, 820, 536, 210, 162, 96, 130, 261, 124, 310, 178, 121, 67, 184, 252, 130, 235, 137, 127, 126, 164, 55, 115, 46, 352, 136, 267, 124, 62, 198, 106, 198, 227, 201, 150, 802, 258, 368, 146, 118, 29, 527, 160, 45, 338, 290, 446, 482, 172, 117, 740, 231, 362, 185, 253, 143, 146, 129, 279, 61, 460, 129, 315, 240, 122, 169, 287, 173, 112, 313, 237, 209, 159, 20, 125, 116, 124, 197, 238, 84, 74, 149, 189, 283, 288, 127, 375, 113, 148, 208, 185, 46, 131, 193, 184, 478, 125, 140, 127, 180, 114, 161, 149, 127, 141, 233, 211, 240, 135, 233, 374, 183, 136, 185, 538, 80, 271, 113, 192, 84, 171, 700, 267, 126, 297, 196, 247, 239, 53, 207, 184, 868, 116, 129, 181, 463, 218, 324, 435, 546, 169, 142, 148, 118, 146, 364, 181, 131, 164, 285, 315, 214, 382, 758, 154, 119, 417, 201, 220, 147, 240, 69, 238, 229, 133, 211, 158, 562, 121, 121, 168, 459, 171, 227, 171, 137, 207, 163, 557, 168, 380, 148, 231, 286, 54, 402, 204, 120, 123, 260, 204, 203, 132, 303, 243, 321, 73, 189, 324, 244, 443, 256, 183, 132, 184, 221, 328, 141, 193, 129, 258, 122, 101, 251, 132, 439, 200, 146, 373, 175, 131, 311, 223, 431, 215, 377, 360, 640, 266, 340, 140, 199, 134, 146, 155, 349, 164, 152, 212, 202, 129, 307, 198, 196, 153, 106, 122, 142, 120, 95, 973, 169, 301, 161, 110, 629, 399, 264, 790, 506, 147, 131, 252, 122, 287, 147, 209, 152, 325, 186, 332, 167, 165, 284, 182, 273, 334, 244, 320, 448, 721, 142, 266, 623, 191, 209, 138, 177, 115, 79, 322, 269, 213, 56, 183, 285, 151, 358, 206, 51, 394, 37, 134, 141, 144, 106, 321, 216, 519, 144, 221, 532, 131, 643, 356, 137, 137, 204, 209, 146, 96, 194, 121, 155, 155, 150, 129, 461, 244, 199, 199, 92, 131, 329, 129, 121, 150, 166, 252, 198, 405, 382, 102, 133, 138, 727, 109, 134, 152, 456, 517, 175, 191, 387, 490, 143, 833, 208, 182, 324, 127, 131, 96, 171, 146, 1020, 135, 261, 179, 824, 135, 140, 163, 321, 852, 130, 171, 124, 171, 131, 629, 737, 88, 449, 130, 227, 152, 90, 68, 77, 90, 791, 214, 56, 153, 193, 273, 105, 49, 47, 511, 405, 143, 80, 380, 178, 141, 107, 177, 319, 342, 128, 250, 334, 321, 271, 134, 481, 174, 151, 113, 388, 702, 176, 310, 158, 155, 132, 42, 641, 96, 116, 169, 197, 151, 164, 230, 230, 145, 146, 355, 406, 149, 211, 111, 392, 176, 143, 110, 264, 313, 207, 149, 216, 445, 131, 385, 589, 97, 121, 316, 145, 138, 201, 137, 455, 126, 418, 409, 313, 192, 502, 133, 218, 929, 138, 94, 307, 142, 186, 188, 392, 329, 135, 59, 327, 117, 127, 154, 363, 242, 182, 376, 142, 176, 264, 230, 602, 126, 539, 158, 198, 381, 71, 191, 264, 458, 145, 798, 354, 228, 189, 100, 125, 136, 259, 152, 40, 187, 199, 196, 242, 110, 337, 473, 93, 344, 152, 386, 161, 72, 141, 69, 176, 68, 239, 128, 385, 122, 64, 177, 149, 90, 172, 406, 257, 121, 271, 401, 129, 212, 202, 131, 132, 100, 492, 220, 193, 49, 219, 136, 54, 55, 233, 147, 176, 143, 57, 344, 285, 202, 148, 129, 77, 593, 115, 177, 72, 157, 441, 145, 143, 139, 296, 190, 178, 233, 165, 224, 153, 550, 92, 142, 195, 136, 116, 130, 236, 150, 446, 158, 135, 115, 398, 192, 126, 345, 487, 147, 242, 166, 50, 506, 122, 273, 191, 120, 178, 306, 105, 133, 207, 368, 134, 548, 572, 167, 107, 181, 81, 163, 245, 492, 166, 127, 153, 155, 61, 168, 156, 142, 294, 149, 134, 248, 92, 143, 141, 223, 260, 56, 169, 179, 177, 450, 169, 52, 179, 148, 400, 82, 142, 127, 165, 51, 300, 45, 183, 145, 262, 134, 268, 722, 418, 138, 160, 121, 95, 166, 448, 176, 115, 188, 416, 212, 163, 93, 707, 1010, 131, 512, 150, 172, 296, 123, 239, 311, 676, 309, 436, 514, 125, 359, 116, 155, 363, 378, 129, 130, 169, 278, 426, 126, 615, 118, 243, 163, 258, 45, 111, 163, 140, 824, 148, 438, 48, 424, 424, 817, 131, 59, 150, 92, 127, 77, 270, 76, 330, 262, 198, 80, 529, 176, 119, 770, 51, 376, 161, 133, 481, 327, 259, 198, 75, 443, 128, 185, 395, 186, 888, 683, 183, 532, 47, 124, 144, 177, 120, 143, 286, 490, 136, 199, 150, 65, 170, 585, 160, 530, 68, 157, 107, 132, 242, 256, 158, 62, 136, 401, 229, 464, 488, 76, 222, 147, 116, 143, 184, 81, 217, 68, 713, 135, 349, 947, 144, 134, 155, 132, 253, 272, 37, 210, 165, 129, 137, 107, 176, 182, 118, 479, 50, 293, 183, 107, 160, 83, 168, 138, 193, 126, 199, 315, 123, 263, 136, 202, 53, 99, 253, 130, 156, 290, 122, 960, 243, 77, 115, 565, 178, 563, 122, 165, 293, 133, 200, 50, 260, 297, 26, 195, 205, 217, 114, 387, 246, 74, 325, 148, 83, 508, 374, 192, 135, 40, 161, 159, 136, 216, 212, 307, 127, 319, 106, 310, 108, 173, 230, 263, 163, 570, 288, 202, 401, 121, 93, 214, 184, 134, 413, 272, 62, 139, 310, 390, 218, 221, 135, 168, 131, 138, 131, 163, 273, 150, 112, 282, 452, 56, 206, 138, 63, 133, 125, 184, 294, 811, 349, 199, 365, 568, 65, 137, 162, 152, 305, 96, 150, 139, 134, 321, 443, 122, 149, 87, 136, 134, 95, 181, 794, 607, 188, 522, 127, 681, 104, 155, 565, 215, 100, 55, 611, 191, 132, 117, 200, 152, 162, 95, 598, 338, 90, 57, 327, 200, 580, 290, 441, 212, 313, 238, 125, 578, 119, 319, 228, 135, 752, 181, 156, 63, 191, 236, 116, 304, 144, 144, 323, 164, 279, 235, 197, 245, 118, 193, 383, 157, 111, 378, 295, 134, 94, 134, 149, 139, 158, 45, 212, 100, 279, 268, 319, 122, 265, 220, 217, 329, 187, 334, 168, 92, 132, 189, 382, 613, 233, 63, 150, 393, 120, 135, 47, 264, 169, 409, 197, 397, 415, 129, 213, 158, 484, 138, 145, 491, 61, 205, 154, 196, 441, 116, 341, 147, 421, 613, 270, 134, 154, 70, 117, 335, 146, 72, 203, 141, 124, 263, 277, 332, 169, 140, 252, 86, 346, 214, 60, 309, 218, 165, 171, 69, 131, 313, 257, 147, 73, 560, 97, 479, 147, 202, 921, 72, 129, 247, 113, 287, 196, 260, 585, 175, 217, 219, 380, 135, 116, 155, 77, 146, 316, 135, 484, 137, 122, 120, 75, 90, 253, 1003, 224, 233, 51, 586, 388, 140, 130, 118, 266, 351, 576, 62, 228, 70, 298, 263, 103, 188, 96, 340, 261, 268, 246, 133, 57, 168, 165, 215, 488, 245, 255, 293, 173, 437, 194, 559, 56, 124, 153, 173, 107, 236, 285, 879, 348, 370, 153, 172, 205, 106, 192, 189, 191, 418, 347, 88, 321, 450, 130, 75, 112, 171, 478, 125, 154, 119, 128, 315, 173, 150, 180, 79, 180, 145, 137, 183, 638, 139, 82, 167, 216, 72, 220, 64, 135, 172, 374, 181, 395, 121, 698, 163, 128, 132, 259, 197, 189, 46, 71, 117, 82, 536, 171, 157, 313, 187, 273, 72, 166, 552, 429, 347, 84, 283, 273, 66, 138, 257, 170, 150, 129, 196, 128, 388, 481, 298, 345, 112, 58, 103, 554, 124, 133, 346, 594, 176, 127, 223, 322, 190, 296, 146, 381, 115, 205, 71, 161, 300, 142, 137, 165, 118, 124, 160, 173, 185, 53, 142, 125, 116, 403, 54, 87, 323, 125, 135, 59, 115, 321, 206, 232, 317, 333, 1111, 135, 114, 126, 264, 153, 246, 140, 132, 380, 171, 132, 167, 219, 252, 724, 176, 197, 159, 130, 567, 153, 42, 115, 124, 351, 134, 141, 214, 191, 344, 105, 147, 981, 225, 130, 264, 178, 148, 188, 234, 466, 281, 62, 38, 111, 147, 150, 229, 121, 232, 341, 487, 224, 327, 264, 232, 793, 159, 864, 62, 174, 312, 53, 331, 272, 113, 167, 55, 408, 428, 241, 110, 500, 138, 195, 158, 160, 205, 201, 36, 466, 202, 917, 86, 73, 150, 168, 133, 288, 140, 79, 208, 131, 144, 169, 99, 128, 45, 119, 97, 244, 132, 64, 427, 560, 153, 694, 255, 89, 332, 308, 79, 199, 795, 268, 144, 229, 174, 154, 44, 74, 211, 330, 584, 342, 68, 135, 62, 124, 254, 235, 305, 130, 76, 161, 154, 570, 434, 99, 201, 123, 222, 252, 202, 43, 331, 369, 160, 49, 132, 279, 159, 165, 56, 152, 488, 374, 80, 126, 133, 499, 242, 229, 219, 222, 906, 120, 426, 402, 408, 99, 159, 120, 130, 540, 524, 262, 113, 153, 172, 205, 137, 198, 912, 236, 129, 49, 125, 358, 57, 580, 83, 122, 102, 270, 568, 129, 183, 116, 139, 124, 806, 123, 318, 394, 100, 561, 270, 285, 141, 37, 117, 126, 141, 259, 187, 529, 256, 144, 129, 992, 107, 167, 379, 146, 128, 50, 54, 227, 281, 195, 96, 122, 158, 210, 128, 36, 120, 167, 72, 140, 145, 109, 494, 293, 743, 116, 123, 74, 135, 86, 303, 231, 360, 449, 47, 132, 284, 207, 159, 131, 253, 404, 308, 366, 116, 57, 508, 182, 208, 705, 250, 346, 301, 246, 349, 124, 168, 575, 115, 139, 326, 114, 257, 161, 1016, 120, 237, 133, 127, 68, 661, 475, 184, 360, 148, 132, 171, 250, 223, 86, 265, 123, 351, 153, 145, 224, 157, 166, 133, 130, 52, 55, 147, 136, 439, 135, 604, 262, 297, 117, 585, 113, 183, 220, 251, 201, 215, 405, 185, 221, 376, 224, 175, 151, 135, 127, 86, 169, 114, 110, 216, 364, 62, 26, 161, 133, 134, 839, 201, 328, 57, 189, 83, 112, 227, 151, 139, 127, 203, 215, 295, 266, 145, 334, 123, 136, 243, 155, 566, 409, 118, 405, 161, 260, 105, 154, 1010, 268, 359, 166, 219, 42, 262, 406, 177, 373, 203, 325, 123, 227, 121, 479, 146, 201, 160, 631, 199, 878, 115, 210, 395, 124, 141, 262, 151, 164, 186, 104, 89, 142, 129, 580, 184, 143, 248, 228, 168, 434, 133, 110, 145, 376, 148, 137, 91, 182, 161, 78, 124, 180, 150, 118, 319, 259, 233, 322, 332, 55, 384, 99, 621, 403, 136, 253, 110, 132, 204, 150, 162, 125, 487, 192, 44, 138, 155, 112, 323, 404, 235, 423, 175, 209, 44, 152, 220, 116, 76, 71, 430, 521, 84, 145, 318, 273, 115, 166, 358, 238, 149, 175, 277, 381, 151, 135, 277, 562, 189, 149, 756, 224, 260, 241, 789, 216, 139, 831, 223, 223, 158, 159, 151, 296, 161, 426, 267, 142, 145, 254, 170, 142, 141, 99, 185, 139, 159, 97, 186, 308, 110, 404, 149, 452, 256, 276, 171, 52, 368, 117, 115, 187, 161, 74, 197, 151, 250, 215, 259, 167, 914, 323, 265, 185, 178, 50, 118, 286, 75, 203, 156, 106, 201, 140, 181, 276, 120, 384, 462, 678, 111, 45, 126, 151, 482, 274, 128, 180, 348, 683, 110, 664, 548, 117, 149, 800, 203, 562, 297, 176, 59, 226, 165, 80, 36, 124, 137, 112, 119, 160, 377, 227, 185, 256, 95, 149, 150, 82, 97, 963, 568, 130, 203, 439, 880, 144, 180, 140, 92, 71, 122, 305, 280, 150, 253, 381, 242, 201, 545, 277, 214, 377, 250, 125, 225, 143, 119, 117, 57, 133, 257, 197, 53, 74, 426, 132, 132, 44, 90, 242, 133, 174, 169, 126, 182, 52, 175, 119, 163, 528, 130, 158, 118, 219, 244, 126, 281, 162, 83, 989, 216, 176, 163, 193, 536, 103, 958, 442, 176, 144, 180, 239, 191, 58, 868, 119, 149, 181, 243, 130, 62, 133, 244, 230, 534, 171, 152, 166, 592, 204, 145, 140, 198, 126, 494, 223, 236, 127, 153, 531, 112, 126, 295, 207, 145, 526, 133, 148, 179, 89, 165, 152, 104, 255, 417, 130, 219, 232, 432, 45, 182, 134, 236, 103, 421, 256, 152, 102, 196, 102, 339, 104, 164, 127, 110, 142, 352, 125, 146, 147, 179, 139, 281, 296, 409, 123, 279, 216, 34, 168, 229, 462, 274, 154, 500, 127, 144, 186, 238, 130, 78, 201, 79, 157, 164, 135, 313, 38, 423, 138, 255, 251, 215, 117, 141, 186, 172, 194, 207, 37, 232, 142, 135, 154, 519, 134, 240, 117, 141, 158, 133, 355, 187, 97, 11, 213, 407, 98, 845, 341, 268, 74, 147, 71, 940, 237, 83, 157, 139, 600, 125, 155, 156, 328, 129, 57, 454, 146, 114, 210, 236, 144, 979, 126, 361, 179, 147, 131, 197, 241, 45, 438, 473, 225, 248, 111, 145, 101, 155, 84, 274, 130, 160, 137, 130, 601, 53, 334, 181, 135, 290, 153, 291, 121, 308, 199, 155, 195, 204, 143, 156, 64, 264, 398, 274, 190, 189, 184, 75, 185, 114, 484, 239, 191, 123, 110, 484, 137, 146, 381, 125, 284, 312, 144, 154, 202, 274, 148, 197, 340, 174, 168, 599, 50, 224, 327, 425, 154, 255, 170, 124, 281, 217, 160, 244, 92, 117, 202, 127, 104, 137, 129, 154, 288, 384, 165, 276, 794, 158, 620, 165, 160, 130, 344, 119, 137, 377, 144, 601, 152, 738, 267, 699, 409, 121, 45, 199, 207, 364, 133, 297, 342, 112, 201, 970, 115, 588, 530, 354, 135, 46, 258, 49, 169, 622, 299, 146, 138, 222, 245, 453, 690, 551, 237, 55, 124, 130, 272, 138, 131, 378, 133, 128, 216, 188, 421, 219, 79, 145, 105, 251, 220, 625, 317, 463, 133, 149, 118, 101, 134, 178, 192, 143, 182, 168, 568, 85, 198, 672, 158, 101, 422, 131, 63, 364, 170, 146, 471, 238, 88, 185, 352, 207, 800, 57, 202, 213, 98, 126, 268, 60, 192, 131, 334, 109, 141, 162, 192, 41, 45, 416, 180, 132, 130, 193, 139, 325, 429, 82, 226, 129, 108, 143, 93, 178, 253, 396, 345, 181, 295, 146, 237, 212, 444, 466, 46, 102, 163, 557, 213, 49, 220, 302, 358, 396, 177, 217, 109, 140, 246, 44, 144, 664, 130, 100, 75, 68, 150, 126, 183, 134, 121, 364, 151, 35, 170, 237, 138, 136, 355, 198, 214, 149, 153, 28, 83, 308, 147, 175, 273, 262, 118, 260, 197, 576, 127, 279, 124, 124, 123, 56, 123, 290, 192, 470, 76, 62, 240, 133, 68, 157, 130, 66, 283, 73, 451, 150, 106, 143, 104, 220, 170, 120, 168, 333, 125, 528, 165, 108, 56, 528, 175, 130, 134, 102, 390, 212, 205, 63, 37, 365, 257, 54, 109, 205, 155, 179, 118, 423, 124, 457, 158, 795, 246, 179, 323, 83, 560, 39, 456, 101, 214, 646, 316, 458, 119, 222, 51, 187, 51, 122, 299, 295, 363, 205, 189, 255, 133, 126, 107, 110, 383, 152, 412, 286, 303, 261, 502, 236, 111, 285, 220, 181, 158, 159, 225, 350, 188, 328, 136, 826, 111, 1014, 270, 208, 209, 105, 610, 73, 122, 473, 183, 171, 302, 758, 190, 98, 118, 356, 325, 843, 119, 190, 125, 179, 137, 154, 230, 352, 330, 147, 283, 74, 191, 579, 106, 149, 103, 168, 136, 254, 172, 313, 333, 157, 125, 262, 142, 166, 141, 162, 51, 287, 153, 231, 200, 296, 232, 174, 418, 133, 331, 228, 155, 119, 176, 162, 569, 259, 273, 134, 154, 135, 44, 275, 73, 183, 194, 461, 159, 107, 139, 236, 131, 78, 223, 197, 168, 480, 240, 150, 595, 158, 310, 773, 158, 342, 382, 301, 45, 27, 132, 194, 133, 216, 76, 177, 538, 143, 210, 418, 415, 266, 725, 142, 116, 526, 514, 61, 323, 102, 147, 150, 168, 271, 119, 135, 166, 135, 251, 144, 285, 57, 197, 138, 190, 958, 307, 167, 610, 220, 124, 373, 190, 198, 128, 220, 129, 184, 652, 422, 262, 65, 116, 289, 269, 658, 261, 174, 250, 92, 106, 202, 190, 250, 626, 11, 246, 424, 141, 247, 214, 326, 124, 256, 228, 178, 124, 335, 108, 157, 39, 150, 198, 731, 159, 131, 137, 93, 526, 241, 236, 46, 283, 214, 333, 173, 272, 188, 560, 238, 129, 566, 445, 411, 693, 37, 163, 129, 125, 246, 40, 220, 116, 130, 95, 713, 149, 503, 219, 112, 274, 65, 58, 151, 317, 105, 112, 69, 162, 139, 272, 172, 222, 64, 457, 88, 193, 208, 192, 299, 125, 111, 274, 180, 431, 156, 237, 47, 593, 142, 346, 190, 85, 47, 343, 151, 499, 130, 132, 309, 274, 707, 161, 136, 213, 149, 241, 716, 148, 236, 194, 94, 87, 140, 453, 346, 184, 147, 293, 133, 320, 314, 223, 175, 517, 632, 565, 354, 361, 68, 461, 145, 121, 262, 101, 155, 231, 439, 597, 945, 627, 85, 151, 253, 197, 307, 170, 294, 224, 202, 119, 129, 206, 71, 42, 116, 285, 581, 159, 944, 536, 137, 193, 58, 717, 137, 138, 380, 139, 124, 131, 213, 284, 928, 93, 327, 210, 212, 206, 184, 155, 408, 139, 185, 180, 81, 283, 253, 561, 94, 207, 50, 363, 211, 116, 125, 112, 481, 181, 153, 226, 107, 46, 135, 92, 46, 125, 124, 198, 286, 172, 194, 209, 168, 240, 120, 152, 147, 173, 218, 140, 155, 224, 83, 214, 413, 523, 293, 185, 203, 191, 74, 193, 133, 286, 268, 157, 84, 185, 141, 205, 143, 342, 134, 94, 120, 158, 342, 584, 211, 278, 120, 200, 26, 113, 98, 339, 254, 450, 616, 143, 138, 222, 103, 168, 243, 186, 125, 144, 139, 318, 133, 116, 91, 102, 310, 148, 118, 111, 206, 164, 146, 94, 112, 359, 181, 41, 180, 338, 232, 386, 241, 124, 174, 74, 274, 131, 319, 158, 131, 317, 678, 146, 66, 114, 156, 166, 156, 397, 155, 144, 134, 1300, 190, 131, 612, 119, 418, 486, 799, 245, 190, 152, 273, 214, 159, 771, 136, 159, 46, 182, 116, 583, 121, 125, 236, 134, 266, 363, 326, 58, 147, 214, 23, 220, 156, 570, 584, 371, 45, 172, 250, 136, 133, 337, 226, 133, 155, 162, 132, 114, 136, 310, 1394, 53, 131, 241, 115, 201, 149, 411, 709, 127, 192, 144, 92, 864, 340, 428, 109, 120, 144, 262, 159, 231, 203, 168, 198, 130, 115, 201, 242, 203, 619, 274, 110, 139, 285, 130, 166, 29, 141, 363, 455, 145, 221, 127, 130, 132, 131, 277, 162, 118, 89, 149, 307, 454, 146, 188, 120, 200, 100, 124, 139, 176, 127, 134, 177, 261, 793, 137, 198, 130, 260, 125, 151, 206, 268, 52, 47, 109, 202, 996, 93, 380, 256, 153, 636, 138, 114, 126, 124, 78, 258, 162, 256, 1006, 150, 157, 148, 376, 319, 283, 104, 156, 217, 73, 511, 722, 126, 177, 71, 195, 263, 73, 286, 38, 141, 114, 264, 250, 385, 229, 127, 42, 575, 133, 1009, 230, 399, 230, 153, 187, 175, 153, 381, 225, 291, 232, 383, 317, 121, 231, 128, 192, 142, 271, 136, 145, 310, 195, 173, 185, 101, 177, 57, 329, 158, 40, 130, 617, 44, 78, 559, 184, 749, 88, 926, 299, 134, 253, 109, 206, 924, 104, 229, 384, 44, 132, 512, 177, 586, 224, 109, 399, 351, 847, 68, 257, 208, 481, 182, 167, 54, 76, 111, 398, 149, 246, 450, 140, 136, 394, 593, 179, 219, 228, 188, 98, 178, 153, 165, 408, 87, 42, 129, 299, 143, 428, 111, 320, 1033, 347, 407, 196, 128, 314, 134, 56, 152, 41, 196, 213, 84, 154, 288, 161, 351, 64, 207, 167, 245, 588, 442, 148, 495, 188, 197, 168, 149, 277, 211, 134, 252, 348, 280, 119, 129, 199, 278, 538, 375, 349, 138, 598, 122, 209, 146, 319, 185, 343, 600, 265, 284, 79, 43, 407, 131, 641, 135, 159, 203, 248, 122, 205, 100, 167, 147, 142, 208, 360, 120, 141, 304, 239, 731, 143, 1013, 143, 193, 118, 122, 129, 166, 182, 202, 407, 241, 529, 166, 261, 269, 130, 346, 71, 105, 224, 183, 152, 163, 166, 50, 137, 306, 452, 425, 104, 576, 249, 355, 127, 647, 155, 261, 168, 320, 657, 384, 499, 151, 62, 139, 58, 200, 109, 150, 669, 127, 63, 201, 327, 69, 139, 137, 198, 69, 59, 184, 214, 252, 177, 282, 128, 70, 344, 109, 327, 147, 92, 157, 694, 114, 273, 227, 51, 602, 124, 122, 125, 119, 151, 155, 128, 150, 184, 174, 356, 105, 172, 208, 50, 179, 122, 189, 191, 71, 115, 160, 682, 207, 57, 212, 179, 367, 193, 142, 276, 194, 268, 1015, 57, 135, 193, 230, 189, 359, 146, 228, 378, 268, 236, 194, 49, 124, 132, 119, 248, 160, 794, 122, 300, 250, 307, 150, 198, 279, 176, 126, 95, 191, 429, 145, 63, 120, 155, 154, 561, 214, 161, 331, 132, 304, 258, 183, 413, 315, 441, 251, 71, 127, 252, 222, 143, 175, 111, 145, 523, 553, 148, 626, 85, 862, 285, 267, 129, 131, 415, 382, 275, 132, 788, 68, 179, 701, 163, 449, 76, 134, 81, 482, 477, 147, 147, 132, 95, 204, 144, 97, 159, 147, 131, 114, 635, 127, 104, 360, 119, 243, 59, 143, 577, 137, 131, 51, 160, 462, 343, 240, 746, 446, 220, 166, 674, 116, 100, 183, 278, 183, 114, 288, 60, 236, 204, 133, 304, 155, 444, 132, 89, 177, 209, 477, 266, 121, 108, 309, 125, 161, 852, 548, 143, 161, 138, 241, 290, 109, 177, 152, 309, 494, 198, 197, 159, 122, 52, 92, 201, 120, 163, 122, 156, 231, 267, 234, 428, 331, 283, 111, 118, 306, 125, 167, 149, 283, 950, 140, 340, 231, 145, 234, 146, 134, 99, 162, 112, 112, 40, 506, 142, 110, 180, 64, 204, 515, 145, 149, 340, 134, 138, 256, 77, 387, 108, 107, 94, 324, 289, 134, 268, 125, 455, 440, 420, 78, 440, 945, 126, 215, 123, 177, 127, 70, 71, 285, 213, 141, 43, 137, 791, 278, 319, 212, 180, 125, 118, 148, 135, 179, 129, 467, 162, 183, 93, 161, 335, 144, 400, 107, 481, 146, 152, 318, 228, 156, 397, 148, 170, 450, 169, 55, 120, 253, 63, 119, 223, 1001, 62, 133, 133, 488, 129, 132, 352, 320, 130, 180, 218, 105, 275, 105, 275, 153, 109, 230, 134, 397, 229, 493, 134, 133, 131, 239, 227, 77, 195, 173, 314, 304, 417, 57, 132, 433, 54, 121, 300, 168, 292, 123, 233, 105, 814, 203, 150, 107, 373, 130, 146, 426, 190, 178, 187, 131, 104, 69, 234, 205, 375, 349, 447, 134, 87, 297, 166, 184, 502, 141, 183, 124, 154, 764, 120, 54, 160, 39, 594, 178, 200, 137, 241, 54, 351, 217, 376, 89, 257, 404, 165, 138, 346, 47, 183, 279, 281, 227, 120, 247, 329, 364, 122, 148, 466, 746, 92, 129, 271, 260, 81, 474, 237, 88, 148, 551, 319, 221, 308, 73, 153, 227, 265, 90, 146, 117, 98, 440, 435, 145, 147, 530, 372, 209, 147, 255, 248, 716, 286, 130, 141, 86, 135, 681, 117, 141, 59, 787, 351, 167, 357, 215, 120, 244, 378, 43, 220, 105, 50, 123, 142, 163, 244, 164, 281, 274, 126, 119, 143, 119, 707, 71, 170, 164, 296, 352, 158, 188, 118, 130, 196, 257, 46, 406, 183, 71, 83, 191, 125, 303, 129, 156, 322, 190, 148, 170, 37, 329, 141, 207, 124, 372, 69, 939, 161, 128, 300, 340, 300, 176, 232, 198, 815, 172, 486, 262, 149, 430, 178, 89, 138, 309, 425, 127, 138, 401, 122, 1023, 137, 243, 192, 113, 85, 203, 107, 528, 190, 386, 140, 309, 145, 253, 257, 270, 194, 123, 92, 99, 140, 123, 265, 131, 303, 180, 157, 166, 156, 157, 121, 92, 155, 120, 223, 265, 214, 571, 255, 320, 296, 118, 232, 501, 129, 179, 117, 184, 251, 235, 235, 116, 140, 405, 366, 200, 149, 91, 119, 137, 82, 138, 282, 136, 549, 147, 218, 152, 183, 172, 152, 136, 331, 71, 227, 196, 78, 183, 446, 116, 145, 132, 110, 213, 142, 55, 154, 156, 98, 410, 70, 46, 446, 472, 334, 125, 380, 245, 719, 169, 277, 217, 124, 136, 249, 389, 104, 282, 270, 145, 109, 218, 125, 168, 269, 130, 134, 233, 191, 140, 201, 454, 116, 60, 170, 367, 207, 808, 97, 123, 281, 100, 660, 595, 187, 170, 126, 299, 162, 88, 249, 116, 155, 369, 712, 121, 204, 563, 106, 597, 150, 529, 127, 127, 134, 301, 91, 348, 147, 623, 150, 370, 116, 73, 45, 408, 573, 169, 47, 177, 77, 332, 142, 138, 123, 128, 205, 50, 80, 282, 88, 304, 433, 212, 162, 152, 204, 49, 79, 427, 551, 259, 361, 175, 329, 151, 512, 274, 139, 186, 250, 169, 112, 132, 223, 145, 239, 174, 571, 104, 758, 92, 208, 198, 140, 479, 156, 309, 60, 141, 143, 119, 399, 115, 336, 380, 154, 256, 987, 44, 132, 191, 309, 272, 271, 158, 71, 84, 158, 128, 152, 379, 43, 87, 202, 190, 137, 81, 221, 186, 179, 257, 139, 336, 297, 173, 344, 521, 64, 368, 242, 143, 310, 192, 200, 227, 203, 127, 136, 331, 159, 200, 438, 103, 142, 140, 139, 273, 404, 490, 119, 192, 353, 355, 160, 169, 178, 546, 170, 75, 175, 125, 94, 46, 113, 385, 123, 154, 63, 252, 132, 45, 328, 87, 200, 499, 161, 37, 87, 335, 401, 138, 164, 253, 117, 159, 219, 126, 347, 128, 363, 268, 241, 586, 164, 117, 562, 133, 565, 277, 470, 293, 153, 228, 209, 222, 162, 45, 253, 147, 84, 90, 112, 280, 172, 104, 669, 70, 162, 129, 133, 29, 165, 82, 147, 109, 191, 154, 297, 613, 184, 253, 141, 102, 214, 118, 118, 200, 152, 132, 140, 131, 148, 144, 71, 276, 135, 207, 208, 510, 123, 160, 195, 317, 142, 126, 135, 155, 341, 297, 262, 132, 117, 110, 191, 123, 141, 865, 145, 138, 301, 146, 154, 129, 205, 149, 303, 163, 244, 496, 375, 138, 253, 189, 264, 381, 214, 114, 304, 99, 172, 213, 588, 399, 360, 127, 249, 579, 764, 275, 123, 413, 159, 315, 164, 577, 147, 163, 138, 279, 119, 77, 257, 156, 133, 162, 51, 241, 123, 140, 109, 200, 122, 153, 74, 154, 225, 153, 90, 115, 99, 166, 70, 247, 125, 160, 433, 138, 657, 640, 602, 188, 317, 140, 617, 150, 248, 120, 140, 147, 132, 137, 306, 133, 480, 68, 84, 757, 140, 427, 117, 176, 655, 417, 325, 369, 262, 407, 151, 171, 250, 175, 45, 321, 138, 387, 475, 283, 122, 255, 366, 459, 334, 125, 120, 75, 130, 400, 540, 123, 49, 258, 120, 132, 132, 145, 90, 173, 228, 146, 109, 84, 60, 226, 124, 90, 159, 120, 969, 505, 119, 231, 232, 91, 168, 185, 283, 360, 290, 80, 203, 1015, 167, 119, 128, 440, 219, 351, 129, 130, 169, 182, 122, 128, 64, 135, 137, 337, 183, 150, 130, 626, 311, 124, 74, 160, 350, 94, 231, 138, 143, 100, 140, 773, 126, 255, 161, 114, 86, 264, 203, 263, 168, 307, 263, 147, 162, 1733, 452, 69, 286, 175, 156, 169, 612, 248, 211, 164, 420, 313, 198, 213, 80, 77, 150, 208, 136, 366, 67, 171, 79, 360, 1034, 160, 330, 309, 153, 90, 97, 164, 281, 520, 255, 662, 228, 480, 479, 166, 105, 183, 33, 53, 333, 900, 72, 297, 1000, 195, 416, 169, 152, 120, 92, 246, 157, 132, 333, 150, 251, 102, 159, 163, 121, 210, 208, 473, 95, 357, 147, 187, 138, 625, 145, 70, 145, 172, 174, 174, 163, 102, 929, 270, 210, 129, 235, 130, 151, 234, 96, 582, 177, 192, 176, 132, 160, 184, 314, 201, 107, 198, 507, 157, 203, 390, 139, 245, 350, 416, 79, 145, 241, 591, 62, 607, 167, 158, 375, 257, 194, 91, 127, 358, 259, 680, 366, 180, 154, 994, 179, 729, 96, 137, 457, 259, 476, 177, 237, 258, 54, 839, 182, 212, 282, 164, 182, 221, 201, 63, 173, 290, 68, 154, 225, 190, 104, 140, 318, 249, 221, 134, 848, 204, 246, 195, 488, 121, 419, 45, 201, 290, 289, 145, 951, 153, 199, 200, 109, 163, 190, 84, 175, 397, 101, 307, 249, 129, 219, 474, 180, 114, 125, 155, 365, 110, 163, 140, 181, 126, 127, 177, 330, 293, 133, 130, 397, 228, 175, 69, 138, 653, 229, 341, 166, 210, 127, 82, 179, 157, 321, 134, 116, 145, 520, 612, 90, 147, 41, 121, 308, 144, 119, 211, 210, 438, 175, 250, 187, 245, 241, 237, 161, 146, 589, 132, 540, 65, 221, 98, 76, 79, 331, 613, 205, 104, 175, 760, 231, 197, 93, 244, 114, 254, 230, 88, 132, 66, 105, 230, 157, 76, 198, 159, 118, 187, 130, 371, 161, 343, 276, 788, 199, 116, 165, 233, 608, 85, 323, 472, 116, 232, 207, 421, 620, 284, 124, 552, 101, 130, 154, 456, 139, 1020, 121, 758, 265, 973, 96, 122, 146, 88, 1002, 47, 352, 506, 167, 209, 151, 40, 266, 241, 146, 141, 248, 927, 140, 154, 48, 104, 229, 93, 42, 151, 102, 126, 150, 230, 392, 138, 175, 395, 125, 334, 1016, 158, 143, 45, 205, 154, 294, 295, 147, 323, 454, 441, 335, 169, 588, 179, 168, 529, 374, 51, 180, 79, 202, 269, 268, 145, 171, 239, 113, 130, 257, 143, 165, 620, 239, 164, 491, 81, 489, 199, 204, 132, 134, 187, 132, 115, 112, 558, 610, 451, 343, 150, 433, 324, 108, 468, 143, 304, 120, 275, 271, 250, 307, 80, 546, 90, 127, 119, 161, 121, 134, 58, 1092, 109, 229, 152, 304, 250, 69, 279, 331, 280, 371, 112, 152, 471, 155, 75, 225, 129, 287, 111, 428, 254, 196, 95, 358, 98, 137, 124, 127, 182, 700, 49, 129, 520, 586, 170, 173, 164, 179, 338, 183, 202, 174, 647, 304, 62, 123, 162, 135, 147, 180, 89, 336, 120, 59, 125, 138, 418, 208, 340, 233, 911, 69, 228, 225, 272, 378, 128, 309, 248, 382, 346, 1036, 117, 56, 176, 148, 99, 194, 255, 134, 136, 153, 155, 44, 47, 156, 218, 130, 59, 407, 153, 146, 217, 84, 271, 181, 320, 146, 113, 293, 181, 65, 113, 163, 458, 300, 223, 592, 392, 74, 157, 85, 414, 121, 169, 365, 235, 232, 360, 198, 136, 150, 351, 66, 240, 366, 364, 37, 155, 162, 469, 163, 129, 122, 194, 202, 815, 368, 199, 53, 133, 167, 964, 245, 363, 122, 170, 203, 239, 214, 124, 426, 107, 536, 127, 329, 218, 179, 209, 125, 140, 178, 58, 117, 143, 110, 141, 433, 185, 112, 205, 132, 159, 496, 139, 217, 166, 498, 356, 275, 625, 56, 211, 287, 178, 985, 48, 58, 158, 294, 668, 186, 121, 125, 251, 628, 352, 210, 226, 345, 133, 106, 390, 134, 110, 168, 201, 355, 213, 157, 311, 367, 163, 161, 153, 162, 337, 249, 165, 78, 80, 127, 437, 160, 355, 159, 137, 339, 95, 745, 136, 212, 195, 263, 154, 155, 546, 139, 75, 76, 62, 186, 292, 148, 171, 242, 150, 142, 64, 130, 170, 128, 125, 411, 102, 470, 36, 233, 379, 262, 56, 115, 61, 119, 160, 147, 86, 156, 260, 131, 108, 184, 117, 119, 185, 181, 127, 132, 167, 210, 141, 684, 449, 394, 371, 254, 216, 119, 70, 380, 357, 125, 392, 125, 43, 700, 150, 209, 97, 195, 118, 134, 359, 121, 200, 866, 67, 50, 119, 252, 203, 134, 80, 96, 501, 277, 609, 121, 254, 230, 149, 173, 283, 270, 336, 74, 203, 113, 169, 113, 43, 157, 54, 287, 218, 322, 129, 200, 299, 128, 174, 168, 181, 130, 129, 104, 171, 126, 423, 155, 73, 347, 40, 102, 247, 412, 132, 223, 159, 197, 71, 140, 131, 747, 58, 297, 257, 539, 217, 596, 184, 62, 141, 279, 187, 248, 179, 271, 128, 306, 334, 99, 180, 169, 117, 326, 117, 145, 174, 76, 210, 240, 168, 119, 261, 160, 184, 515, 103, 119, 225, 137, 116, 404, 292, 132, 363, 156, 355, 255, 38, 232, 271, 247, 64, 89, 116, 146, 138, 85, 155, 533, 141, 127, 149, 118, 38, 196, 215, 72, 186, 129, 653, 106, 134, 335, 214, 256, 128, 155, 337, 99, 206, 195, 343, 837, 127, 121, 143, 165, 236, 183, 205, 766, 377, 276, 191, 215, 272, 205, 125, 489, 191, 158, 694, 452, 145, 165, 345, 484, 182, 129, 136, 212, 365, 247, 185, 199, 301, 156, 125, 172, 870, 299, 393, 108, 99, 220, 130, 114, 174, 236, 330, 159, 453, 113, 79, 209, 258, 128, 306, 89, 175, 536, 122, 222, 133, 398, 171, 108, 131, 297, 220, 108, 148, 296, 349, 113, 277, 248, 63, 468, 321, 333, 247, 167, 320, 582, 471, 706, 109, 107, 303, 160, 224, 270, 132, 120, 221, 424, 275, 135, 122, 144, 122, 130, 283, 138, 26, 217, 127, 134, 142, 187, 833, 153, 170, 124, 197, 123, 64, 258, 113, 57, 100, 134, 817, 356, 143, 150, 174, 158, 199, 53, 899, 140, 161, 114, 704, 154, 57, 178, 193, 410, 1000, 755, 458, 115, 156, 365, 466, 85, 898, 35, 426, 251, 164, 63, 116, 670, 411, 252, 985, 131, 285, 117, 250, 356, 204, 125, 546, 146, 141, 263, 252, 122, 1011, 129, 278, 105, 52, 150, 105, 84, 347, 418, 129, 118, 120, 192, 180, 199, 169, 276, 307, 326, 151, 183, 146, 199, 268, 234, 139, 521, 167, 109, 141, 83, 278, 819, 302, 202, 313, 163, 209, 161, 163, 457, 1546, 106, 129, 243, 186, 101, 297, 128, 195, 131, 112, 554, 229, 256, 144, 144, 220, 457, 123, 160, 167, 242, 217, 139, 70, 299, 418, 122, 154, 185, 464, 126, 114, 84, 399, 137, 217, 86, 33, 127, 129, 293, 113, 116, 233, 604, 342, 199, 131, 643, 274, 1022, 86, 189, 43, 80, 265, 141, 146, 179, 294, 300, 278, 857, 208, 80, 310, 103, 215, 248, 195, 67, 84, 299, 331, 313, 215, 261, 351, 186, 137, 232, 164, 69, 129, 168, 189, 174, 376, 65, 104, 477, 117, 364, 58, 863, 863, 274, 145, 322, 301, 151, 61, 125, 123, 244, 103, 100, 142, 191, 261, 139, 271, 142, 225, 173, 123, 212, 227, 565, 483, 444, 391, 343, 257, 134, 250, 131, 143, 326, 122, 205, 274, 358, 47, 201, 68, 827, 135, 115, 132, 218, 156, 129, 243, 67, 183, 64, 186, 199, 127, 260, 136, 215, 264, 196, 111, 296, 138, 112, 155, 159, 79, 143, 320, 182, 133, 221, 470, 268, 100, 43, 137, 48, 168, 291, 164, 125, 69, 123, 196, 159, 453, 177, 132, 149, 346, 231, 117, 312, 229, 149, 171, 79, 267, 213, 193, 353, 225, 608, 135, 138, 81, 142, 338, 70, 256, 136, 137, 92, 136, 142, 529, 160, 88, 166, 172, 149, 628, 341, 236, 650, 197, 529, 496, 158, 222, 128, 176, 222, 65, 157, 130, 418, 1162, 126, 55, 82, 174, 198, 634, 293, 171, 576, 140, 283, 125, 178, 323, 192, 145, 147, 151, 298, 173, 149, 139, 148, 181, 132, 66, 153, 54, 631, 60, 205, 190, 529, 58, 268, 192, 107, 409, 144, 157, 251, 159, 125, 221, 105, 664, 113, 291, 76, 156, 122, 237, 209, 133, 157, 172, 63, 549, 130, 171, 89, 172, 375, 124, 81, 124, 121, 218, 391, 275, 712, 90, 161, 197, 489, 160, 119, 276, 173, 162, 579, 452, 155, 277, 120, 686, 183, 173, 307, 529, 199, 36, 116, 993, 220, 106, 195, 134, 148, 128, 313, 114, 220, 49, 530, 339, 918, 151, 144, 67, 135, 151, 145, 121, 189, 291, 83, 474, 364, 288, 299, 132, 45, 162, 210, 89, 382, 257, 122, 50, 124, 138, 184, 460, 131, 252, 66, 426, 380, 89, 225, 645, 76, 130, 163, 205, 332, 229, 263, 433, 132, 82, 123, 136, 138, 296, 117, 495, 101, 80, 250, 565, 213, 150, 122, 100, 324, 146, 141, 143, 54, 440, 138, 138, 110, 169, 44, 125, 243, 334, 78, 227, 69, 223, 162, 341, 253, 309, 343, 362, 170, 239, 152, 131, 175, 140, 97, 190, 576, 201, 521, 114, 167, 116, 297, 133, 264, 957, 151, 152, 77, 301, 177, 247, 134, 211, 340, 215, 174, 141, 158, 73, 194, 260, 346, 217, 147, 377, 110, 197, 105, 112, 196, 183, 175, 81, 509, 304, 583, 201, 211, 118, 363, 124, 570, 185, 241, 209, 19, 249, 356, 320, 209, 158, 512, 58, 812, 116, 117, 200, 387, 148, 465, 140, 365, 174, 120, 235, 536, 35, 139, 73, 129, 154, 401, 52, 647, 392, 324, 135, 211, 69, 222, 52, 146, 155, 193, 502, 391, 263, 268, 157, 47, 301, 181, 188, 714, 158, 130, 263, 220, 94, 31, 189, 139, 176, 135, 213, 96, 154, 154, 74, 264, 247, 139, 532, 370, 53, 118, 80, 185, 136, 129, 126, 138, 119, 242, 153, 234, 172, 188, 302, 417, 235, 302, 165, 115, 131, 169, 275, 35, 207, 328, 123, 33, 247, 157, 246, 75, 256, 156, 280, 61, 161, 217, 200, 286, 190, 151, 175, 167, 240, 173, 745, 255, 173, 119, 211, 508, 552, 168, 112, 219, 102, 186, 1222, 146, 104, 156, 198, 336, 144, 337, 446, 110, 121, 145, 197, 182, 479, 878, 257, 252, 118, 207, 502, 158, 146, 125, 218, 132, 71, 70, 144, 167, 171, 362, 254, 71, 157, 155, 94, 403, 137, 166, 163, 144, 66, 244, 157, 396, 440, 58, 292, 221, 163, 82, 80, 124, 443, 165, 133, 167, 349, 140, 164, 120, 96, 136, 559, 183, 266, 233, 181, 255, 114, 157, 269, 90, 292, 162, 129, 82, 264, 243, 157, 148, 382, 195, 109, 137, 121, 171, 145, 115, 217, 139, 134, 54, 80, 216, 129, 87, 267, 435, 172, 181, 331, 68, 364, 153, 107, 332, 413, 122, 1000, 149, 122, 550, 71, 82, 152, 225, 136, 162, 250, 153, 150, 107, 151, 249, 123, 192, 142, 333, 123, 267, 236, 412, 148, 138, 218, 273, 246, 302, 348, 138, 246, 358, 267, 103, 169, 45, 382, 325, 226, 128, 247, 112, 224, 253, 168, 168, 210, 29, 180, 117, 127, 226, 411, 208, 154, 259, 143, 357, 219, 88, 104, 60, 172, 110, 120, 116, 124, 343, 276, 225, 156, 764, 138, 575, 253, 173, 184, 157, 215, 143, 326, 113, 193, 137, 261, 314, 294, 141, 90, 227, 185, 173, 244, 72, 45, 148, 737, 308, 149, 166, 691, 63, 151, 217, 55, 173, 331, 470, 244, 189, 163, 150, 128, 183, 240, 112, 283, 407, 145, 464, 131, 85, 187, 501, 178, 104, 136, 61, 458, 73, 238, 131, 115, 85, 98, 350, 193, 223, 120, 198, 104, 53, 124, 133, 157, 127, 343, 127, 466, 42, 86, 210, 617, 292, 253, 110, 201, 110, 350, 306, 119, 56, 149, 283, 132, 120, 334, 347, 63, 140, 146, 136, 373, 136, 308, 653, 192, 552, 56, 149, 139, 142, 151, 474, 122, 540, 310, 53, 139, 153, 59, 144, 209, 397, 124, 150, 133, 171, 502, 464, 54, 79, 141, 47, 258, 127, 131, 763, 163, 148, 29, 423, 142, 166, 50, 400, 118, 188, 97, 169, 179, 292, 279, 226, 324, 226, 87, 184, 164, 140, 123, 147, 259, 175, 88, 161, 132, 108, 99, 173, 225, 55, 146, 683, 75, 255, 59, 440, 157, 84, 160, 178, 147, 211, 205, 49, 154, 136, 294, 342, 128, 245, 153, 124, 484, 110, 447, 136, 150, 586, 239, 141, 114, 1023, 128, 108, 216, 223, 512, 123, 57, 66, 148, 269, 150, 214, 190, 110, 320, 217, 172, 619, 91, 394, 209, 186, 92, 143, 371, 177, 752, 135, 535, 248, 138, 196, 296, 386, 156, 160, 161, 135, 138, 313, 158, 233, 471, 153, 295, 182, 240, 592, 167, 62, 71, 228, 138, 61, 72, 231, 166, 446, 397, 453, 300, 300, 193, 610, 233, 348, 163, 182, 198, 121, 355, 199, 481, 136, 40, 136, 72, 518, 185, 158, 271, 130, 117, 180, 142, 60, 142, 195, 157, 124, 193, 195, 192, 63, 389, 325, 288, 254, 148, 539, 214, 137, 241, 172, 355, 216, 490, 242, 197, 63, 336, 365, 129, 134, 164, 120, 332, 85, 272, 249, 120, 98, 141, 217, 200, 141, 598, 191, 45, 117, 73, 635, 808, 168, 198, 669, 142, 352, 205, 262, 207, 977, 783, 58, 327, 167, 197, 237, 258, 116, 198, 133, 195, 357, 246, 54, 221, 251, 289, 171, 188, 369, 527, 179, 97, 156, 589, 186, 313, 286, 229, 321, 115, 179, 77, 320, 777, 119, 388, 189, 130, 316, 437, 185, 161, 76, 130, 411, 183, 797, 139, 129, 454, 149, 366, 129, 61, 386, 83, 141, 122, 119, 125, 229, 449, 46, 115, 281, 553, 221, 468, 153, 123, 61, 119, 156, 199, 150, 511, 426, 409, 160, 115, 170, 620, 159, 305, 89, 149, 74, 134, 214, 173, 391, 35, 123, 1006, 83, 203, 393, 58, 100, 107, 283, 126, 494, 112, 188, 438, 232, 120, 113, 135, 190, 79, 237, 170, 217, 53, 145, 314, 196, 628, 165, 241, 135, 32, 265, 115, 205, 123, 159, 120, 44, 98, 376, 61, 331, 170, 127, 677, 139, 579, 59, 163, 226, 302, 221, 157, 741, 838, 182, 39, 156, 553, 500, 618, 267, 62, 255, 398, 275, 160, 126, 294, 86, 124, 119, 303, 282, 258, 79, 93, 117, 260, 75, 263, 294, 268, 438, 216, 279, 144, 184, 126, 323, 165, 107, 124, 214, 136, 229, 207, 652, 68, 239, 198, 150, 233, 153, 207, 137, 134, 87, 139, 98, 136, 80, 105, 413, 208, 370, 174, 652, 252, 292, 121, 434, 567, 427, 196, 118, 996, 134, 68, 193, 294, 131, 88, 218, 121, 171, 735, 97, 372, 233, 55, 173, 138, 150, 575, 388, 64, 184, 156, 117, 116, 130, 485, 161, 257, 531, 162, 192, 117, 121, 97, 268, 136, 161, 116, 143, 275, 368, 336, 61, 122, 146, 121, 339, 152, 130, 173, 124, 215, 182, 39, 187, 232, 191, 577, 682, 463, 142, 81, 160, 123, 342, 207, 97, 420, 73, 221, 145, 472, 379, 229, 568, 221, 201, 119, 753, 243, 155, 467, 112, 89, 271, 436, 109, 174, 587, 308, 396, 104, 137, 197, 171, 130, 210, 139, 305, 262, 155, 140, 499, 168, 274, 623, 1008, 358, 142, 242, 275, 269, 157, 134, 131, 239, 250, 80, 809, 270, 163, 141, 263, 262, 130, 151, 145, 241, 256, 122, 93, 391, 142, 137, 293, 251, 162, 120, 378, 169, 52, 217, 493, 245, 93, 295, 298, 152, 148, 476, 626, 168, 190, 81, 348, 389, 78, 106, 157, 255, 163, 408, 404, 128, 146, 137, 154, 321, 212, 117, 358, 195, 139, 255, 181, 179, 122, 160, 134, 414, 199, 500, 193, 128, 91, 462, 97, 343, 51, 94, 186, 132, 43, 287, 104, 559, 58, 79, 687, 81, 141, 163, 229, 104, 105, 51, 64, 398, 188, 204, 83, 191, 135, 520, 86, 592, 113, 146, 104, 147, 202, 222, 61, 286, 235, 202, 650, 107, 92, 148, 146, 287, 148, 335, 80, 1009, 127, 229, 388, 563, 95, 149, 139, 154, 116, 166, 354, 510, 28, 287, 115, 199, 490, 62, 191, 180, 127, 527, 115, 261, 126, 194, 200, 114, 217, 139, 320, 234, 80, 141, 38, 92, 411, 259, 408, 107, 469, 517, 140, 209, 131, 206, 184, 100, 88, 127, 140, 63, 199, 169, 230, 638, 65, 191, 195, 138, 197, 300, 222, 1003, 122, 120, 788, 143, 119, 429, 280, 107, 121, 205, 420, 405, 128, 294, 166, 272, 94, 131, 283, 692, 374, 73, 166, 383, 166, 138, 52, 55, 166, 358, 163, 145, 83, 135, 110, 166, 143, 269, 136, 129, 813, 289, 135, 75, 61, 300, 181, 148, 144, 175, 242, 183, 176, 545, 107, 65, 275, 376, 166, 146, 165, 186, 139, 115, 208, 266, 725, 153, 298, 216, 244, 123, 197, 764, 955, 181, 96, 196, 167, 93, 415, 133, 102, 285, 129, 211, 71, 226, 192, 143, 424, 371, 70, 218, 176, 79, 118, 238, 336, 154, 620, 128, 145, 249, 250, 74, 140, 120, 252, 135, 190, 210, 459, 50, 120, 692, 130, 455, 232, 166, 193, 222, 140, 448, 799, 444, 309, 164, 132, 279, 128, 211, 368, 246, 114, 842, 188, 290, 230, 142, 203, 141, 435, 843, 267, 205, 334, 62, 56, 337, 93, 361, 159, 537, 103, 129, 147, 811, 199, 216, 146, 531, 142, 240, 154, 77, 257, 262, 244, 713, 211, 113, 209, 155, 110, 92, 235, 250, 163, 322, 170, 162, 273, 330, 131, 174, 108, 304, 53, 123, 88, 234, 269, 184, 125, 122, 135, 119, 136, 121, 167, 31, 138, 140, 491, 171, 133, 145, 162, 180, 129, 187, 258, 134, 113, 67, 249, 170, 116, 194, 172, 916, 262, 260, 180, 196, 112, 269, 404, 252, 152, 159, 409, 111, 471, 155, 147, 955, 186, 349, 219, 138, 172, 179, 174, 242, 117, 171, 356, 233, 166, 365, 388, 83, 144, 132, 200, 173, 325, 216, 345, 159, 120, 386, 195, 125, 118, 90, 155, 76, 109, 163, 103, 125, 289, 758, 229, 246, 423, 130, 210, 48, 271, 219, 235, 137, 113, 60, 56, 230, 61, 561, 133, 329, 155, 166, 565, 150, 163, 489, 150, 322, 455, 95, 171, 135, 120, 198, 312, 326, 68, 211, 551, 71, 128, 126, 132, 69, 253, 181, 150, 40, 176, 135, 227, 122, 183, 79, 253, 506, 172, 58, 88, 481, 271, 121, 127, 400, 59, 231, 677, 181, 417, 137, 139, 109, 116, 642, 129, 743, 158, 104, 159, 115, 230, 576, 289, 271, 199, 113, 211, 516, 114, 177, 110, 123, 257, 906, 153, 123, 365, 144, 132, 264, 120, 192, 185, 204, 133, 94, 169, 90, 223, 85, 389, 228, 404, 56, 185, 202, 528, 625, 189, 151, 77, 259, 151, 256, 648, 121, 132, 429, 499, 121, 136, 251, 80, 127, 150, 85, 167, 267, 338, 162, 115, 176, 110, 239, 424, 760, 325, 165, 272, 155, 149, 224, 284, 312, 129, 301, 919, 264, 308, 203, 525, 129, 302, 223, 193, 125, 120, 138, 311, 325, 152, 128, 351, 76, 147, 188, 209, 689, 348, 168, 133, 312, 168, 172, 147, 119, 48, 130, 149, 284, 88, 314, 205, 235, 214, 122, 551, 119, 113, 208, 208, 45, 283, 193, 55, 208, 121, 56, 226, 486, 313, 137, 127, 128, 162, 272, 360, 134, 468, 235, 143, 272, 274, 207, 194, 203, 143, 153, 52, 78, 538, 523, 235, 380, 116, 232, 319, 65, 134, 252, 91, 269, 232, 330, 138, 96, 130, 156, 246, 502, 126, 108, 102, 107, 368, 135, 255, 130, 156, 191, 216, 102, 122, 158, 944, 109, 595, 119, 275, 121, 129, 472, 630, 180, 100, 78, 401, 302, 151, 211, 139, 846, 551, 70, 194, 120, 162, 155, 125, 278, 150, 823, 94, 153, 630, 124, 195, 624, 268, 105, 388, 321, 368, 107, 727, 412, 876, 286, 136, 315, 132, 292, 1015, 127, 205, 257, 221, 284, 88, 227, 498, 700, 582, 188, 213, 78, 126, 84, 191, 88, 817, 203, 509, 225, 140, 127, 183, 134, 243, 146, 73, 519, 222, 128, 216, 202, 143, 133, 180, 65, 285, 161, 175, 205, 274, 113, 148, 268, 252, 174, 166, 119, 143, 198, 272, 155, 381, 171, 230, 379, 167, 76, 559, 173, 359, 43, 46, 429, 191, 119, 223, 186, 308, 400, 36, 56, 319, 114, 229, 159, 645, 144, 211, 43, 36, 165, 122, 244, 41, 108, 82, 307, 423, 48, 198, 202, 676, 191, 154, 693, 983, 54, 367, 118, 560, 137, 163, 139, 133, 474, 412, 288, 146, 206, 377, 120, 165, 239, 306, 52, 84, 193, 479, 442, 176, 279, 141, 198, 68, 223, 179, 401, 337, 98, 191, 292, 909, 189, 244, 131, 151, 348, 224, 268, 200, 253, 123, 302, 241, 168, 379, 177, 128, 193, 137, 339, 725, 125, 272, 69, 136, 140, 183, 669, 135, 423, 236, 383, 393, 109, 174, 147, 104, 202, 139, 310, 72, 48, 133, 404, 292, 83, 195, 181, 141, 131, 635, 550, 92, 121, 119, 383, 163, 42, 128, 250, 123, 241, 176, 130, 130, 147, 124, 156, 406, 138, 165, 142, 138, 459, 45, 262, 220, 120, 166, 219, 149, 511, 285, 131, 534, 178, 92, 107, 140, 269, 113, 211, 982, 50, 373, 470, 162, 185, 170, 328, 166, 223, 144, 298, 567, 160, 176, 213, 182, 126, 202, 442, 281, 469, 453, 75, 105, 201, 248, 139, 348, 679, 140, 159, 316, 95, 151, 181, 971, 307, 394, 149, 113, 63, 631, 100, 205, 503, 306, 95, 141, 119, 222, 171, 116, 277, 1005, 12, 553, 164, 118, 149, 672, 184, 53, 124, 328, 170, 234, 268, 143, 67, 154, 476, 984, 361, 49, 56, 828, 168, 163, 413, 146, 132, 217, 129, 236, 214, 104, 355, 194, 267, 207, 152, 168, 52, 374, 112, 362, 828, 435, 188, 122, 1030, 486, 143, 528, 158, 203, 270, 205, 144, 560, 166, 69, 155, 66, 110, 114, 251, 335, 238, 444, 143, 552, 864, 154, 108, 146, 60, 234, 165, 471, 70, 383, 48, 484, 50, 94, 255, 144, 511, 159, 138, 230, 131, 196, 130, 82, 154, 67, 119, 371, 139, 484, 424, 343, 415, 189, 358, 254, 481, 126, 171, 229, 165, 370, 88, 193, 155, 246, 221, 208, 143, 113, 219, 165, 157, 160, 185, 149, 200, 739, 221, 144, 79, 68, 124, 136, 82, 345, 108, 128, 166, 192, 59, 93, 357, 173, 380, 739, 207, 69, 132, 314, 153, 135, 131, 78, 272, 179, 127, 155, 133, 201, 350, 119, 167, 310, 298, 376, 395, 149, 42, 146, 156, 151, 606, 187, 138, 60, 145, 221, 69, 100, 201, 591, 182, 104, 108, 129, 120, 40, 142, 260, 131, 183, 310, 127, 286, 88, 86, 160, 265, 139, 141, 222, 146, 190, 807, 212, 64, 106, 89, 228, 109, 47, 609, 215, 300, 124, 202, 64, 162, 87, 413, 155, 122, 351, 343, 61, 262, 343, 307, 294, 218, 274, 194, 216, 224, 116, 103, 135, 694, 203, 75, 424, 195, 165, 233, 475, 165, 177, 184, 378, 266, 139, 349, 281, 982, 1008, 84, 103, 424, 159, 155, 127, 136, 407, 97, 283, 130, 182, 151, 177, 149, 214, 368, 260, 171, 419, 600, 233, 233, 315, 174, 420, 175, 187, 140, 233, 125, 142, 322, 42, 372, 57, 277, 81, 133, 108, 207, 132, 432, 575, 158, 187, 225, 295, 425, 15, 391, 279, 169, 254, 313, 469, 141, 50, 174, 148, 117, 104, 639, 344, 315, 127, 998, 206, 122, 394, 77, 145, 202, 380, 251, 468, 89, 114, 186, 122, 257, 154, 584, 126, 136, 150, 472, 141, 436, 833, 149, 112, 192, 318, 124, 278, 213, 134, 189, 372, 389, 228, 210, 173, 122, 164, 497, 84, 64, 178, 194, 111, 212, 84, 174, 380, 149, 158, 246, 150, 142, 422, 181, 638, 200, 345, 144, 58, 172, 141, 968, 131, 123, 245, 241, 239, 130, 575, 294, 142, 454, 196, 143, 295, 162, 138, 231, 155, 146, 182, 128, 361, 332, 86, 112, 86, 193, 149, 132, 231, 189, 156, 235, 218, 417, 97, 125, 69, 143, 151, 209, 128, 163, 158, 126, 54, 379, 146, 387, 649, 134, 128, 120, 476, 106, 74, 83, 153, 137, 368, 123, 146, 169, 134, 134, 451, 129, 179, 282, 128, 337, 706, 70, 159, 156, 196, 64, 190, 134, 176, 245, 233, 155, 133, 189, 307, 361, 986, 220, 396, 119, 330, 152, 145, 145, 193, 1036, 192, 86, 129, 259, 495, 207, 187, 47, 757, 993, 136, 89, 212, 670, 412, 35, 122, 168, 147, 254, 343, 385, 223, 106, 227, 622, 199, 369, 523, 276, 262, 268, 171, 176, 713, 403, 1006, 153, 190, 118, 314, 557, 132, 286, 263, 466, 215, 869, 395, 124, 179, 125, 147, 214, 159, 113, 314, 218, 53, 97, 952, 127, 128, 187, 615, 156, 107, 197, 232, 125, 190, 124, 326, 353, 262, 359, 182, 315, 178, 176, 279, 249, 248, 321, 83, 178, 944, 96, 203, 245, 95, 141, 86, 124, 202, 135, 55, 263, 221, 216, 169, 435, 132, 170, 174, 181, 203, 374, 232, 238, 220, 695, 270, 400, 277, 116, 283, 120, 208, 318, 136, 127, 625, 407, 151, 105, 225, 163, 428, 401, 280, 187, 373, 243, 348, 123, 184, 204, 385, 59, 193, 558, 158, 127, 414, 298, 305, 279, 314, 131, 160, 255, 397, 331, 124, 79, 168, 134, 113, 112, 143, 94, 165, 179, 174, 210, 239, 130, 308, 174, 247, 331, 115, 172, 97, 263, 187, 295, 183, 168, 144, 40, 101, 236, 235, 159, 172, 144, 674, 159, 193, 156, 161, 646, 133, 286, 241, 170, 221, 337, 562, 141, 157, 106, 413, 138, 163, 546, 281, 175, 182, 185, 731, 487, 99, 235, 68, 156, 66, 322, 351, 210, 53, 207, 137, 151, 518, 470, 310, 186, 215, 118, 574, 135, 278, 150, 140, 115, 161, 267, 169, 140, 371, 120, 151, 89, 111, 92, 274, 121, 225, 361, 436, 115, 1854, 191, 105, 139, 149, 101, 138, 222, 139, 89, 174, 266, 170, 342, 227, 231, 65, 426, 247, 155, 405, 91, 280, 230, 131, 1015, 346, 126, 175, 275, 227, 339, 112, 185, 62, 370, 113, 71, 488, 159, 138, 126, 185, 260, 237, 273, 114, 170, 187, 113, 247, 244, 307, 298, 789, 144, 123, 256, 120, 369, 298, 310, 86, 125, 443, 313, 138, 61, 624, 589, 164, 159, 200, 72, 207, 89, 117, 131, 129, 83, 215, 121, 126, 622, 371, 147, 89, 158, 604, 145, 626, 244, 333, 138, 197, 143, 237, 159, 223, 172, 247, 195, 120, 153, 191, 64, 121, 202, 594, 686, 367, 369, 178, 908, 95, 431, 118, 321, 184, 249, 517, 80, 208, 47, 190, 272, 249, 507, 371, 123, 125, 238, 636, 248, 107, 366, 483, 978, 83, 221, 382, 111, 150, 114, 399, 196, 205, 302, 109, 213, 131, 71, 141, 52, 375, 193, 153, 242, 200, 236, 197, 1018, 45, 141, 56, 133, 142, 640, 183, 166, 173, 175, 232, 124, 188, 133, 162, 338, 237, 182, 214, 440, 371, 382, 154, 196, 125, 279, 121, 150, 165, 176, 260, 165, 31, 194, 285, 444, 165, 103, 264, 157, 209, 183, 306, 127, 495, 690, 83, 125, 139, 61, 94, 134, 50, 389, 294, 67, 382, 80, 581, 159, 143, 204, 280, 201, 239, 160, 266, 119, 57, 276, 344, 149, 265, 248, 54, 123, 159, 154, 123, 152, 219, 171, 244, 214, 292, 359, 164, 50, 167, 158, 230, 133, 251, 103, 743, 387, 501, 240, 326, 103, 198, 426, 542, 57, 168, 629, 391, 205, 494, 132, 313, 184, 122, 195, 181, 58, 35, 122, 112, 199, 300, 206, 136, 454, 128, 228, 178, 138, 210, 57, 163, 377, 165, 256, 252, 144, 115, 128, 356, 200, 204, 236, 275, 202, 233, 132, 123, 166, 772, 237, 70, 171, 342, 269, 293, 58, 268, 287, 153, 222, 455, 124, 116, 141, 106, 169, 139, 161, 208, 286, 291, 262, 143, 500, 267, 134, 224, 223, 135, 163, 161, 141, 236, 51, 292, 430, 176, 417, 585, 193, 649, 161, 195, 268, 162, 114, 532, 328, 140, 109, 195, 304, 187, 621, 124, 139, 140, 164, 168, 128, 202, 128, 174, 354, 170, 780, 97, 159, 207, 154, 289, 598, 143, 13, 133, 508, 292, 441, 259, 161, 211, 362, 342, 296, 163, 141, 124, 203, 82, 132, 745, 140, 77, 453, 194, 221, 210, 461, 158, 290, 265, 148, 150, 130, 376, 826, 137, 139, 487, 122, 519, 143, 89, 314, 141, 466, 242, 302, 305, 170, 310, 76, 161, 226, 573, 593, 1003, 185, 120, 190, 496, 118, 661, 215, 292, 527, 699, 217, 195, 495, 83, 149, 129, 168, 251, 381, 137, 190, 154, 402, 110, 102, 297, 256, 877, 240, 158, 334, 189, 411, 182, 444, 371, 360, 91, 319, 146, 272, 89, 676, 191, 60, 405, 64, 148, 45, 122, 168, 207, 241, 125, 114, 127, 104, 243, 129, 153, 169, 100, 735, 433, 286, 144, 511, 121, 317, 132, 145, 478, 710, 329, 156, 85, 204, 78, 166, 126, 136, 95, 319, 303, 242, 133, 239, 171, 178, 140, 351, 109, 311, 96, 184, 411, 509, 610, 748, 182, 255, 199, 187, 137, 641, 129, 255, 133, 135, 328, 228, 137, 361, 135, 56, 186, 102, 182, 409, 536, 604, 107, 156, 757, 158, 377, 225, 147, 272, 304, 260, 302, 105, 192, 114, 445, 260, 240, 164, 238, 280, 271, 185, 618, 731, 118, 158, 257, 300, 165, 118, 169, 115, 1306, 423, 94, 189, 121, 146, 37, 873, 175, 139, 206, 182, 155, 115, 852, 196, 175, 131, 310, 95, 61, 507, 398, 203, 407, 342, 274, 113, 108, 233, 165, 417, 189, 168, 165, 61, 396, 79, 308, 115, 294, 144, 293, 515, 124, 246, 410, 156, 190, 118, 547, 142, 114, 239, 611, 147, 725, 195, 566, 102, 138, 398, 777, 303, 174, 663, 49, 519, 234, 208, 164, 145, 315, 287, 206, 148, 143, 175, 44, 110, 223, 672, 215, 127, 18, 132, 59, 69, 120, 130, 166, 200, 157, 93, 428, 129, 73, 96, 166, 248, 361, 52, 119, 106, 329, 429, 150, 424, 231, 527, 84, 112, 205, 192, 41, 78, 73, 130, 143, 215, 670, 175, 218, 84, 69, 147, 207, 129, 92, 248, 168, 776, 581, 238, 846, 306, 788, 167, 162, 351, 499, 174, 126, 303, 163, 908, 333, 187, 329, 140, 792, 102, 170, 354, 68, 179, 60, 703, 235, 180, 52, 287, 177, 140, 136, 131, 351, 139, 190, 386, 168, 112, 135, 403, 97, 208, 146, 275, 388, 1015, 102, 96, 121, 343, 480, 151, 47, 297, 140, 144, 117, 958, 162, 389, 78, 190, 150, 210, 123, 166, 296, 173, 334, 267, 587, 43, 144, 151, 338, 358, 311, 154, 430, 662, 144, 116, 205, 721, 168, 123, 122, 167, 171, 146, 471, 262, 174, 446, 124, 133, 191, 258, 318, 170, 200, 208, 131, 113, 468, 252, 204, 186, 240, 53, 413, 424, 177, 213, 38, 157, 122, 192, 153, 108, 804, 53, 152, 57, 80, 180, 144, 157, 156, 208, 184, 154, 131, 191, 551, 242, 704, 78, 123, 195, 196, 196, 439, 241, 153, 168, 101, 291, 212, 123, 679, 80, 706, 163, 67, 257, 179, 238, 105, 155, 278, 544, 269, 102, 102, 173, 78, 45, 150, 165, 170, 149, 415, 811, 263, 182, 225, 259, 826, 111, 218, 156, 269, 308, 150, 175, 145, 131, 467, 115, 242, 291, 181, 816, 399, 142, 461, 853, 315, 264, 208, 183, 174, 483, 94, 145, 319, 224, 135, 101, 217, 267, 189, 336, 93, 204, 286, 219, 429, 106, 175, 622, 151, 336, 123, 129, 905, 222, 229, 254, 129, 202, 261, 594, 110, 448, 116, 162, 128, 323, 177, 144, 507, 489, 217, 130, 588, 177, 253, 167, 122, 123, 452, 259, 376, 517, 632, 85, 47, 303, 235, 357, 216, 348, 224, 115, 186, 311, 66, 104, 89, 120, 578, 778, 123, 468, 224, 307, 128, 113, 114, 185, 260, 38, 391, 102, 480, 76, 436, 225, 431, 452, 239, 53, 83, 249, 223, 199, 135, 382, 142, 279, 116, 129, 343, 210, 400, 117, 122, 55, 156, 253, 182, 46, 67, 144, 132, 362, 200, 159, 53, 124, 107, 153, 141, 301, 110, 155, 239, 187, 345, 184, 170, 86, 422, 207, 151, 133, 365, 125, 191, 127, 271, 444, 49, 73, 167, 121, 533, 408, 126, 141, 142, 342, 170, 152, 203, 355, 547, 104, 249, 111, 160, 183, 385, 338, 244, 108, 127, 149, 153, 144, 71, 249, 198, 127, 282, 516, 250, 209, 78, 582, 1291, 38, 240, 148, 192, 728, 237, 173, 112, 133, 241, 432, 177, 202, 484, 167, 291, 53, 211, 227, 196, 652, 239, 509, 95, 191, 144, 123, 219, 197, 313, 360, 125, 241, 141, 276, 266, 179, 329, 171, 82, 861, 280, 168, 380, 427, 180, 167, 169, 322, 132, 518, 83, 122, 136, 189, 542, 104, 138, 181, 171, 51, 407, 327, 123, 221, 40, 178, 116, 122, 139, 312, 350, 124, 120, 249, 126, 121, 189, 483, 40, 450, 152, 137, 172, 175, 316, 597, 553, 159, 134, 274, 248, 214, 364, 332, 46, 252, 138, 274, 145, 494, 344, 130, 189, 128, 136, 57, 166, 161, 267, 123, 316, 406, 176, 127, 329, 142, 98, 86, 146, 178, 283, 60, 135, 200, 191, 116, 54, 131, 58, 180, 166, 137, 156, 163, 121, 153, 186, 469, 110, 341, 161, 426, 210, 384, 159, 128, 134, 106, 55, 257, 146, 1024, 127, 127, 118, 101, 152, 160, 135, 172, 128, 483, 118, 555, 80, 137, 196, 310, 193, 500, 334, 150, 121, 767, 240, 253, 208, 77, 728, 163, 631, 248, 79, 64, 630, 306, 228, 200, 74, 626, 459, 585, 384, 73, 149, 63, 342, 144, 327, 204, 136, 460, 140, 171, 124, 101, 186, 141, 118, 200, 199, 174, 240, 377, 216, 136, 42, 274, 130, 238, 267, 66, 176, 154, 72, 282, 250, 162, 139, 409, 145, 666, 290, 164, 787, 167, 308, 140, 114, 149, 111, 231, 554, 270, 153, 273, 649, 322, 189, 197, 269, 170, 192, 333, 105, 42, 230, 676, 332, 435, 612, 276, 130, 842, 109, 65, 667, 124, 592, 518, 50, 198, 222, 144, 65, 576, 168, 162, 141, 129, 116, 148, 341, 50, 138, 199, 108, 329, 489, 424, 677, 137, 597, 209, 670, 805, 374, 518, 110, 227, 107, 306, 161, 213, 319, 358, 159, 151, 135, 155, 163, 134, 181, 238, 222, 123, 350, 130, 271, 70, 200, 148, 164, 208, 146, 156, 103, 257, 125, 138, 35, 143, 182, 264, 184, 214, 43, 130, 163, 141, 191, 143, 230, 195, 133, 143, 162, 194, 143, 409, 245, 84, 354, 139, 117, 74, 1034, 310, 88, 171, 122, 149, 160, 354, 368, 336, 553, 203, 569, 59, 133, 256, 157, 413, 161, 333, 171, 289, 122, 155, 199, 275, 72, 123, 139, 109, 482, 205, 413, 241, 153, 165, 140, 246, 416, 176, 281, 82, 125, 99, 273, 40, 159, 100, 182, 167, 228, 309, 44, 283, 124, 460, 153, 194, 414, 129, 178, 157, 190, 334, 124, 208, 50, 670, 172, 61, 234, 64, 173, 229, 101, 189, 314, 240, 135, 212, 360, 238, 899, 129, 121, 52, 475, 342, 102, 104, 155, 116, 255, 400, 131, 52, 359, 139, 122, 152, 226, 168, 234, 291, 580, 56, 86, 160, 48, 154, 150, 165, 300, 151, 65, 97, 131, 180, 1039, 150, 448, 248, 166, 351, 360, 181, 231, 565, 118, 210, 217, 96, 164, 157, 213, 502, 106, 160, 64, 122, 216, 161, 154, 376, 154, 237, 134, 281, 184, 145, 737, 111, 141, 406, 132, 148, 229, 69, 52, 159, 115, 144, 580, 214, 140, 320, 127, 88, 375, 122, 242, 230, 202, 106, 90, 96, 386, 134, 410, 186, 340, 126, 104, 123, 175, 536, 506, 272, 487, 106, 127, 172, 254, 218, 210, 172, 365, 288, 157, 144, 76, 138, 56, 604, 173, 271, 176, 101, 282, 129, 122, 153, 173, 147, 86, 965, 119, 143, 198, 156, 164, 58, 753, 371, 133, 698, 133, 244, 289, 117, 127, 212, 194, 131, 57, 275, 256, 374, 299, 130, 73, 301, 50, 225, 191, 210, 284, 81, 104, 197, 255, 128, 122, 275, 276, 117, 145, 681, 47, 509, 301, 204, 206, 163, 443, 164, 136, 282, 178, 347, 163, 200, 218, 232, 285, 319, 113, 304, 259, 183, 191, 396, 903, 121, 234, 296, 321, 108, 161, 243, 106, 70, 611, 239, 117, 129, 195, 134, 754, 220, 399, 234, 628, 137, 102, 533, 126, 182, 173, 159, 185, 242, 310, 390, 280, 159, 228, 159, 598, 178, 168, 266, 327, 642, 148, 250, 183, 294, 434, 182, 145, 247, 325, 255, 222, 625, 144, 196, 229, 247, 127, 319, 123, 204, 135, 130, 233, 49, 346, 52, 433, 55, 199, 92, 92, 173, 495, 372, 158, 122, 117, 75, 203, 167, 628, 81, 225, 192, 233, 246, 741, 207, 1003, 238, 227, 162, 161, 167, 190, 165, 293, 250, 115, 218, 147, 320, 119, 59, 82, 108, 358, 122, 107, 753, 370, 420, 133, 99, 674, 169, 204, 247, 117, 46, 183, 193, 193, 59, 303, 134, 122, 144, 214, 161, 92, 70, 73, 154, 337, 141, 257, 155, 248, 377, 123, 148, 184, 113, 31, 143, 81, 104, 123, 138, 112, 88, 362, 232, 152, 51, 491, 239, 487, 156, 80, 632, 153, 137, 194, 56, 145, 180, 196, 139, 120, 80, 104, 369, 292, 158, 84, 91, 121, 49, 174, 740, 887, 115, 118, 92, 315, 296, 753, 329, 151, 176, 287, 138, 653, 88, 123, 146, 697, 98, 351, 468, 359, 1530, 161, 191, 171, 133, 183, 120, 229, 349, 264, 135, 236, 165, 335, 191, 346, 123, 308, 426, 351, 364, 298, 205, 163, 68, 256, 200, 328, 284, 1018, 186, 263, 43, 304, 502, 773, 228, 155, 675, 206, 111, 504, 135, 121, 38, 151, 535, 987, 230, 900, 301, 106, 663, 170, 114, 123, 669, 225, 98, 262, 126, 504, 143, 199, 280, 541, 106, 125, 190, 29, 192, 303, 104, 121, 176, 160, 129, 266, 121, 114, 209, 42, 154, 116, 117, 183, 357, 62, 79, 218, 148, 139, 203, 126, 198, 594, 194, 107, 134, 146, 200, 126, 555, 126, 255, 158, 137, 203, 167, 98, 142, 150, 175, 104, 78, 241, 304, 298, 154, 191, 130, 762, 210, 132, 136, 447, 209, 67, 197, 223, 176, 145, 327, 726, 158, 348, 557, 132, 294, 154, 382, 114, 195, 266, 212, 173, 121, 140, 268, 246, 114, 222, 81, 139, 726, 154, 191, 179, 157, 138, 847, 128, 137, 128, 244, 342, 569, 169, 194, 222, 110, 114, 253, 473, 126, 130, 169, 143, 119, 110, 78, 59, 152, 97, 266, 325, 46, 161, 59, 223, 81, 116, 305, 485, 300, 307, 449, 245, 154, 449, 155, 111, 354, 278, 262, 158, 378, 104, 187, 138, 122, 196, 151, 123, 410, 254, 373, 191, 128, 421, 53, 131, 107, 290, 183, 170, 442, 321, 147, 176, 177, 255, 316, 59, 207, 135, 126, 92, 140, 924, 200, 212, 137, 114, 210, 254, 156, 308, 123, 236, 47, 287, 396, 291, 66, 274, 158, 133, 289, 136, 77, 153, 191, 246, 142, 53, 49, 114, 400, 155, 154, 404, 140, 208, 177, 155, 126, 634, 672, 146, 289, 114, 117, 183, 63, 80, 565, 337, 171, 173, 83, 297, 77, 219, 119, 243, 696, 202, 140, 251, 120, 166, 62, 168, 127, 160, 93, 446, 174, 228, 236, 138, 494, 175, 137, 974, 122, 237, 212, 154, 377, 439, 215, 113, 112, 171, 933, 193, 208, 128, 171, 433, 281, 100, 155, 165, 188, 123, 223, 179, 271, 97, 206, 430, 202, 176, 136, 472, 70, 105, 387, 309, 333, 635, 192, 50, 427, 115, 147, 301, 75, 287, 184, 158, 59, 159, 276, 274, 291, 260, 110, 127, 240, 212, 122, 159, 143, 62, 115, 243, 126, 435, 113, 168, 282, 192, 300, 466, 144, 145, 602, 121, 334, 121, 53, 189, 314, 113, 254, 126, 21, 364, 776, 46, 217, 150, 116, 181, 633, 365, 139, 184, 460, 349, 350, 777, 134, 153, 148, 64, 117, 74, 132, 155, 324, 293, 236, 141, 473, 733, 175, 117, 136, 363, 86, 246, 52, 158, 140, 278, 123, 598, 89, 240, 424, 174, 289, 515, 90, 291, 112, 122, 182, 236, 132, 150, 680, 245, 492, 469, 618, 138, 110, 174, 149, 129, 178, 69, 208, 254, 50, 96, 249, 89, 119, 111, 223, 378, 248, 447, 155, 313, 191, 499, 187, 481, 165, 171, 96, 102, 203, 201, 295, 373, 514, 113, 808, 61, 164, 199, 197, 278, 256, 157, 289, 735, 114, 363, 115, 181, 217, 328, 541, 356, 50, 230, 372, 365, 127, 173, 270, 81, 123, 133, 100, 218, 390, 125, 231, 141, 296, 166, 179, 291, 330, 316, 131, 171, 278, 713, 244, 162, 151, 505, 177, 123, 458, 170, 172, 590, 131, 146, 252, 333, 153, 142, 159, 237, 346, 237, 225, 128, 332, 230, 118, 79, 53, 270, 458, 104, 109, 152, 189, 150, 254, 142, 64, 211, 198, 126, 517, 42, 131, 223, 299, 355, 378, 231, 174, 111, 99, 164, 144, 176, 157, 259, 180, 138, 137, 667, 710, 177, 512, 125, 250, 305, 205, 431, 141, 59, 168, 143, 196, 146, 171, 123, 61, 523, 278, 570, 58, 778, 291, 177, 130, 109, 43, 382, 242, 65, 142, 84, 61, 518, 320, 134, 256, 59, 94, 74, 131, 89, 221, 120, 69, 149, 132, 198, 204, 111, 162, 216, 187, 241, 250, 181, 177, 149, 73, 142, 197, 188, 138, 135, 982, 137, 133, 114, 147, 283, 151, 88, 295, 93, 95, 701, 121, 237, 337, 90, 172, 276, 119, 196, 249, 140, 170, 582, 252, 352, 157, 77, 163, 137, 101, 137, 174, 242, 140, 237, 163, 170, 615, 463, 308, 184, 134, 241, 489, 129, 772, 40, 124, 683, 253, 125, 289, 154, 359, 45, 449, 171, 131, 388, 127, 307, 146, 107, 121, 283, 131, 269, 466, 122, 555, 165, 141, 155, 204, 539, 210, 435, 693, 225, 124, 116, 204, 136, 120, 172, 183, 272, 158, 130, 473, 60, 112, 531, 162, 399, 265, 126, 512, 159, 129, 181, 179, 143, 165, 90, 206, 134, 389, 187, 226, 116, 97, 1005, 329, 595, 102, 72, 205, 204, 101, 327, 180, 167, 149, 208, 128, 59, 108, 90, 190, 164, 130, 556, 381, 32, 121, 860, 140, 152, 79, 211, 185, 90, 309, 154, 826, 192, 155, 294, 445, 141, 837, 164, 141, 143, 98, 115, 228, 168, 182, 157, 338, 186, 396, 107, 819, 258, 115, 167, 344, 268, 213, 159, 377, 452, 103, 79, 134, 169, 153, 555, 337, 130, 131, 754, 439, 97, 196, 361, 146, 133, 383, 172, 72, 199, 279, 301, 60, 387, 138, 227, 106, 126, 128, 115, 69, 138, 137, 293, 406, 208, 65, 130, 124, 133, 265, 232, 679, 138, 306, 700, 252, 180, 186, 540, 149, 295, 128, 187, 253, 167, 133, 146, 598, 117, 122, 85, 148, 51, 287, 197, 164, 151, 191, 170, 339, 158, 198, 181, 561, 122, 151, 139, 197, 435, 493, 43, 160, 191, 161, 203, 119, 136, 232, 292, 717, 164, 88, 88, 548, 117, 215, 106, 244, 108, 164, 351, 399, 106, 525, 63, 107, 214, 179, 46, 303, 249, 149, 1027, 182, 239, 232, 199, 162, 176, 220, 149, 115, 109, 181, 196, 581, 135, 125, 237, 68, 157, 212, 99, 166, 318, 186, 227, 192, 168, 159, 212, 130, 357, 355, 157, 132, 115, 119, 114, 60, 191, 279, 132, 174, 120, 371, 317, 198, 159, 92, 140, 414, 303, 42, 213, 114, 150, 98, 185, 521, 392, 274, 60, 618, 169, 94, 164, 403, 200, 215, 186, 309, 187, 342, 74, 126, 165, 531, 116, 170, 156, 271, 287, 137, 237, 141, 78, 199, 699, 1165, 126, 373, 324, 145, 544, 148, 56, 155, 230, 265, 59, 213, 170, 91, 311, 184, 147, 445, 280, 58, 102, 365, 131, 244, 112, 128, 417, 872, 153, 180, 145, 77, 189, 309, 218, 148, 94, 286, 211, 355, 602, 112, 124, 141, 337, 155, 175, 93, 414, 99, 137, 173, 233, 179, 245, 209, 48, 232, 72, 199, 187, 135, 164, 170, 462, 598, 228, 142, 300, 134, 459, 364, 805, 130, 149, 71, 135, 162, 131, 240, 175, 250, 127, 158, 506, 236, 169, 209, 143, 136, 131, 684, 154, 406, 313, 272, 605, 174, 439, 252, 107, 230, 939, 132, 274, 146, 829, 191, 62, 128, 423, 94, 167, 176, 706, 324, 134, 92, 241, 55, 314, 339, 355, 155, 342, 268, 141, 139, 116, 78, 171, 249, 450, 124, 62, 311, 585, 323, 66, 386, 170, 953, 204, 232, 118, 92, 855, 231, 133, 426, 236, 381, 114, 622, 418, 124, 175, 120, 328, 177, 122, 332, 384, 448, 454, 169, 543, 113, 391, 89, 242, 338, 124, 272, 180, 191, 131, 123, 150, 296, 430, 200, 177, 480, 217, 59, 492, 143, 198, 242, 334, 257, 35, 97, 242, 91, 155, 258, 1016, 194, 117, 365, 90, 194, 321, 824, 753, 194, 103, 156, 219, 202, 187, 218, 148, 92, 154, 199, 505, 335, 125, 311, 1022, 144, 268, 518, 354, 138, 283, 200, 143, 370, 327, 262, 342, 311, 177, 101, 70, 254, 189, 124, 246, 126, 290, 152, 173, 295, 537, 221, 247, 347, 119, 156, 128, 171, 126, 219, 405, 595, 158, 149, 314, 112, 153, 158, 238, 154, 132, 137, 62, 112, 157, 243, 143, 160, 285, 132, 671, 63, 197, 203, 484, 162, 132, 844, 128, 176, 963, 108, 129, 201, 306, 443, 170, 135, 371, 133, 338, 106, 150, 58, 267, 122, 725, 423, 168, 161, 151, 296, 162, 434, 1012, 209, 109, 108, 128, 445, 125, 385, 196, 96, 123, 153, 174, 79, 156, 166, 58, 242, 66, 283, 140, 390, 142, 168, 142, 276, 221, 285, 128, 172, 1020, 88, 388, 321, 363, 135, 113, 208, 136, 199, 460, 779, 376, 215, 49, 121, 137, 64, 169, 473, 115, 276, 162, 207, 196, 156, 97, 375, 75, 137, 274, 176, 146, 155, 185, 59, 203, 276, 137, 259, 147, 322, 198, 65, 261, 143, 127, 95, 129, 272, 274, 120, 252, 240, 132, 661, 302, 213, 152, 201, 791, 194, 144, 308, 362, 450, 461, 167, 194, 332, 263, 51, 136, 612, 72, 445, 181, 206, 178, 271, 132, 44, 131, 221, 186, 139, 437, 689, 144, 204, 190, 216, 159, 172, 155, 174, 230, 193, 150, 130, 137, 161, 335, 168, 390, 130, 279, 56, 89, 133, 159, 119, 192, 115, 249, 265, 139, 127, 125, 137, 319, 272, 132, 216, 467, 57, 343, 265, 107, 438, 287, 125, 185, 110, 54, 139, 65, 98, 165, 124, 122, 107, 226, 167, 277, 186, 343, 340, 242, 417, 137, 147, 159, 243, 395, 126, 334, 240, 131, 169, 266, 149, 196, 165, 234, 224, 122, 197, 255, 559, 119, 148, 411, 230, 374, 55, 219, 66, 674, 506, 205, 42, 158, 175, 73, 206, 183, 174, 44, 976, 455, 167, 290, 128, 127, 151, 81, 285, 197, 63, 103, 161, 265, 188, 136, 139, 130, 207, 70, 54, 204, 131, 102, 115, 116, 176, 169, 178, 145, 144, 386, 247, 208, 133, 193, 125, 158, 241, 193, 242, 241, 416, 134, 150, 359, 154, 154, 168, 76, 86, 141, 152, 55, 197, 113, 180, 608, 1337, 169, 66, 146, 165, 67, 715, 129, 90, 117, 264, 838, 37, 337, 105, 190, 190, 1090, 185, 256, 367, 126, 177, 177, 130, 92, 408, 48, 56, 975, 142, 872, 248, 320, 158, 648, 227, 141, 71, 120, 68, 683, 182, 565, 210, 298, 56, 224, 142, 124, 170, 125, 226, 205, 117, 86, 296, 148, 580, 273, 117, 178, 130, 166, 234, 190, 353, 281, 634, 393, 113, 319, 323, 137, 138, 91, 587, 576, 525, 1216, 60, 57, 177, 89, 165, 206, 299, 227, 190, 135, 181, 338, 84, 121, 76, 574, 148, 376, 191, 371, 260, 267, 107, 281, 154, 269, 43, 139, 870, 737, 90, 173, 138, 178, 131, 876, 415, 48, 169, 297, 175, 63, 235, 138, 199, 221, 74, 138, 616, 214, 151, 116, 236, 431, 132, 681, 192, 235, 193, 414, 142, 188, 789, 224, 127, 84, 170, 174, 69, 205, 188, 149, 445, 160, 127, 108, 40, 457, 204, 178, 68, 190, 118, 224, 417, 116, 371, 151, 236, 287, 141, 262, 84, 308, 204, 340, 410, 106, 975, 34, 266, 132, 900, 284, 499, 210, 58, 65, 122, 124, 236, 139, 168, 261, 179, 258, 486, 205, 118, 119, 130, 132, 133, 121, 150, 88, 138, 122, 152, 218, 834, 127, 104, 121, 57, 139, 47, 223, 331, 289, 234, 159, 79, 175, 74, 345, 159, 290, 347, 316, 171, 84, 337, 307, 605, 147, 238, 194, 113, 59, 147, 300, 443, 136, 490, 197, 123, 315, 215, 192, 227, 128, 208, 141, 241, 296, 212, 161, 103, 357, 624, 111, 132, 118, 321, 250, 74, 146, 299, 120, 144, 187, 136, 135, 137, 817, 131, 314, 377, 181, 198, 219, 185, 176, 78, 126, 122, 168, 137, 162, 142, 227, 125, 241, 139, 177, 58, 237, 147, 239, 506, 477, 135, 60, 116, 147, 273, 148, 462, 317, 291, 171, 168, 126, 127, 134, 394, 107, 866, 192, 194, 300, 279, 514, 168, 498, 136, 134, 698, 170, 115, 102, 227, 132, 124, 656, 170, 239, 291, 223, 45, 119, 129, 132, 158, 330, 297, 48, 134, 139, 115, 140, 235, 153, 108, 157, 224, 249, 882, 383, 112, 221, 290, 142, 110, 121, 462, 99, 128, 329, 400, 552, 359, 129, 124, 164, 315, 322, 368, 120, 163, 96, 151, 145, 192, 148, 303, 287, 214, 293, 120, 461, 107, 135, 187, 176, 316, 148, 56, 368, 140, 99, 190, 249, 143, 75, 160, 246, 83, 218, 151, 78, 163, 237, 144, 122, 112, 184, 261, 225, 124, 76, 111, 150, 122, 85, 124, 172, 259, 163, 135, 138, 239, 204, 272, 149, 118, 200, 110, 488, 464, 122, 390, 139, 88, 737, 204, 77, 126, 127, 274, 74, 166, 61, 616, 546, 236, 83, 386, 106, 862, 286, 297, 205, 63, 92, 302, 115, 181, 275, 206, 184, 164, 138, 147, 232, 177, 454, 119, 609, 127, 83, 334, 201, 133, 181, 519, 138, 249, 149, 111, 429, 142, 95, 203, 188, 207, 190, 477, 119, 650, 381, 154, 893, 135, 142, 265, 58, 103, 47, 184, 134, 104, 298, 537, 130, 205, 146, 128, 557, 128, 175, 136, 158, 103, 184, 146, 79, 184, 308, 34, 128, 261, 139, 325, 166, 140, 105, 125, 108, 562, 143, 92, 65, 128, 211, 204, 16, 432, 169, 369, 110, 66, 134, 147, 179, 168, 143, 48, 452, 265, 197, 59, 130, 443, 521, 180, 514, 297, 130, 100, 145, 407, 186, 141, 127, 341, 157, 210, 281, 224, 142, 259, 371, 109, 116, 1018, 225, 206, 136, 76, 527, 191, 81, 989, 125, 135, 988, 132, 478, 210, 640, 361, 431, 194, 50, 269, 221, 207, 130, 259, 125, 208, 164, 686, 86, 63, 157, 308, 116, 369, 120, 123, 144, 38, 226, 529, 536, 293, 312, 125, 198, 281, 34, 151, 181, 106, 118, 283, 228, 274, 619, 150, 280, 106, 345, 369, 210, 134, 39, 157, 210, 205, 218, 157, 229, 225, 245, 156, 171, 134, 94, 57, 116, 190, 201, 797, 234, 233, 329, 263, 319, 41, 346, 236, 229, 139, 506, 150, 306, 121, 336, 107, 156, 222, 187, 330, 415, 68, 161, 239, 139, 163, 182, 136, 173, 296, 75, 160, 688, 99, 128, 296, 660, 252, 251, 176, 158, 158, 68, 116, 125, 148, 123, 220, 147, 167, 776, 464, 126, 98, 490, 170, 163, 346, 125, 340, 150, 71, 377, 164, 447, 288, 212, 367, 460, 221, 375, 531, 417, 203, 291, 117, 595, 62, 274, 532, 257, 342, 512, 133, 198, 196, 207, 132, 132, 290, 425, 253, 471, 166, 155, 207, 116, 216, 321, 128, 144, 183, 160, 99, 173, 158, 329, 133, 215, 119, 108, 160, 154, 168, 237, 118, 316, 453, 143, 366, 168, 116, 50, 563, 669, 184, 156, 127, 148, 181, 393, 278, 226, 58, 206, 440, 301, 179, 89, 192, 487, 230, 156, 236, 140, 148, 129, 323, 97, 380, 62, 247, 210, 481, 209, 367, 301, 16, 125, 280, 13, 259, 287, 351, 62, 157, 30, 126, 149, 66, 350, 175, 64, 191, 133, 266, 150, 349, 182, 335, 154, 484, 83, 119, 141, 242, 209, 153, 252, 103, 2494, 173, 115, 1002, 137, 477, 227, 128, 202, 806, 211, 138, 246, 316, 540, 329, 155, 410, 135, 731, 158, 215, 174, 627, 383, 147, 149, 133, 95, 361, 136, 67, 146, 78, 124, 272, 201, 178, 220, 675, 173, 347, 82, 1403, 518, 117, 116, 325, 207, 175, 161, 171, 408, 71, 166, 403, 605, 137, 460, 125, 233, 123, 127, 156, 307, 226, 636, 133, 206, 563, 126, 1236, 142, 169, 243, 140, 67, 191, 141, 280, 173, 112, 139, 244, 49, 398, 201, 71, 153, 141, 60, 439, 154, 516, 584, 139, 500, 90, 173, 124, 170, 217, 621, 165, 195, 133, 107, 203, 147, 123, 381, 485, 105, 59, 122, 265, 284, 366, 569, 426, 522, 759, 118, 53, 344, 197, 621, 246, 421, 355, 192, 130, 118, 249, 357, 119, 535, 184, 192, 898, 274, 149, 221, 78, 182, 319, 526, 197, 121, 323, 331, 192, 169, 128, 193, 308, 147, 256, 124, 310, 120, 131, 350, 190, 164, 158, 92, 136, 126, 481, 138, 204, 427, 886, 346, 125, 263, 82, 60, 280, 308, 147, 134, 76, 326, 78, 222, 812, 56, 237, 139, 116, 116, 152, 50, 147, 111, 130, 124, 379, 114, 248, 273, 467, 166, 125, 128, 449, 296, 288, 228, 139, 126, 129, 94, 501, 445, 289, 427, 131, 172, 315, 414, 125, 111, 104, 234, 133, 312, 297, 161, 83, 147, 602, 308, 533, 121, 70, 278, 125, 87, 120, 239, 146, 49, 106, 163, 126, 231, 657, 425, 123, 157, 149, 91, 220, 564, 170, 127, 230, 197, 839, 203, 157, 731, 288, 294, 392, 561, 148, 294, 230, 103, 268, 131, 359, 186, 365, 125, 281, 163, 290, 108, 92, 160, 126, 176, 109, 115, 222, 235, 184, 97, 214, 38, 161, 129, 201, 78, 169, 235, 123, 138, 116, 174, 179, 252, 114, 169, 121, 136, 440, 119, 241, 182, 156, 125, 245, 330, 182, 366, 162, 133, 136, 359, 404, 98, 162, 367, 312, 188, 142, 157, 157, 156, 419, 70, 121, 135, 319, 218, 112, 118, 150, 323, 334, 121, 104, 226, 183, 235, 112, 427, 407, 139, 210, 583, 296, 424, 183, 523, 183, 186, 992, 491, 161, 254, 287, 356, 142, 285, 233, 284, 132, 122, 121, 168, 218, 184, 134, 265, 212, 161, 488, 254, 70, 295, 136, 182, 394, 166, 188, 188, 151, 213, 164, 210, 68, 206, 189, 93, 165, 132, 227, 281, 118, 135, 267, 65, 172, 583, 116, 223, 23, 522, 470, 183, 50, 196, 196, 154, 209, 258, 126, 300, 121, 396, 193, 120, 373, 125, 71, 241, 153, 120, 444, 181, 183, 237, 157, 330, 706, 323, 256, 302, 132, 276, 107, 297, 91, 386, 288, 175, 122, 189, 60, 40, 962, 740, 365, 108, 245, 407, 297, 448, 227, 455, 445, 122, 126, 154, 805, 184, 249, 162, 42, 90, 168, 131, 109, 139, 145, 44, 266, 121, 269, 1010, 111, 105, 157, 420, 184, 130, 503, 148, 170, 155, 344, 169, 258, 365, 280, 100, 342, 725, 223, 283, 469, 184, 97, 158, 255, 169, 88, 115, 74, 168, 231, 66, 253, 139, 360, 60, 146, 804, 724, 469, 166, 175, 266, 163, 643, 51, 118, 258, 159, 135, 565, 31, 367, 121, 344, 383, 184, 194, 178, 188, 292, 471, 180, 294, 361, 428, 299, 101, 271, 156, 24, 88, 136, 366, 143, 133, 155, 45, 114, 156, 119, 167, 510, 170, 177, 134, 352, 518, 538, 152, 202, 208, 188, 86, 290, 95, 478, 126, 63, 320, 192, 41, 259, 178, 182, 248, 218, 216, 200, 144, 75, 188, 168, 576, 237, 133, 160, 213, 133, 178, 158, 365, 387, 553, 121, 131, 423, 61, 538, 238, 206, 133, 166, 125, 194, 191, 561, 155, 234, 123, 81, 143, 290, 746, 387, 288, 157, 140, 229, 174, 296, 531, 163, 52, 172, 206, 161, 16, 124, 189, 147, 141, 128, 109, 177, 359, 127, 76, 415, 132, 164, 866, 218, 148, 129, 160, 215, 183, 190, 454, 185, 241, 95, 249, 173, 206, 101, 157, 406, 572, 272, 230, 163, 375, 115, 194, 186, 156, 213, 54, 405, 228, 161, 88, 201, 163, 928, 263, 137, 117, 436, 224, 211, 113, 186, 198, 134, 217, 137, 137, 226, 283, 165, 207, 125, 162, 164, 44, 622, 193, 284, 140, 79, 93, 122, 255, 133, 76, 113, 252, 234, 344, 108, 130, 112, 321, 155, 714, 115, 399, 148, 195, 152, 61, 143, 142, 245, 183, 182, 103, 151, 201, 224, 200, 117, 89, 152, 186, 46, 158, 114, 124, 110, 526, 35, 262, 125, 153, 125, 164, 155, 123, 217, 231, 558, 202, 128, 302, 171, 391, 137, 137, 171, 269, 509, 120, 42, 189, 791, 623, 326, 141, 466, 366, 166, 160, 296, 138, 720, 453, 144, 155, 129, 377, 377, 412, 150, 725, 216, 124, 615, 218, 167, 176, 405, 448, 528, 60, 138, 323, 1028, 150, 151, 142, 151, 183, 108, 188, 177, 281, 274, 152, 140, 185, 111, 889, 131, 74, 87, 134, 264, 368, 353, 174, 319, 144, 113, 126, 192, 127, 154, 127, 421, 120, 135, 273, 334, 154, 155, 229, 112, 234, 116, 162, 138, 274, 520, 275, 182, 160, 554, 363, 271, 216, 205, 454, 559, 107, 434, 162, 227, 393, 173, 155, 335, 203, 261, 273, 261, 177, 334, 91, 551, 163, 457, 142, 166, 459, 150, 227, 46, 708, 207, 123, 242, 345, 225, 264, 338, 184, 286, 68, 226, 43, 48, 124, 161, 281, 426, 155, 176, 376, 115, 363, 137, 172, 212, 81, 122, 914, 139, 113, 446, 491, 170, 90, 236, 148, 234, 46, 247, 153, 782, 586, 141, 183, 167, 569, 264, 227, 364, 225, 273, 157, 122, 170, 156, 153, 1016, 538, 135, 452, 139, 322, 137, 171, 129, 94, 206, 129, 39, 989, 708, 227, 102, 164, 153, 300, 233, 221, 124, 234, 218, 369, 45, 118, 205, 359, 139, 326, 170, 268, 137, 65, 58, 309, 733, 294, 131, 622, 383, 313, 503, 48, 102, 152, 106, 304, 553, 93, 118, 128, 409, 473, 193, 74, 448, 323, 359, 271, 159, 317, 159, 107, 553, 504, 133, 237, 401, 173, 179, 85, 181, 217, 220, 200, 239, 133, 153, 82, 130, 74, 197, 585, 129, 100, 255, 286, 293, 296, 279, 278, 313, 116, 142, 671, 110, 34, 68, 96, 152, 61, 173, 167, 119, 325, 195, 250, 257, 155, 186, 140, 274, 220, 54, 130, 396, 920, 165, 315, 175, 260, 414, 105, 321, 968, 397, 65, 108, 115, 124, 141, 492, 443, 399, 110, 152, 137, 258, 183, 320, 132, 123, 66, 176, 150, 123, 107, 297, 122, 132, 140, 172, 217, 263, 309, 199, 740, 237, 122, 67, 198, 53, 379, 153, 263, 155, 146, 202, 158, 155, 165, 128, 178, 379, 477, 140, 407, 135, 103, 201, 339, 147, 161, 135, 551, 85, 99, 296, 257, 152, 140, 502, 248, 130, 373, 212, 163, 244, 152, 512, 190, 206, 234, 156, 184, 298, 237, 427, 203, 171, 152, 86, 219, 141, 152, 590, 220, 160, 430, 230, 337, 82, 107, 278, 227, 188, 143, 234, 182, 486, 239, 47, 163, 73, 820, 477, 228, 204, 230, 223, 159, 193, 116, 332, 148, 141, 347, 139, 208, 62, 147, 271, 86, 272, 128, 98, 234, 174, 179, 501, 160, 1000, 262, 466, 136, 340, 171, 125, 168, 303, 225, 205, 113, 193, 125, 120, 239, 123, 77, 130, 240, 418, 146, 123, 170, 226, 418, 117, 291, 94, 581, 142, 206, 189, 225, 125, 141, 159, 196, 184, 174, 137, 140, 160, 172, 252, 457, 76, 137, 189, 159, 408, 108, 199, 170, 396, 209, 74, 315, 70, 71, 187, 429, 195, 299, 253, 463, 525, 729, 165, 165, 403, 784, 257, 87, 149, 177, 324, 137, 439, 243, 90, 158, 307, 133, 442, 350, 143, 441, 164, 56, 362, 153, 246, 170, 225, 116, 249, 274, 200, 254, 92, 452, 403, 65, 188, 177, 104, 156, 90, 133, 67, 340, 183, 884, 274, 133, 179, 139, 503, 122, 137, 95, 81, 167, 371, 827, 123, 382, 56, 182, 386, 240, 106, 117, 174, 240, 337, 814, 123, 318, 555, 360, 141, 244, 257, 76, 95, 140, 142, 33, 265, 75, 173, 327, 130, 104, 274, 30, 123, 116, 365, 92, 118, 200, 82, 124, 632, 119, 128, 400, 140, 152, 383, 458, 126, 307, 188, 273, 176, 396, 310, 361, 193, 214, 469, 114, 159, 171, 174, 244, 124, 120, 131, 207, 246, 127, 167, 127, 386, 45, 138, 117, 36, 153, 50, 61, 747, 366, 146, 140, 218, 163, 166, 123, 151, 325, 132, 935, 307, 132, 170, 144, 124, 216, 194, 72, 463, 594, 263, 210, 182, 118, 260, 271, 172, 129, 163, 213, 149, 151, 320, 93, 295, 98, 935, 64, 153, 118, 256, 246, 209, 597, 249, 374, 130, 266, 114, 73, 201, 150, 137, 47, 107, 111, 158, 191, 717, 441, 142, 123, 174, 88, 214, 115, 494, 142, 537, 303, 147, 191, 86, 126, 154, 223, 138, 127, 138, 716, 274, 167, 100, 225, 126, 129, 862, 143, 53, 267, 323, 131, 162, 176, 122, 125, 220, 113, 166, 156, 133, 314, 131, 301, 135, 246, 654, 214, 132, 158, 367, 167, 255, 597, 267, 50, 972, 145, 96, 283, 270, 318, 149, 118, 138, 200, 49, 190, 334, 167, 127, 223, 80, 113, 89, 164, 491, 234, 634, 463, 167, 135, 119, 183, 102, 277, 398, 141, 161, 1409, 311, 70, 330, 119, 182, 120, 71, 85, 114, 414, 298, 122, 150, 152, 381, 331, 224, 55, 316, 268, 212, 128, 368, 130, 727, 521, 281, 278, 172, 314, 235, 271, 514, 444, 216, 318, 107, 464, 368, 83, 274, 166, 150, 67, 102, 357, 170, 228, 874, 62, 225, 215, 112, 147, 188, 186, 266, 134, 328, 143, 74, 114, 120, 119, 319, 73, 432, 358, 141, 300, 184, 244, 264, 130, 231, 173, 115, 266, 84, 344, 197, 209, 253, 136, 129, 313, 54, 89, 251, 175, 138, 332, 332, 28, 346, 119, 152, 122, 844, 229, 89, 130, 257, 125, 606, 281, 138, 231, 200, 139, 156, 305, 177, 139, 395, 139, 124, 54, 595, 492, 193, 152, 112, 97, 379, 183, 777, 961, 302, 187, 77, 170, 283, 168, 99, 68, 130, 349, 146, 169, 315, 276, 166, 251, 350, 363, 190, 180, 82, 138, 195, 140, 135, 172, 117, 149, 256, 184, 132, 186, 157, 685, 131, 448, 609, 614, 411, 141, 166, 122, 129, 163, 239, 150, 423, 132, 144, 357, 404, 268, 715, 177, 155, 272, 116, 183, 308, 131, 149, 150, 138, 309, 344, 134, 901, 114, 219, 358, 383, 421, 493, 59, 160, 293, 62, 121, 147, 147, 225, 122, 142, 138, 219, 163, 132, 234, 754, 137, 247, 166, 136, 74, 716, 253, 151, 133, 217, 157, 139, 122, 157, 109, 57, 206, 568, 128, 186, 219, 235, 126, 203, 380, 78, 256, 374, 124, 125, 890, 390, 156, 71, 249, 200, 246, 357, 145, 213, 304, 256, 327, 500, 166, 871, 616, 162, 122, 167, 291, 146, 56, 535, 86, 159, 57, 165, 314, 274, 43, 109, 151, 176, 222, 39, 174, 148, 315, 242, 104, 177, 81, 237, 62, 37, 263, 50, 456, 152, 193, 352, 101, 104, 120, 344, 142, 246, 711, 211, 183, 431, 1082, 127, 141, 143, 139, 451, 505, 120, 112, 131, 251, 419, 157, 163, 53, 220, 332, 132, 703, 139, 144, 123, 113, 395, 108, 358, 129, 192, 811, 135, 114, 120, 91, 129, 131, 226, 93, 79, 124, 296, 146, 443, 900, 75, 354, 197, 136, 184, 139, 127, 204, 344, 293, 210, 130, 146, 150, 133, 88, 568, 275, 192, 116, 177, 171, 223, 364, 106, 168, 868, 190, 85, 207, 52, 141, 45, 127, 133, 130, 126, 128, 278, 204, 246, 125, 128, 152, 158, 159, 257, 241, 131, 1016, 45, 244, 53, 178, 200, 123, 890, 462, 293, 389, 469, 136, 322, 186, 297, 294, 126, 36, 508, 384, 174, 687, 133, 140, 471, 48, 98, 289, 171, 272, 299, 143, 627, 178, 275, 487, 125, 790, 262, 310, 401, 54, 138, 122, 52, 83, 219, 173, 187, 563, 246, 659, 151, 158, 500, 154, 275, 302, 124, 265, 203, 126, 146, 132, 116, 150, 121, 294, 110, 117, 45, 432, 221, 330, 123, 248, 258, 73, 85, 97, 115, 213, 175, 241, 116, 915, 450, 252, 118, 67, 198, 53, 525, 151, 123, 263, 179, 427, 151, 114, 122, 172, 182, 135, 220, 487, 432, 99, 975, 157, 63, 371, 212, 156, 54, 723, 178, 222, 38, 225, 134, 232, 261, 199, 193, 238, 349, 107, 204, 47, 156, 536, 402, 552, 175, 135, 771, 138, 255, 472, 556, 94, 210, 145, 63, 593, 60, 124, 121, 810, 278, 172, 112, 413, 134, 403, 197, 201, 145, 114, 334, 134, 115, 166, 350, 62, 1033, 182, 380, 69, 399, 240, 554, 162, 503, 167, 126, 168, 78, 258, 598, 659, 125, 310, 122, 144, 161, 126, 196, 208, 193, 129, 271, 219, 265, 95, 211, 128, 261, 143, 803, 217, 682, 205, 66, 185, 292, 153, 174, 67, 133, 554, 200, 319, 267, 454, 392, 233, 142, 136, 87, 92, 122, 106, 166, 364, 639, 282, 239, 180, 254, 527, 72, 77, 380, 45, 30, 133, 128, 137, 220, 267, 433, 136, 338, 483, 61, 146, 412, 429, 128, 364, 172, 669, 158, 218, 187, 65, 608, 125, 172, 166, 167, 227, 177, 356, 980, 231, 430, 353, 587, 511, 320, 161, 253, 120, 41, 133, 260, 285, 211, 130, 306, 180, 665, 116, 126, 555, 138, 65, 653, 149, 504, 173, 125, 204, 194, 222, 295, 159, 168, 930, 136, 244, 469, 123, 151, 101, 159, 114, 334, 133, 406, 750, 797, 72, 232, 278, 178, 228, 179, 360, 241, 554, 281, 222, 408, 1020, 402, 205, 113, 494, 542, 99, 100, 234, 125, 542, 302, 325, 513, 372, 181, 258, 236, 78, 111, 189, 133, 131, 347, 177, 168, 179, 214, 394, 254, 58, 153, 122, 109, 136, 180, 120, 178, 124, 510, 986, 123, 157, 553, 300, 391, 152, 208, 529, 225, 144, 130, 116, 93, 479, 661, 165, 153, 715, 369, 245, 232, 459, 149, 238, 328, 156, 457, 150, 180, 209, 158, 407, 158, 147, 238, 153, 240, 117, 340, 128, 53, 284, 195, 240, 128, 140, 113, 288, 135, 107, 244, 241, 189, 130, 267, 118, 148, 382, 117, 93, 132, 74, 145, 460, 145, 128, 267, 147, 158, 186, 206, 289, 236, 142, 162, 144, 319, 305, 186, 222, 185, 245, 54, 429, 492, 192, 417, 251, 178, 325, 391, 188, 122, 237, 475, 251, 290, 243, 143, 65, 202, 176, 161, 212, 258, 145, 181, 453, 432, 491, 120, 134, 136, 167, 114, 423, 173, 371, 293, 353, 588, 171, 109, 143, 315, 113, 149, 214, 208, 282, 475, 73, 121, 458, 195, 173, 186, 216, 169, 150, 380, 130, 88, 488, 691, 139, 222, 173, 80, 142, 123, 189, 197, 169, 162, 391, 131, 305, 66, 100, 194, 186, 240, 589, 167, 147, 112, 222, 153, 409, 328, 255, 73, 133, 228, 175, 43, 188, 946, 141, 149, 199, 81, 340, 85, 124, 129, 104, 189, 277, 148, 156, 231, 38, 143, 56, 122, 121, 120, 187, 167, 101, 120, 100, 200, 739, 142, 64, 309, 142, 201, 84, 701, 130, 251, 519, 474, 128, 460, 161, 553, 121, 154, 137, 182, 221, 374, 44, 124, 50, 155, 613, 111, 136, 572, 85, 307, 102, 195, 462, 176, 81, 668, 252, 46, 128, 169, 107, 159, 46, 101, 256, 330, 86, 615, 358, 126, 83, 621, 210, 108, 649, 76, 148, 136, 438, 110, 182, 151, 144, 91, 152, 139, 47, 228, 61, 117, 74, 212, 513, 110, 170, 191, 344, 172, 605, 339, 99, 286, 119, 163, 349, 133, 303, 150, 122, 598, 279, 312, 143, 590, 86, 196, 129, 139, 336, 231, 146, 668, 148, 133, 158, 303, 132, 186, 138, 120, 190, 149, 126, 83, 229, 93, 256, 130, 90, 341, 134, 244, 235, 307, 175, 142, 110, 465, 103, 159, 369, 135, 140, 138, 132, 122, 267, 206, 123, 139, 449, 807, 172, 481, 122, 141, 157, 377, 81, 319, 117, 139, 95, 133, 67, 668, 496, 165, 157, 371, 745, 122, 80, 161, 99, 138, 69, 115, 122, 178, 211, 75, 341, 140, 163, 292, 250, 389, 126, 138, 368, 894, 190, 135, 30, 120, 81, 535, 312, 134, 108, 157, 116, 261, 299, 768, 315, 855, 529, 535, 236, 132, 148, 35, 183, 122, 427, 185, 797, 214, 1024, 134, 479, 422, 68, 471, 174, 367, 429, 116, 76, 121, 122, 241, 172, 245, 552, 371, 131, 305, 643, 175, 191, 110, 116, 502, 993, 137, 143, 529, 297, 270, 157, 177, 122, 136, 194, 791, 62, 197, 188, 79, 137, 78, 862, 330, 158, 135, 58, 117, 136, 280, 128, 330, 193, 179, 1025, 372, 287, 77, 106, 130, 272, 510, 54, 397, 186, 158, 254, 181, 124, 245, 47, 136, 556, 292, 131, 265, 241, 460, 75, 901, 77, 47, 282, 704, 109, 299, 240, 349, 174, 82, 218, 198, 169, 97, 128, 315, 131, 150, 203, 154, 135, 360, 444, 175, 115, 268, 122, 349, 247, 182, 157, 194, 395, 383, 190, 195, 114, 113, 621, 180, 51, 124, 44, 120, 154, 123, 426, 180, 122, 367, 49, 180, 178, 147, 162, 144, 51, 153, 295, 129, 301, 115, 84, 96, 190, 161, 223, 262, 303, 156, 125, 720, 578, 212, 471, 176, 358, 132, 119, 181, 185, 76, 71, 321, 212, 276, 120, 170, 57, 152, 137, 138, 154, 182, 130, 345, 345, 141, 383, 136, 158, 205, 116, 431, 57, 136, 167, 121, 136, 457, 186, 116, 233, 256, 47, 89, 228, 142, 138, 201, 147, 118, 136, 167, 95, 82, 416, 169, 105, 519, 172, 842, 568, 142, 47, 112, 238, 237, 124, 131, 459, 160, 249, 405, 69, 128, 125, 366, 124, 122, 187, 989, 165, 999, 164, 308, 129, 237, 92, 228, 298, 119, 958, 375, 74, 86, 840, 154, 112, 173, 240, 185, 262, 79, 252, 128, 264, 153, 86, 206, 290, 182, 280, 180, 114, 301, 139, 131, 212, 644, 1014, 149, 393, 805, 207, 220, 114, 320, 366, 229, 247, 70, 164, 104, 306, 142, 80, 693, 49, 159, 110, 317, 169, 148, 203, 141, 115, 345, 114, 157, 365, 43, 315, 178, 311, 148, 77, 93, 44, 368, 140, 377, 186, 186, 305, 131, 126, 228, 199, 284, 703, 209, 189, 90, 343, 119, 136, 247, 149, 353, 200, 109, 242, 79, 156, 283, 166, 335, 525, 298, 99, 660, 206, 126, 225, 114, 281, 103, 66, 198, 219, 184, 204, 48, 120, 214, 61, 201, 193, 111, 142, 370, 172, 187, 132, 144, 149, 176, 76, 285, 326, 117, 95, 174, 213, 156, 178, 515, 512, 154, 286, 110, 235, 152, 112, 350, 127, 113, 263, 117, 138, 60, 509, 178, 160, 126, 277, 330, 113, 371, 299, 137, 157, 62, 107, 132, 428, 416, 414, 169, 302, 118, 160, 175, 209, 179, 594, 165, 265, 120, 211, 234, 756, 45, 123, 336, 314, 313, 180, 83, 45, 186, 218, 230, 101, 255, 105, 341, 154, 178, 674, 166, 143, 204, 80, 186, 101, 130, 127, 297, 110, 127, 481, 120, 487, 107, 179, 225, 114, 176, 115, 173, 187, 131, 334, 196, 162, 153, 445, 143, 132, 60, 59, 262, 209, 253, 85, 373, 99, 119, 134, 146, 273, 149, 589, 51, 45, 351, 170, 418, 211, 288, 113, 210, 73, 184, 153, 225, 141, 170, 43, 369, 388, 142, 109, 133, 180, 156, 326, 222, 313, 245, 419, 124, 156, 124, 142, 257, 361, 178, 39, 334, 227, 138, 233, 366, 471, 486, 250, 126, 266, 158, 388, 126, 198, 223, 514, 209, 217, 214, 152, 155, 165, 418, 156, 198, 634, 203, 157, 171, 149, 207, 331, 250, 336, 152, 65, 180, 256, 183, 155, 143, 212, 139, 470, 211, 126, 141, 124, 150, 174, 436, 142, 449, 518, 612, 176, 63, 569, 146, 444, 746, 148, 116, 297, 527, 350, 504, 113, 264, 532, 136, 240, 190, 121, 142, 92, 58, 119, 131, 314, 179, 209, 481, 166, 336, 588, 139, 131, 135, 86, 252, 639, 221, 144, 169, 147, 124, 143, 274, 141, 263, 291, 187, 159, 804, 154, 81, 107, 129, 429, 154, 770, 145, 180, 636, 227, 244, 531, 118, 117, 96, 847, 138, 156, 540, 125, 353, 165, 132, 402, 189, 131, 38, 130, 155, 119, 252, 132, 239, 201, 232, 191, 162, 111, 84, 104, 150, 558, 153, 233, 53, 366, 348, 196, 441, 232, 228, 58, 106, 152, 55, 256, 464, 292, 77, 58, 401, 173, 227, 224, 275, 130, 233, 264, 160, 319, 117, 130, 99, 690, 118, 89, 436, 147, 122, 124, 80, 318, 139, 163, 533, 176, 271, 122, 121, 295, 152, 399, 259, 254, 230, 261, 130, 127, 479, 189, 390, 485, 403, 143, 193, 286, 160, 181, 406, 385, 192, 386, 229, 169, 82, 199, 433, 476, 135, 133, 187, 347, 61, 175, 247, 212, 127, 163, 103, 633, 121, 202, 62, 42, 177, 363, 109, 51, 121, 155, 170, 220, 304, 141, 185, 125, 134, 114, 73, 484, 207, 305, 116, 113, 142, 242, 257, 440, 315, 593, 269, 116, 98, 412, 446, 220, 131, 56, 147, 184, 468, 176, 54, 405, 71, 153, 828, 80, 194, 126, 155, 370, 337, 183, 138, 192, 276, 497, 117, 149, 142, 135, 148, 140, 217, 521, 139, 239, 137, 119, 287, 48, 113, 136, 921, 306, 138, 98, 175, 187, 143, 215, 86, 314, 263, 110, 512, 226, 230, 414, 72, 174, 125, 398, 225, 244, 169, 780, 159, 282, 183, 210, 275, 125, 255, 879, 121, 731, 697, 159, 132, 217, 52, 401, 192, 144, 56, 732, 58, 569, 93, 297, 147, 154, 92, 132, 65, 176, 296, 161, 146, 241, 173, 58, 224, 59, 120, 501, 129, 255, 529, 332, 142, 420, 172, 166, 108, 423, 124, 122, 204, 597, 143, 135, 182, 265, 186, 196, 102, 189, 415, 221, 177, 250, 126, 176, 462, 448, 52, 165, 211, 394, 247, 153, 189, 152, 486, 247, 141, 323, 167, 136, 125, 120, 140, 126, 52, 111, 746, 152, 300, 87, 215, 103, 212, 93, 136, 389, 91, 168, 132, 145, 411, 1006, 204, 545, 51, 689, 356, 52, 53, 235, 186, 110, 62, 214, 83, 188, 308, 123, 1006, 205, 447, 129, 216, 110, 109, 171, 83, 654, 711, 187, 406, 247, 481, 96, 41, 152, 93, 103, 137, 143, 132, 296, 671, 100, 429, 140, 149, 135, 282, 61, 280, 102, 551, 210, 27, 135, 199, 202, 133, 238, 239, 72, 173, 56, 139, 164, 196, 155, 166, 59, 212, 240, 116, 550, 167, 255, 236, 143, 124, 147, 168, 569, 117, 382, 127, 533, 140, 300, 144, 130, 480, 421, 321, 121, 212, 72, 145, 477, 286, 127, 222, 255, 452, 412, 210, 159, 147, 324, 260, 156, 229, 126, 51, 240, 162, 738, 189, 61, 102, 97, 232, 252, 279, 97, 331, 158, 189, 302, 137, 177, 125, 585, 152, 204, 159, 222, 306, 127, 200, 214, 102, 320, 501, 217, 925, 28, 153, 260, 270, 77, 175, 135, 443, 221, 147, 174, 118, 185, 188, 161, 687, 109, 556, 158, 88, 509, 115, 303, 205, 137, 395, 139, 492, 112, 279, 182, 191, 304, 149, 139, 297, 246, 202, 102, 118, 140, 58, 294, 60, 57, 104, 113, 196, 241, 122, 160, 193, 264, 332, 386, 222, 825, 144, 160, 657, 438, 145, 54, 201, 52, 886, 831, 165, 130, 209, 67, 122, 357, 612, 138, 227, 133, 153, 219, 177, 117, 55, 321, 223, 159, 453, 416, 250, 600, 457, 118, 147, 225, 213, 191, 118, 169, 356, 165, 239, 67, 444, 24, 792, 139, 223, 139, 171, 428, 310, 209, 321, 359, 127, 119, 176, 90, 60, 103, 157, 255, 581, 362, 493, 360, 184, 311, 129, 71, 429, 101, 110, 169, 145, 126, 729, 134, 93, 636, 86, 110, 478, 116, 159, 463, 131, 258, 148, 206, 421, 971, 701, 149, 398, 131, 120, 308, 323, 333, 177, 143, 445, 200, 683, 333, 495, 123, 180, 139, 238, 618, 69, 96, 99, 47, 160, 130, 743, 148, 151, 84, 237, 140, 144, 252, 99, 271, 540, 70, 131, 167, 35, 146, 191, 131, 57, 222, 225, 310, 141, 141, 147, 402, 100, 194, 172, 70, 151, 116, 505, 144, 108, 132, 240, 160, 219, 88, 356, 86, 489, 266, 145, 350, 438, 131, 122, 200, 210, 172, 126, 201, 145, 298, 77, 242, 421, 44, 257, 141, 166, 185, 490, 180, 130, 930, 166, 224, 148, 120, 264, 120, 115, 411, 114, 143, 121, 671, 450, 218, 480, 42, 131, 125, 138, 280, 152, 66, 119, 122, 184, 412, 523, 316, 1042, 86, 213, 286, 307, 111, 39, 388, 182, 74, 120, 73, 384, 191, 149, 190, 159, 124, 186, 98, 247, 165, 90, 380, 332, 907, 64, 349, 170, 296, 220, 156, 153, 72, 295, 900, 157, 514, 290, 165, 41, 169, 348, 354, 338, 223, 219, 267, 119, 83, 272, 144, 218, 173, 238, 212, 333, 46, 127, 123, 107, 133, 269, 138, 82, 501, 184, 220, 856, 254, 132, 184, 310, 147, 128, 336, 241, 223, 909, 832, 114, 182, 178, 111, 166, 310, 132, 266, 387, 56, 595, 186, 292, 189, 192, 144, 179, 113, 41, 48, 247, 403, 475, 114, 105, 148, 161, 982, 225, 106, 95, 112, 90, 358, 200, 307, 71, 134, 223, 276, 156, 819, 271, 589, 418, 939, 132, 413, 85, 204, 153, 365, 827, 128, 332, 212, 84, 349, 315, 192, 131, 577, 461, 177, 532, 366, 72, 135, 156, 119, 155, 142, 126, 181, 499, 144, 513, 325, 103, 280, 189, 281, 183, 465, 435, 56, 125, 375, 513, 374, 309, 151, 138, 202, 167, 134, 263, 152, 88, 79, 483, 166, 132, 121, 162, 140, 149, 171, 109, 145, 209, 212, 183, 39, 113, 53, 635, 154, 153, 57, 90, 198, 162, 284, 125, 190, 138, 208, 677, 174, 248, 237, 809, 150, 283, 165, 109, 150, 149, 341, 75, 570, 280, 374, 125, 168, 224, 284, 115, 125, 281, 146, 68, 188, 354, 416, 126, 120, 149, 227, 258, 707, 229, 211, 213, 135, 87, 67, 215, 598, 464, 228, 391, 115, 121, 121, 111, 291, 663, 124, 151, 137, 344, 130, 222, 157, 173, 77, 460, 230, 183, 129, 151, 259, 113, 94, 87, 324, 421, 133, 169, 217, 227, 368, 54, 139, 159, 649, 288, 759, 697, 179, 189, 134, 120, 250, 44, 127, 121, 155, 57, 166, 135, 196, 138, 139, 50, 129, 306, 165, 61, 169, 143, 454, 181, 119, 171, 150, 505, 1029, 325, 303, 223, 223, 441, 273, 161, 143, 74, 128, 198, 145, 125, 367, 134, 190, 49, 161, 163, 383, 171, 125, 680, 171, 71, 135, 827, 635, 177, 111, 91, 186, 491, 132, 401, 363, 220, 139, 115, 193, 510, 148, 245, 120, 124, 728, 79, 913, 154, 379, 182, 239, 142, 243, 84, 111, 447, 203, 190, 378, 150, 192, 98, 529, 175, 192, 148, 144, 138, 176, 148, 162, 149, 253, 242, 195, 620, 147, 144, 168, 221, 216, 106, 347, 120, 142, 461, 150, 225, 119, 127, 142, 243, 186, 110, 436, 117, 1084, 128, 180, 372, 492, 204, 328, 685, 229, 65, 226, 252, 227, 125, 221, 146, 138, 366, 145, 107, 180, 107, 134, 64, 456, 367, 93, 131, 220, 139, 247, 130, 367, 54, 650, 110, 157, 127, 159, 124, 392, 339, 396, 132, 248, 307, 78, 164, 202, 46, 771, 244, 273, 156, 187, 179, 350, 47, 412, 125, 234, 184, 245, 368, 213, 204, 367, 287, 533, 466, 367, 306, 374, 486, 122, 107, 254, 380, 215, 201, 190, 358, 351, 127, 104, 125, 171, 699, 71, 90, 125, 377, 73, 209, 174, 336, 594, 254, 149, 71, 143, 236, 766, 237, 142, 140, 376, 109, 175, 60, 178, 249, 362, 137, 190, 194, 183, 225, 802, 123, 113, 247, 43, 219, 362, 294, 262, 52, 177, 768, 183, 118, 153, 107, 55, 168, 156, 124, 259, 349, 211, 267, 197, 113, 178, 130, 192, 131, 913, 205, 54, 329, 125, 182, 233, 202, 148, 211, 323, 135, 331, 256, 400, 286, 507, 217, 232, 65, 146, 195, 100, 116, 168, 132, 125, 146, 132, 287, 333, 134, 111, 93, 55, 121, 178, 121, 237, 108, 227, 260, 484, 249, 165, 359, 205, 237, 590, 145, 136, 138, 142, 172, 273, 126, 44, 242, 187, 299, 551, 126, 141, 187, 406, 111, 584, 999, 124, 147, 114, 117, 170, 135, 149, 168, 480, 261, 355, 50, 405, 216, 294, 259, 218, 371, 216, 52, 91, 255, 519, 158, 114, 343, 132, 188, 112, 506, 150, 335, 149, 474, 132, 84, 115, 181, 62, 440, 184, 145, 281, 287, 214, 975, 297, 738, 288, 180, 163, 198, 249, 170, 169, 202, 584, 823, 133, 168, 469, 660, 322, 938, 486, 180, 262, 126, 124, 121, 84, 99, 224, 151, 375, 300, 179, 228, 46, 200, 523, 105, 239, 410, 200, 268, 312, 418, 975, 129, 185, 149, 117, 45, 128, 732, 179, 158, 309, 164, 377, 135, 94, 137, 546, 635, 125, 273, 165, 114, 122, 168, 40, 373, 198, 434, 132, 119, 199, 42, 209, 150, 222, 195, 80, 314, 252, 130, 188, 53, 225, 134, 212, 137, 139, 182, 609, 759, 144, 361, 117, 310, 105, 188, 225, 133, 435, 206, 586, 62, 115, 459, 176, 243, 419, 137, 145, 180, 421, 142, 148, 241, 135, 505, 348, 581, 101, 520, 178, 198, 265, 208, 112, 170, 66, 322, 66, 206, 604, 229, 177, 132, 287, 63, 221, 209, 155, 183, 283, 147, 69, 331, 141, 198, 457, 124, 221, 1011, 122, 434, 248, 182, 162, 217, 218, 128, 244, 329, 187, 235, 263, 152, 345, 174, 58, 378, 213, 115, 162, 719, 122, 61, 194, 362, 142, 176, 430, 129, 90, 132, 165, 186, 219, 123, 159, 264, 406, 180, 540, 185, 299, 134, 126, 71, 537, 191, 195, 148, 152, 90, 117, 112, 212, 135, 197, 297, 153, 165, 137, 115, 127, 217, 795, 200, 156, 194, 110, 201, 91, 137, 292, 331, 230, 418, 159, 231, 169, 239, 120, 1057, 191, 182, 178, 143, 765, 244, 108, 840, 210, 207, 83, 181, 51, 245, 231, 155, 549, 255, 160, 295, 280, 384, 357, 169, 50, 150, 367, 134, 71, 84, 128, 138, 68, 328, 427, 42, 41, 271, 142, 132, 138, 200, 159, 114, 88, 123, 128, 174, 137, 135, 179, 222, 350, 532, 192, 223, 222, 69, 147, 908, 376, 671, 409, 701, 152, 165, 235, 61, 401, 147, 175, 295, 199, 188, 339, 151, 131, 515, 137, 44, 157, 198, 142, 790, 188, 175, 83, 180, 338, 187, 166, 146, 211, 286, 317, 33, 220, 180, 72, 80, 119, 122, 493, 95, 289, 540, 177, 778, 227, 146, 185, 414, 345, 494, 141, 53, 59, 180, 161, 273, 239, 64, 73, 146, 288, 442, 251, 185, 128, 187, 134, 311, 50, 454, 154, 76, 442, 86, 123, 209, 205, 167, 74, 295, 315, 325, 187, 169, 287, 623, 423, 368, 307, 225, 147, 163, 142, 570, 161, 201, 273, 287, 171, 256, 262, 418, 262, 190, 149, 396, 179, 79, 183, 119, 341, 579, 463, 171, 748, 905, 174, 311, 331, 213, 134, 78, 154, 115, 120, 56, 136, 131, 446, 274, 308, 224, 128, 156, 612, 349, 166, 192, 494, 163, 280, 189, 55, 300, 188, 452, 944, 412, 110, 41, 334, 165, 166, 149, 215, 55, 192, 85, 80, 46, 183, 129, 104, 617, 600, 175, 116, 124, 368, 413, 126, 349, 179, 150, 48, 122, 199, 206, 150, 209, 252, 316, 344, 151, 220, 63, 470, 334, 564, 226, 105, 189, 123, 136, 151, 467, 76, 138, 165, 921, 122, 254, 313, 241, 421, 197, 313, 47, 416, 145, 257, 391, 151, 419, 203, 135, 162, 639, 330, 508, 152, 370, 472, 256, 147, 663, 110, 61, 174, 290, 222, 140, 120, 102, 151, 224, 193, 373, 117, 184, 219, 318, 173, 200, 110, 176, 176, 169, 403, 201, 294, 681, 108, 161, 321, 168, 444, 259, 444, 117, 201, 153, 50, 213, 176, 69, 179, 166, 187, 377, 69, 135, 133, 121, 149, 161, 182, 367, 327, 58, 104, 493, 145, 205, 117, 368, 233, 531, 120, 437, 119, 141, 112, 1006, 123, 291, 186, 245, 136, 117, 171, 128, 1007, 279, 76, 168, 164, 108, 407, 226, 462, 285, 156, 257, 276, 73, 164, 99, 370, 646, 254, 487, 170, 159, 123, 135, 458, 160, 170, 277, 413, 70, 129, 115, 501, 107, 208, 184, 214, 137, 292, 195, 239, 344, 125, 214, 190, 140, 121, 285, 97, 199, 159, 207, 270, 242, 63, 65, 134, 69, 116, 784, 122, 160, 231, 639, 411, 872, 245, 237, 545, 127, 148, 245, 200, 124, 120, 220, 136, 88, 220, 102, 71, 134, 368, 104, 167, 347, 140, 83, 134, 152, 383, 644, 134, 207, 196, 643, 139, 471, 74, 1063, 271, 228, 124, 84, 185, 807, 123, 164, 77, 279, 123, 143, 148, 843, 49, 224, 412, 284, 59, 110, 166, 308, 76, 119, 366, 128, 132, 192, 139, 127, 85, 465, 151, 122, 222, 115, 311, 263, 124, 132, 345, 295, 449, 124, 163, 122, 140, 179, 67, 230, 364, 128, 668, 303, 168, 26, 319, 429, 385, 138, 465, 229, 147, 116, 92, 583, 58, 761, 340, 160, 139, 158, 117, 290, 176, 213, 144, 368, 193, 139, 380, 516, 237, 129, 114, 128, 243, 152, 257, 347, 328, 170, 191, 384, 265, 65, 284, 270, 38, 352, 73, 141, 121, 113, 376, 74, 113, 197, 82, 133, 324, 178, 407, 43, 362, 705, 146, 169, 57, 127, 103, 156, 132, 188, 173, 171, 218, 151, 235, 572, 114, 167, 636, 148, 210, 112, 1008, 78, 216, 329, 116, 68, 171, 146, 149, 125, 174, 197, 120, 252, 999, 223, 169, 663, 153, 432, 138, 107, 60, 223, 120, 122, 237, 141, 61, 240, 306, 520, 365, 167, 154, 74, 57, 135, 823, 168, 306, 247, 295, 145, 119, 155, 251, 191, 139, 113, 784, 131, 145, 187, 327, 313, 141, 170, 479, 21, 86, 250, 77, 353, 195, 173, 140, 128, 96, 120, 442, 125, 293, 166, 63, 440, 153, 177, 210, 378, 408, 61, 169, 217, 119, 376, 234, 286, 253, 271, 178, 100, 217, 374, 125, 208, 814, 536, 204, 423, 168, 128, 79, 165, 143, 318, 117, 118, 1003, 123, 64, 95, 270, 136, 133, 260, 100, 313, 525, 270, 184, 159, 154, 502, 140, 500, 312, 833, 167, 376, 121, 135, 139, 216, 39, 208, 143, 131, 272, 72, 154, 159, 176, 54, 152, 130, 132, 150, 765, 283, 209, 253, 380, 187, 119, 692, 117, 164, 101, 745, 111, 266, 188, 110, 43, 296, 136, 316, 114, 566, 156, 117, 147, 1007, 130, 84, 140, 157, 199, 173, 290, 148, 305, 104, 117, 171, 155, 206, 703, 370, 155, 186, 360, 126, 383, 99, 314, 773, 118, 294, 153, 80, 169, 246, 186, 209, 116, 578, 119, 90, 220, 136, 201, 140, 814, 318, 355, 427, 138, 168, 156, 146, 556, 123, 295, 154, 127, 452, 123, 109, 148, 360, 647, 137, 189, 132, 114, 598, 122, 172, 191, 295, 133, 405, 65, 75, 419, 730, 135, 48, 268, 158, 362, 438, 64, 118, 159, 48, 138, 157, 861, 95, 125, 231, 538, 131, 139, 311, 523, 101, 202, 445, 975, 56, 129, 125, 84, 362, 269, 410, 164, 197, 157, 215, 207, 158, 140, 181, 178, 136, 271, 297, 240, 138, 170, 191, 255, 144, 109, 597, 253, 462, 173, 190, 60, 140, 95, 194, 439, 245, 263, 354, 248, 267, 269, 552, 143, 300, 192, 449, 445, 271, 132, 230, 159, 139, 92, 211, 175, 657, 131, 314, 489, 153, 296, 56, 153, 361, 112, 219, 385, 257, 152, 235, 154, 534, 184, 211, 118, 185, 234, 397, 162, 919, 448, 70, 196, 148, 138, 332, 134, 338, 351, 84, 116, 138, 266, 726, 95, 224, 141, 596, 62, 478, 53, 114, 903, 136, 159, 448, 135, 125, 143, 559, 146, 307, 154, 257, 250, 116, 951, 149, 153, 515, 184, 873, 155, 491, 183, 162, 376, 170, 182, 422, 363, 115, 237, 232, 390, 158, 182, 136, 156, 265, 147, 109, 75, 977, 321, 1012, 140, 167, 168, 205, 326, 530, 308, 148, 755, 148, 315, 190, 204, 401, 132, 141, 617, 390, 261, 212, 136, 165, 159, 113, 149, 261, 495, 108, 55, 72, 171, 352, 123, 477, 630, 211, 418, 538, 214, 135, 152, 218, 41, 165, 322, 145, 464, 467, 105, 134, 402, 335, 217, 122, 176, 123, 391, 100, 117, 945, 51, 176, 244, 128, 227, 70, 200, 132, 339, 193, 187, 159, 131, 130, 163, 133, 398, 202, 362, 126, 183, 636, 519, 207, 105, 334, 279, 161, 406, 227, 156, 984, 410, 975, 176, 239, 201, 783, 140, 123, 141, 165, 117, 207, 96, 284, 540, 122, 135, 369, 254, 824, 214, 135, 113, 460, 103, 112, 157, 66, 248, 164, 605, 323, 149, 428, 50, 146, 474, 168, 126, 233, 166, 187, 245, 445, 127, 557, 156, 171, 468, 132, 167, 118, 150, 59, 256, 664, 175, 270, 140, 183, 191, 183, 163, 65, 163, 145, 118, 456, 98, 643, 175, 83, 146, 78, 95, 384, 292, 507, 445, 127, 270, 336, 54, 144, 243, 334, 36, 294, 395, 521, 157, 136, 116, 446, 131, 197, 161, 355, 52, 167, 378, 89, 141, 124, 202, 278, 223, 141, 175, 97, 440, 168, 196, 209, 113, 144, 129, 87, 125, 147, 445, 139, 188, 196, 255, 173, 127, 132, 169, 118, 357, 139, 309, 311, 303, 147, 622, 211, 132, 160, 430, 155, 597, 622, 322, 136, 108, 139, 112, 107, 135, 43, 158, 232, 501, 242, 250, 136, 367, 140, 126, 346, 447, 175, 148, 133, 420, 137, 86, 473, 112, 81, 120, 407, 190, 130, 134, 155, 976, 118, 213, 129, 487, 243, 302, 92, 453, 48, 195, 231, 242, 275, 225, 692, 154, 130, 161, 197, 446, 1006, 125, 110, 366, 187, 152, 471, 525, 75, 117, 373, 234, 138, 246, 220, 194, 136, 38, 71, 119, 114, 141, 130, 182, 171, 247, 166, 261, 108, 55, 259, 106, 336, 193, 70, 694, 145, 449, 173, 431, 147, 198, 130, 119, 36, 477, 169, 409, 540, 230, 151, 247, 328, 114, 324, 155, 263, 343, 156, 117, 501, 86, 257, 242, 175, 251, 204, 212, 178, 132, 244, 719, 425, 223, 195, 129, 134, 305, 213, 244, 773, 51, 184, 174, 898, 277, 800, 615, 553, 38, 79, 139, 206, 125, 203, 245, 124, 276, 126, 239, 72, 103, 57, 156, 572, 191, 188, 184, 226, 65, 292, 252, 119, 206, 147, 151, 211, 635, 580, 262, 583, 153, 276, 1571, 207, 188, 142, 209, 325, 123, 217, 206, 162, 382, 147, 201, 132, 30, 157, 123, 230, 348, 233, 175, 403, 161, 101, 145, 150, 114, 79, 153, 124, 390, 132, 484, 592, 348, 281, 163, 134, 171, 152, 111, 466, 136, 273, 399, 306, 373, 102, 243, 116, 29, 485, 194, 515, 139, 335, 150, 353, 639, 238, 178, 133, 79, 159, 135, 184, 99, 72, 51, 174, 549, 149, 147, 579, 168, 515, 201, 273, 175, 198, 313, 163, 165, 182, 170, 384, 149, 370, 138, 232, 134, 101, 153, 327, 137, 109, 241, 121, 117, 100, 366, 170, 162, 162, 325, 384, 236, 200, 501, 173, 461, 174, 613, 115, 714, 293, 294, 380, 144, 123, 46, 379, 481, 315, 110, 216, 324, 162, 211, 148, 111, 289, 221, 533, 183, 151, 224, 157, 146, 124, 321, 208, 117, 124, 332, 182, 259, 134, 45, 110, 110, 36, 255, 117, 464, 59, 185, 385, 44, 96, 142, 153, 160, 151, 113, 74, 184, 292, 216, 123, 150, 114, 788, 645, 255, 196, 202, 154, 315, 125, 153, 222, 144, 135, 228, 248, 322, 129, 495, 210, 366, 128, 213, 515, 309, 276, 189, 114, 799, 44, 121, 502, 173, 149, 117, 260, 137, 449, 101, 451, 398, 516, 136, 164, 249, 112, 117, 228, 218, 177, 289, 1015, 98, 179, 159, 388, 118, 124, 128, 144, 796, 166, 330, 271, 210, 219, 156, 116, 400, 118, 126, 106, 118, 79, 276, 126, 281, 155, 199, 766, 49, 596, 378, 366, 213, 136, 355, 176, 133, 1013, 180, 154, 144, 181, 132, 160, 123, 189, 67, 154, 264, 193, 263, 73, 179, 135, 330, 116, 154, 235, 297, 91, 280, 121, 546, 410, 131, 359, 171, 118, 95, 123, 163, 187, 486, 205, 91, 95, 172, 93, 34, 309, 132, 167, 150, 128, 308, 233, 180, 144, 202, 126, 636, 284, 571, 165, 171, 884, 134, 139, 409, 75, 46, 120, 154, 145, 231, 176, 85, 134, 563, 172, 470, 231, 60, 125, 128, 191, 133, 53, 269, 180, 125, 121, 385, 367, 919, 195, 351, 174, 170, 77, 249, 171, 109, 171, 69, 127, 283, 271, 322, 176, 51, 126, 392, 147, 173, 116, 63, 124, 391, 196, 154, 126, 280, 266, 298, 114, 72, 205, 198, 182, 178, 136, 130, 104, 117, 98, 215, 331, 161, 305, 184, 85, 330, 361, 976, 190, 358, 222, 136, 149, 146, 790, 136, 809, 131, 112, 211, 260, 354, 1097, 220, 135, 366, 213, 451, 172, 64, 266, 138, 131, 254, 116, 239, 124, 340, 132, 338, 302, 70, 345, 298, 176, 96, 165, 245, 300, 400, 281, 124, 464, 133, 745, 262, 25, 129, 175, 153, 149, 83, 336, 370, 403, 147, 115, 256, 243, 264, 132, 345, 297, 94, 69, 134, 124, 707, 264, 85, 93, 59, 436, 419, 226, 125, 360, 144, 743, 446, 121, 240, 179, 243, 329, 60, 149, 102, 297, 145, 139, 496, 207, 122, 116, 225, 320, 153, 949, 255, 223, 114, 164, 170, 373, 107, 71, 138, 226, 341, 125, 211, 430, 194, 387, 148, 415, 130, 118, 110, 130, 637, 63, 649, 315, 501, 551, 395, 91, 176, 52, 173, 661, 133, 208, 296, 222, 186, 188, 95, 337, 573, 175, 149, 983, 246, 516, 175, 156, 131, 338, 61, 205, 158, 286, 118, 224, 95, 269, 132, 318, 168, 443, 168, 229, 78, 55, 385, 122, 184, 180, 328, 184, 150, 153]\n"
     ]
    }
   ],
   "source": [
    "#Print k\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2494\n"
     ]
    }
   ],
   "source": [
    "#To get max value of k\n",
    "max=0\n",
    "for i,j in enumerate(k):\n",
    "    if j>max:\n",
    "        max=j\n",
    "print(max)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17934\n"
     ]
    }
   ],
   "source": [
    "#To get max sequence review index\n",
    "for i,j in enumerate(k):\n",
    "    if j==max:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choosing optimul SEQ LEN because previously our model used 20 SEQ LEN where the model performance was not good\n",
    "#Max sequence length of any sentences is 2494 which is way too much\n",
    "#Therefore choosing value between 20 and 2494\n",
    "# this 300 came from the graph above where i plotted avg length and the deviation\n",
    "SEQ_LEN=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#padding train and test set\n",
    "X_train = pad_sequences(x_train, maxlen=SEQ_LEN, padding='post', truncating=\"post\")\n",
    "X_test =  pad_sequences(x_test, maxlen=SEQ_LEN, padding='post', truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 300)\n",
      "(25000, 300)\n"
     ]
    }
   ],
   "source": [
    "#Cross check step to see shape of data if data is padded properly\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#Cross check step to see if variable type \n",
    "print(type(X_train))\n",
    "print(type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,  591,  202, ...,    0,    0,    0],\n",
       "       [   1,   14,   22, ...,    0,    0,    0],\n",
       "       [   1,  111,  748, ...,   97,   38,  111],\n",
       "       ...,\n",
       "       [   1,   13, 1408, ...,    0,    0,    0],\n",
       "       [   1,   11,  119, ...,    0,    0,    0],\n",
       "       [   1,    6,   52, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just to see values in test padded set\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1,   14,   22, ...,    0,    0,    0],\n",
       "       [   1,  194, 1153, ...,    0,    0,    0],\n",
       "       [   1,   14,   47, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   1,   11,    6, ...,    0,    0,    0],\n",
       "       [   1, 1446, 7079, ...,    0,    0,    0],\n",
       "       [   1,   17,    6, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just to see values in train padded set\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "#Provide values and it return decoded output based on decode function\n",
    "k=decode(X_train[0])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> big hair big boobs bad music and a giant safety pin these are the words to best describe this terrible movie i love cheesy horror movies and i've seen hundreds but this had got to be on of the worst ever made the plot is paper thin and ridiculous the acting is an abomination the script is completely laughable the best is the end showdown with the cop and how he worked out who the killer is it's just so damn terribly written the clothes are sickening and funny in equal <UNK> the hair is big lots of boobs <UNK> men wear those cut <UNK> shirts that show off their <UNK> sickening that men actually wore them and the music is just <UNK> trash that plays over and over again in almost every scene there is trashy music boobs and <UNK> taking away bodies and the gym still doesn't close for <UNK> all joking aside this is a truly bad film whose only charm is to look back on the disaster that was the 80's and have a good old laugh at how bad everything was back then <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "#Provide values and it return decoded output based on decode function\n",
    "k=decode(X_train[1])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this has to be one of the worst films of the 1990s when my friends i were watching this film being the target audience it was aimed at we just sat watched the first half an hour with our jaws touching the floor at how bad it really was the rest of the time everyone else in the theatre just started talking to each other leaving or generally crying into their popcorn that they actually paid money they had <UNK> working to watch this feeble excuse for a film it must have looked like a great idea on paper but on film it looks like no one in the film has a clue what is going on crap acting crap costumes i can't get across how <UNK> this is to watch save yourself an hour a bit of your life <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "#Cross check step\n",
    "#Provide values and it return decoded output based on decode function\n",
    "k=decode(X_train[2])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model building\n",
    "\n",
    "NOTE: From now on for the rest of the models we will use sequence length of 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model buliding\n",
    "model = Sequential() #Sequential model\n",
    "\n",
    "model.add(Embedding(MAX_VOCAB, EMBEDDING_DIM, input_length=SEQ_LEN))  #Init embedding layer with no pretrained wts\n",
    "#embedding layer takes input of vocabulary size i.e 10000, embedding dimension i.e 50 and input sequence i.e number of columns of dataset i.e 300\n",
    "model.add(Flatten()) #Use flatten layer\n",
    "model.add(Dropout(0.5)) #use dropout for regularization\n",
    "model.add(Dense(10)) #Hidden layer\n",
    "model.add(Dropout(0.3)) #use dropout for regularization\n",
    "model.add(Dense(1, activation='sigmoid')) #Output layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxNDNhrseCzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 50)           500000    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                150010    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 650,021\n",
      "Trainable params: 650,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#To see what our model have, number of layer , output shape ,etc\n",
    "#model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3CSVVPPeCzD"
   },
   "outputs": [],
   "source": [
    "#Compile model with optimizer adam, loss as binary cross entropy and metric is accuracy\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross check step to see shape of input to our model\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 18s 701us/step - loss: 0.5398 - accuracy: 0.7072 - val_loss: 0.3430 - val_accuracy: 0.8542\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 17s 683us/step - loss: 0.2471 - accuracy: 0.9056 - val_loss: 0.3186 - val_accuracy: 0.8642\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 17s 678us/step - loss: 0.1223 - accuracy: 0.9606 - val_loss: 0.3560 - val_accuracy: 0.8599\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 18s 702us/step - loss: 0.0606 - accuracy: 0.9822 - val_loss: 0.4003 - val_accuracy: 0.8604\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 17s 693us/step - loss: 0.0316 - accuracy: 0.9917 - val_loss: 0.4639 - val_accuracy: 0.8581\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 17s 694us/step - loss: 0.0224 - accuracy: 0.9939 - val_loss: 0.5269 - val_accuracy: 0.8537\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 17s 677us/step - loss: 0.0166 - accuracy: 0.9951 - val_loss: 0.5567 - val_accuracy: 0.8595s - loss: 0.0166 - accuracy: 0.99\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 17s 691us/step - loss: 0.0161 - accuracy: 0.9953 - val_loss: 0.5951 - val_accuracy: 0.8577\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 17s 678us/step - loss: 0.0143 - accuracy: 0.9955 - val_loss: 0.6708 - val_accuracy: 0.8512s - loss: 0.0143 - accuracy\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 17s 678us/step - loss: 0.0170 - accuracy: 0.9944 - val_loss: 0.6995 - val_accuracy: 0.8520\n"
     ]
    }
   ],
   "source": [
    "#Fitting model to xtrain and ytrain with defined epochs and batch size\n",
    "r = model.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_data=(X_test,y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 4s 150us/step\n",
      "Test accuracy:  0.8519600033760071\n"
     ]
    }
   ],
   "source": [
    "#Evaluate test set and then print accuracy\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 4s 140us/step\n",
      "Test accuracy:  0.9999200105667114\n"
     ]
    }
   ],
   "source": [
    "#Evaluate train set and then print accuracy\n",
    "results = model.evaluate(X_train, y_train)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note\n",
    "The accuracy increase when we increased sequence length still model is going toward overfitzone\n",
    "Lets use grid search to increase accuarcy further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building layes and setting hparams\n",
    "def build_classifier(optimizer,neurons,EMBEDDING_DIM):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_VOCAB, EMBEDDING_DIM, input_length=SEQ_LEN))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(neurons))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    #adam = optimizers.Adam(lr = 0.001)\n",
    "    model.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declaring hyper parameter such as neurons, batch_size, optimizers and embedding dimension to get their best values\n",
    "#Then fitting it to data set\n",
    "model = KerasClassifier(build_fn = build_classifier,epochs=8, verbose=1)\n",
    "parameters = {'batch_size': [75,125],\n",
    "              'neurons': [5,10,15,20],\n",
    "              'optimizer': ['adam', 'rmsprop'],\n",
    "              'EMBEDDING_DIM':[30,50,70]\n",
    "             }\n",
    "grid_search = GridSearchCV(estimator = model,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 158us/step - loss: 0.6201 - accuracy: 0.6474\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 3s 156us/step - loss: 0.3198 - accuracy: 0.8822\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 3s 150us/step - loss: 0.1916 - accuracy: 0.9368\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 3s 153us/step - loss: 0.1119 - accuracy: 0.9696\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 2s 150us/step - loss: 0.0707 - accuracy: 0.9815\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 3s 159us/step - loss: 0.0436 - accuracy: 0.9899\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 3s 151us/step - loss: 0.0309 - accuracy: 0.9929\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 3s 151us/step - loss: 0.0243 - accuracy: 0.9938\n",
      "8334/8334 [==============================] - 0s 23us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 154us/step - loss: 0.6382 - accuracy: 0.6201\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.3238 - accuracy: 0.8771\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.1856 - accuracy: 0.9387\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 157us/step - loss: 0.1074 - accuracy: 0.9708\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.0628 - accuracy: 0.9850\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.0427 - accuracy: 0.9913\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 2s 150us/step - loss: 0.0302 - accuracy: 0.9929\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 153us/step - loss: 0.0239 - accuracy: 0.9939\n",
      "8333/8333 [==============================] - 0s 27us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.6282 - accuracy: 0.6391\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 162us/step - loss: 0.3302 - accuracy: 0.8740\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 155us/step - loss: 0.1954 - accuracy: 0.9348\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.1101 - accuracy: 0.9687\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 163us/step - loss: 0.0693 - accuracy: 0.9820\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 167us/step - loss: 0.0447 - accuracy: 0.9893\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 156us/step - loss: 0.0305 - accuracy: 0.9932\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 161us/step - loss: 0.0234 - accuracy: 0.9951\n",
      "8333/8333 [==============================] - 0s 29us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 2s 149us/step - loss: 0.6344 - accuracy: 0.6362\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 2s 144us/step - loss: 0.3647 - accuracy: 0.8589\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 2s 145us/step - loss: 0.2499 - accuracy: 0.9080\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 2s 146us/step - loss: 0.1859 - accuracy: 0.9356\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 2s 144us/step - loss: 0.1380 - accuracy: 0.9539\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 3s 154us/step - loss: 0.0989 - accuracy: 0.9657\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 2s 145us/step - loss: 0.0711 - accuracy: 0.9771\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 2s 145us/step - loss: 0.0529 - accuracy: 0.9851\n",
      "8334/8334 [==============================] - 0s 30us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.6334 - accuracy: 0.6336\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.3717 - accuracy: 0.8552\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 2s 147us/step - loss: 0.2525 - accuracy: 0.9059\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 154us/step - loss: 0.1832 - accuracy: 0.9380\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 147us/step - loss: 0.1380 - accuracy: 0.9540\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 2s 145us/step - loss: 0.0969 - accuracy: 0.9696\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 2s 146us/step - loss: 0.0739 - accuracy: 0.9767\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 2s 147us/step - loss: 0.0538 - accuracy: 0.9828\n",
      "8333/8333 [==============================] - 0s 33us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.6418 - accuracy: 0.6261\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 160us/step - loss: 0.3691 - accuracy: 0.8586\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 156us/step - loss: 0.2493 - accuracy: 0.9074\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.1779 - accuracy: 0.9376\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 156us/step - loss: 0.1286 - accuracy: 0.9567\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 156us/step - loss: 0.0926 - accuracy: 0.9729\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.0677 - accuracy: 0.9787\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 163us/step - loss: 0.0493 - accuracy: 0.9855\n",
      "8333/8333 [==============================] - 0s 40us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 175us/step - loss: 0.6080 - accuracy: 0.6529\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 3s 170us/step - loss: 0.2943 - accuracy: 0.8850\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 3s 169us/step - loss: 0.1536 - accuracy: 0.9489\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 3s 169us/step - loss: 0.0803 - accuracy: 0.9770\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 3s 168us/step - loss: 0.0429 - accuracy: 0.9892\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 3s 163us/step - loss: 0.0275 - accuracy: 0.99390s -\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 3s 155us/step - loss: 0.0202 - accuracy: 0.9942\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 3s 156us/step - loss: 0.0191 - accuracy: 0.9947\n",
      "8334/8334 [==============================] - 0s 37us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 163us/step - loss: 0.6183 - accuracy: 0.6370\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 156us/step - loss: 0.2984 - accuracy: 0.8818\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 164us/step - loss: 0.1581 - accuracy: 0.9474\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.0824 - accuracy: 0.9750\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 156us/step - loss: 0.0468 - accuracy: 0.9872\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 157us/step - loss: 0.0298 - accuracy: 0.9926\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.0218 - accuracy: 0.9945\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 156us/step - loss: 0.0183 - accuracy: 0.9952\n",
      "8333/8333 [==============================] - 0s 39us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 168us/step - loss: 0.6103 - accuracy: 0.64580s - loss: 0.6186 - accuracy: \n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.2900 - accuracy: 0.8894\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.1536 - accuracy: 0.9485\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.0818 - accuracy: 0.9759\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 161us/step - loss: 0.0489 - accuracy: 0.9866\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 160us/step - loss: 0.0314 - accuracy: 0.99190s - loss: 0.031\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 167us/step - loss: 0.0229 - accuracy: 0.9946\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 159us/step - loss: 0.0178 - accuracy: 0.99510s - loss: 0.017\n",
      "8333/8333 [==============================] - 0s 43us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 159us/step - loss: 0.6055 - accuracy: 0.6662\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 2s 147us/step - loss: 0.3231 - accuracy: 0.8732\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 3s 152us/step - loss: 0.2163 - accuracy: 0.9170\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 3s 153us/step - loss: 0.1476 - accuracy: 0.9462\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 3s 155us/step - loss: 0.0985 - accuracy: 0.9657\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 2s 150us/step - loss: 0.0665 - accuracy: 0.9774\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 2s 150us/step - loss: 0.0500 - accuracy: 0.9834\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 2s 150us/step - loss: 0.0353 - accuracy: 0.9887\n",
      "8334/8334 [==============================] - 0s 43us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.6155 - accuracy: 0.6455\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 150us/step - loss: 0.3344 - accuracy: 0.8634\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.2207 - accuracy: 0.9173\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 149us/step - loss: 0.1539 - accuracy: 0.9456\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 149us/step - loss: 0.1017 - accuracy: 0.9666\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.0695 - accuracy: 0.9774\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 150us/step - loss: 0.0470 - accuracy: 0.9842\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 2s 149us/step - loss: 0.0386 - accuracy: 0.9875\n",
      "8333/8333 [==============================] - 0s 49us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 159us/step - loss: 0.6212 - accuracy: 0.6416\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.3390 - accuracy: 0.8653\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.2193 - accuracy: 0.9158\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.1461 - accuracy: 0.9493\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.1017 - accuracy: 0.96420s - loss:\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 154us/step - loss: 0.0681 - accuracy: 0.9767\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.0458 - accuracy: 0.98500s - loss: 0\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 150us/step - loss: 0.0329 - accuracy: 0.9893\n",
      "8333/8333 [==============================] - 0s 47us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 170us/step - loss: 0.5975 - accuracy: 0.6580\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 3s 159us/step - loss: 0.2731 - accuracy: 0.8932\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 3s 161us/step - loss: 0.1364 - accuracy: 0.9541\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 3s 167us/step - loss: 0.0672 - accuracy: 0.9800\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 3s 159us/step - loss: 0.0401 - accuracy: 0.9884\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 3s 159us/step - loss: 0.0251 - accuracy: 0.99350s - loss: 0.0\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 3s 160us/step - loss: 0.0199 - accuracy: 0.9947\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 3s 160us/step - loss: 0.0161 - accuracy: 0.9956\n",
      "8334/8334 [==============================] - 0s 51us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 174us/step - loss: 0.5918 - accuracy: 0.6665\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 161us/step - loss: 0.2745 - accuracy: 0.8931\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 165us/step - loss: 0.1435 - accuracy: 0.9528\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 164us/step - loss: 0.0746 - accuracy: 0.9767\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 165us/step - loss: 0.0377 - accuracy: 0.9903\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 164us/step - loss: 0.0281 - accuracy: 0.9921\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 170us/step - loss: 0.0193 - accuracy: 0.9952\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 169us/step - loss: 0.0176 - accuracy: 0.9944\n",
      "8333/8333 [==============================] - 0s 58us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 172us/step - loss: 0.6011 - accuracy: 0.6532\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 160us/step - loss: 0.2753 - accuracy: 0.89270s - loss: 0\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 165us/step - loss: 0.1325 - accuracy: 0.9562\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 169us/step - loss: 0.0650 - accuracy: 0.9801\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 181us/step - loss: 0.0426 - accuracy: 0.9878\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 172us/step - loss: 0.0263 - accuracy: 0.9931\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 167us/step - loss: 0.0172 - accuracy: 0.9963\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 168us/step - loss: 0.0189 - accuracy: 0.9940\n",
      "8333/8333 [==============================] - 1s 61us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 174us/step - loss: 0.5919 - accuracy: 0.6703\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 3s 155us/step - loss: 0.3045 - accuracy: 0.8777\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 3s 160us/step - loss: 0.2008 - accuracy: 0.9207\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 3s 159us/step - loss: 0.1328 - accuracy: 0.9533\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 3s 157us/step - loss: 0.0870 - accuracy: 0.9678\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 3s 158us/step - loss: 0.0563 - accuracy: 0.9803\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 3s 161us/step - loss: 0.0390 - accuracy: 0.9870\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 3s 164us/step - loss: 0.0307 - accuracy: 0.9885\n",
      "8334/8334 [==============================] - 1s 62us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 169us/step - loss: 0.5985 - accuracy: 0.6695\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 154us/step - loss: 0.3142 - accuracy: 0.8716\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 157us/step - loss: 0.2052 - accuracy: 0.9236\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.1397 - accuracy: 0.9477\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 165us/step - loss: 0.0907 - accuracy: 0.9685\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.0608 - accuracy: 0.9801\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.0455 - accuracy: 0.9844\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.0300 - accuracy: 0.9900\n",
      "8333/8333 [==============================] - 1s 63us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 172us/step - loss: 0.5946 - accuracy: 0.6677\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 161us/step - loss: 0.3164 - accuracy: 0.8751\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 160us/step - loss: 0.2094 - accuracy: 0.9223\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 159us/step - loss: 0.1432 - accuracy: 0.9474\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 159us/step - loss: 0.0902 - accuracy: 0.9688\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 161us/step - loss: 0.0632 - accuracy: 0.9780\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 160us/step - loss: 0.0442 - accuracy: 0.98420s - loss: 0.0443 - accuracy\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 166us/step - loss: 0.0327 - accuracy: 0.9893\n",
      "8333/8333 [==============================] - 1s 67us/step\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16666/16666 [==============================] - 3s 186us/step - loss: 0.5908 - accuracy: 0.6612\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 3s 172us/step - loss: 0.2684 - accuracy: 0.8945\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 3s 179us/step - loss: 0.1283 - accuracy: 0.9549\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 3s 175us/step - loss: 0.0643 - accuracy: 0.9800\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 3s 183us/step - loss: 0.0401 - accuracy: 0.9875\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 3s 176us/step - loss: 0.0274 - accuracy: 0.9920\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 3s 175us/step - loss: 0.0200 - accuracy: 0.9938\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 3s 176us/step - loss: 0.0199 - accuracy: 0.9935\n",
      "8334/8334 [==============================] - 1s 71us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 187us/step - loss: 0.5991 - accuracy: 0.6591\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 177us/step - loss: 0.2740 - accuracy: 0.8916\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 176us/step - loss: 0.1317 - accuracy: 0.9542\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 176us/step - loss: 0.0616 - accuracy: 0.9821\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 176us/step - loss: 0.0406 - accuracy: 0.9872\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 175us/step - loss: 0.0268 - accuracy: 0.9920\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 182us/step - loss: 0.0188 - accuracy: 0.9947\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 177us/step - loss: 0.0209 - accuracy: 0.9930\n",
      "8333/8333 [==============================] - 1s 72us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 190us/step - loss: 0.5953 - accuracy: 0.6623\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 172us/step - loss: 0.2713 - accuracy: 0.8937\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 175us/step - loss: 0.1298 - accuracy: 0.9551\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 183us/step - loss: 0.0643 - accuracy: 0.9800\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 177us/step - loss: 0.0397 - accuracy: 0.9888\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 176us/step - loss: 0.0277 - accuracy: 0.9913\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 177us/step - loss: 0.0225 - accuracy: 0.9932\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 176us/step - loss: 0.0195 - accuracy: 0.9931\n",
      "8333/8333 [==============================] - 1s 74us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 181us/step - loss: 0.5775 - accuracy: 0.6841\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 3s 164us/step - loss: 0.2958 - accuracy: 0.8786\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 3s 165us/step - loss: 0.1942 - accuracy: 0.9236\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 3s 169us/step - loss: 0.1293 - accuracy: 0.9514\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 3s 167us/step - loss: 0.0829 - accuracy: 0.9715\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 3s 173us/step - loss: 0.0575 - accuracy: 0.9807\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 3s 170us/step - loss: 0.0394 - accuracy: 0.9870\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 3s 167us/step - loss: 0.0320 - accuracy: 0.9894\n",
      "8334/8334 [==============================] - 1s 77us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 184us/step - loss: 0.5926 - accuracy: 0.6671\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 165us/step - loss: 0.3017 - accuracy: 0.8792\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 171us/step - loss: 0.1947 - accuracy: 0.9262\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 169us/step - loss: 0.1366 - accuracy: 0.9498\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 168us/step - loss: 0.0862 - accuracy: 0.9686\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 168us/step - loss: 0.0585 - accuracy: 0.9800\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 170us/step - loss: 0.0435 - accuracy: 0.9849\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 168us/step - loss: 0.0306 - accuracy: 0.9882\n",
      "8333/8333 [==============================] - 1s 86us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 188us/step - loss: 0.5938 - accuracy: 0.6687\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 166us/step - loss: 0.3086 - accuracy: 0.8745\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 167us/step - loss: 0.1930 - accuracy: 0.9249\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 170us/step - loss: 0.1251 - accuracy: 0.9533\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 171us/step - loss: 0.0811 - accuracy: 0.9711\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 177us/step - loss: 0.0558 - accuracy: 0.9799\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 170us/step - loss: 0.0382 - accuracy: 0.9870\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 171us/step - loss: 0.0294 - accuracy: 0.9891\n",
      "8333/8333 [==============================] - 1s 81us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 157us/step - loss: 0.6685 - accuracy: 0.5922\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 2s 139us/step - loss: 0.4059 - accuracy: 0.8433\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 2s 144us/step - loss: 0.2449 - accuracy: 0.9155\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 2s 142us/step - loss: 0.1576 - accuracy: 0.9518\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 2s 142us/step - loss: 0.1047 - accuracy: 0.9726\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 2s 142us/step - loss: 0.0740 - accuracy: 0.9800\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 2s 143us/step - loss: 0.0470 - accuracy: 0.9911\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 2s 142us/step - loss: 0.0346 - accuracy: 0.9947\n",
      "8334/8334 [==============================] - 1s 76us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 160us/step - loss: 0.6728 - accuracy: 0.5732\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 141us/step - loss: 0.4134 - accuracy: 0.8388\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 2s 141us/step - loss: 0.2462 - accuracy: 0.9135\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 144us/step - loss: 0.1592 - accuracy: 0.9513\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 146us/step - loss: 0.1018 - accuracy: 0.9738\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 2s 145us/step - loss: 0.0669 - accuracy: 0.9845\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 154us/step - loss: 0.0462 - accuracy: 0.9911\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.0346 - accuracy: 0.9932\n",
      "8333/8333 [==============================] - 1s 77us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 160us/step - loss: 0.6726 - accuracy: 0.5743\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 142us/step - loss: 0.4191 - accuracy: 0.8325\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 2s 142us/step - loss: 0.2468 - accuracy: 0.9135\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 146us/step - loss: 0.1617 - accuracy: 0.9506\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 150us/step - loss: 0.1015 - accuracy: 0.9741\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 2s 145us/step - loss: 0.0698 - accuracy: 0.9830\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 2s 146us/step - loss: 0.0466 - accuracy: 0.9908\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 2s 146us/step - loss: 0.0360 - accuracy: 0.9927\n",
      "8333/8333 [==============================] - ETA:  - 1s 80us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 157us/step - loss: 0.6611 - accuracy: 0.5964\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16666/16666 [==============================] - 2s 142us/step - loss: 0.4184 - accuracy: 0.8376\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 2s 138us/step - loss: 0.2765 - accuracy: 0.8990\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 2s 139us/step - loss: 0.2032 - accuracy: 0.9304\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 2s 141us/step - loss: 0.1519 - accuracy: 0.9509\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 2s 141us/step - loss: 0.1147 - accuracy: 0.9639\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 2s 141us/step - loss: 0.0848 - accuracy: 0.9738\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 2s 141us/step - loss: 0.0642 - accuracy: 0.9809\n",
      "8334/8334 [==============================] - 1s 84us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.6633 - accuracy: 0.5987\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 138us/step - loss: 0.4335 - accuracy: 0.8313\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 2s 138us/step - loss: 0.2927 - accuracy: 0.8898\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 139us/step - loss: 0.2239 - accuracy: 0.9213\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 141us/step - loss: 0.1720 - accuracy: 0.9422\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 2s 142us/step - loss: 0.1295 - accuracy: 0.9584\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.0997 - accuracy: 0.9694\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 2s 142us/step - loss: 0.0766 - accuracy: 0.9756\n",
      "8333/8333 [==============================] - 1s 83us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 160us/step - loss: 0.6580 - accuracy: 0.6091\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 141us/step - loss: 0.4201 - accuracy: 0.8366\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 2s 141us/step - loss: 0.2797 - accuracy: 0.8972\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 144us/step - loss: 0.2147 - accuracy: 0.9249\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 144us/step - loss: 0.1608 - accuracy: 0.9459\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 2s 143us/step - loss: 0.1225 - accuracy: 0.9609\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 2s 143us/step - loss: 0.0911 - accuracy: 0.9717\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 2s 143us/step - loss: 0.0679 - accuracy: 0.9800\n",
      "8333/8333 [==============================] - 1s 87us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 163us/step - loss: 0.6561 - accuracy: 0.5940\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 2s 148us/step - loss: 0.3567 - accuracy: 0.8601\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 2s 143us/step - loss: 0.2013 - accuracy: 0.9300\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 2s 144us/step - loss: 0.1183 - accuracy: 0.9661\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 2s 146us/step - loss: 0.0674 - accuracy: 0.9826\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 2s 146us/step - loss: 0.0448 - accuracy: 0.9893\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 2s 148us/step - loss: 0.0264 - accuracy: 0.9952\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 3s 151us/step - loss: 0.0211 - accuracy: 0.9963\n",
      "8334/8334 [==============================] - 1s 92us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 163us/step - loss: 0.6619 - accuracy: 0.5931\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 145us/step - loss: 0.3690 - accuracy: 0.8538\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 2s 144us/step - loss: 0.2029 - accuracy: 0.9286\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 144us/step - loss: 0.1201 - accuracy: 0.9645\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 146us/step - loss: 0.0700 - accuracy: 0.9831\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 154us/step - loss: 0.0448 - accuracy: 0.9904\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 2s 145us/step - loss: 0.0290 - accuracy: 0.9947\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 2s 145us/step - loss: 0.0221 - accuracy: 0.9956\n",
      "8333/8333 [==============================] - 1s 92us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 167us/step - loss: 0.6543 - accuracy: 0.6021\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 145us/step - loss: 0.3557 - accuracy: 0.8652\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.2014 - accuracy: 0.9306\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 146us/step - loss: 0.1162 - accuracy: 0.9648\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.0721 - accuracy: 0.9807\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.0429 - accuracy: 0.9911\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 2s 149us/step - loss: 0.0307 - accuracy: 0.9926\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 2s 149us/step - loss: 0.0208 - accuracy: 0.9959\n",
      "8333/8333 [==============================] - 1s 95us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 162us/step - loss: 0.6423 - accuracy: 0.6234\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 2s 140us/step - loss: 0.3782 - accuracy: 0.8538\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 2s 140us/step - loss: 0.2494 - accuracy: 0.9070\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 2s 140us/step - loss: 0.1806 - accuracy: 0.9346\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 2s 141us/step - loss: 0.1277 - accuracy: 0.9564\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 2s 143us/step - loss: 0.0888 - accuracy: 0.9701\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 2s 149us/step - loss: 0.0625 - accuracy: 0.9810\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 2s 145us/step - loss: 0.0451 - accuracy: 0.9861\n",
      "8334/8334 [==============================] - 1s 96us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 164us/step - loss: 0.6525 - accuracy: 0.6138\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 143us/step - loss: 0.3872 - accuracy: 0.8498\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 2s 143us/step - loss: 0.2516 - accuracy: 0.9074\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 144us/step - loss: 0.1801 - accuracy: 0.9359\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 163us/step - loss: 0.1302 - accuracy: 0.9558\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 2s 149us/step - loss: 0.0890 - accuracy: 0.9708\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 2s 145us/step - loss: 0.0592 - accuracy: 0.9816\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 2s 145us/step - loss: 0.0426 - accuracy: 0.9873\n",
      "8333/8333 [==============================] - 1s 97us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 166us/step - loss: 0.6415 - accuracy: 0.6285\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 146us/step - loss: 0.3796 - accuracy: 0.8517\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 2s 143us/step - loss: 0.2511 - accuracy: 0.9057\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 143us/step - loss: 0.1800 - accuracy: 0.9358\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 142us/step - loss: 0.1330 - accuracy: 0.9537\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 2s 146us/step - loss: 0.0952 - accuracy: 0.9689\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 2s 146us/step - loss: 0.0654 - accuracy: 0.9781\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 2s 150us/step - loss: 0.0494 - accuracy: 0.9840\n",
      "8333/8333 [==============================] - 1s 103us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 171us/step - loss: 0.6470 - accuracy: 0.6096\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 2s 148us/step - loss: 0.3346 - accuracy: 0.8654\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 2s 149us/step - loss: 0.1814 - accuracy: 0.9375\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 2s 148us/step - loss: 0.1042 - accuracy: 0.9687\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 3s 152us/step - loss: 0.0601 - accuracy: 0.9849\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 3s 156us/step - loss: 0.0353 - accuracy: 0.9920\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 2s 150us/step - loss: 0.0247 - accuracy: 0.9953\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 2s 150us/step - loss: 0.0166 - accuracy: 0.9962\n",
      "8334/8334 [==============================] - 1s 104us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 170us/step - loss: 0.6398 - accuracy: 0.6216\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 149us/step - loss: 0.3256 - accuracy: 0.8738\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 2s 149us/step - loss: 0.1798 - accuracy: 0.9389\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 147us/step - loss: 0.0998 - accuracy: 0.9701\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 149us/step - loss: 0.0562 - accuracy: 0.98490s - loss: 0.0567 - accuracy: 0.98 - ETA: 0s\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.0357 - accuracy: 0.9915\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.0258 - accuracy: 0.9939\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.0176 - accuracy: 0.9968\n",
      "8333/8333 [==============================] - 1s 108us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 174us/step - loss: 0.6536 - accuracy: 0.6069\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.3478 - accuracy: 0.86120s - loss: 0.3497 - accuracy\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.1888 - accuracy: 0.9336\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.1053 - accuracy: 0.9668\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.0591 - accuracy: 0.9851\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 162us/step - loss: 0.0351 - accuracy: 0.9930\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 154us/step - loss: 0.0235 - accuracy: 0.9950\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 153us/step - loss: 0.0189 - accuracy: 0.99560s - l\n",
      "8333/8333 [==============================] - 1s 109us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - ETA: 0s - loss: 0.6156 - accuracy: 0.65 - 3s 172us/step - loss: 0.6153 - accuracy: 0.6553\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 2s 147us/step - loss: 0.3365 - accuracy: 0.8661\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 3s 151us/step - loss: 0.2249 - accuracy: 0.9153\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 2s 147us/step - loss: 0.1527 - accuracy: 0.9450\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 2s 147us/step - loss: 0.1034 - accuracy: 0.9639\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 2s 147us/step - loss: 0.0698 - accuracy: 0.9760\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 2s 147us/step - loss: 0.0467 - accuracy: 0.9861\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - ETA: 0s - loss: 0.0354 - accuracy: 0.98 - 2s 149us/step - loss: 0.0354 - accuracy: 0.9878\n",
      "8334/8334 [==============================] - 1s 112us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 172us/step - loss: 0.6382 - accuracy: 0.6333\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 149us/step - loss: 0.3654 - accuracy: 0.8531\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.2372 - accuracy: 0.9091\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 149us/step - loss: 0.1693 - accuracy: 0.9392\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.1150 - accuracy: 0.9624\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 151us/step - loss: 0.0827 - accuracy: 0.9731\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.0527 - accuracy: 0.9833\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 150us/step - loss: 0.0408 - accuracy: 0.9868\n",
      "8333/8333 [==============================] - 1s 114us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 174us/step - loss: 0.6326 - accuracy: 0.6408\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 2s 147us/step - loss: 0.3600 - accuracy: 0.8532\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 150us/step - loss: 0.2371 - accuracy: 0.9089\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 2s 147us/step - loss: 0.1685 - accuracy: 0.9362\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.1191 - accuracy: 0.9584\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.0844 - accuracy: 0.9710\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 2s 148us/step - loss: 0.0569 - accuracy: 0.9809\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 2s 150us/step - loss: 0.0404 - accuracy: 0.9857\n",
      "8333/8333 [==============================] - 1s 115us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 3s 182us/step - loss: 0.6245 - accuracy: 0.6370\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 3s 156us/step - loss: 0.3097 - accuracy: 0.8765\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - ETA: 0s - loss: 0.1607 - accuracy: 0.9445 - 3s 156us/step - loss: 0.1607 - accuracy: 0.9446\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 3s 157us/step - loss: 0.0857 - accuracy: 0.97520s\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 3s 157us/step - loss: 0.0518 - accuracy: 0.98640s - loss: 0.0528 - accura\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 3s 163us/step - loss: 0.0311 - accuracy: 0.9923\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 3s 163us/step - loss: 0.0218 - accuracy: 0.99450s - l\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 3s 160us/step - loss: 0.0155 - accuracy: 0.9963\n",
      "8334/8334 [==============================] - 1s 121us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 186us/step - loss: 0.6460 - accuracy: 0.6047\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.3217 - accuracy: 0.8742\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 162us/step - loss: 0.1698 - accuracy: 0.9417\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 158us/step - loss: 0.0911 - accuracy: 0.97260s - loss: 0.0911 - accuracy: 0.97\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 159us/step - loss: 0.0509 - accuracy: 0.9869\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 161us/step - loss: 0.0341 - accuracy: 0.9909\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 162us/step - loss: 0.0229 - accuracy: 0.9938\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 162us/step - loss: 0.0170 - accuracy: 0.9962\n",
      "8333/8333 [==============================] - 1s 128us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 183us/step - loss: 0.6505 - accuracy: 0.60560s - loss: 0.6623 - accura\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 156us/step - loss: 0.3315 - accuracy: 0.8677\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 156us/step - loss: 0.1758 - accuracy: 0.9388\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 156us/step - loss: 0.0943 - accuracy: 0.9710\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 161us/step - loss: 0.0534 - accuracy: 0.9860\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 165us/step - loss: 0.0324 - accuracy: 0.9922\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 160us/step - loss: 0.0199 - accuracy: 0.9959\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 161us/step - loss: 0.0195 - accuracy: 0.9955\n",
      "8333/8333 [==============================] - 1s 126us/step\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16666/16666 [==============================] - 3s 181us/step - loss: 0.6223 - accuracy: 0.6426\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 3s 156us/step - loss: 0.3433 - accuracy: 0.8594\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 3s 152us/step - loss: 0.2193 - accuracy: 0.9176\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 3s 153us/step - loss: 0.1496 - accuracy: 0.9456\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 3s 152us/step - loss: 0.1032 - accuracy: 0.9632\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 3s 153us/step - loss: 0.0697 - accuracy: 0.9771\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 3s 152us/step - loss: 0.0449 - accuracy: 0.9857\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 3s 156us/step - loss: 0.0369 - accuracy: 0.9879\n",
      "8334/8334 [==============================] - 1s 131us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 179us/step - loss: 0.6382 - accuracy: 0.6279\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.3563 - accuracy: 0.8565\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.2313 - accuracy: 0.9111\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.1579 - accuracy: 0.9426\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 157us/step - loss: 0.1027 - accuracy: 0.9671\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.0689 - accuracy: 0.9771\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 152us/step - loss: 0.0454 - accuracy: 0.9851\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 154us/step - loss: 0.0350 - accuracy: 0.9885\n",
      "8333/8333 [==============================] - 1s 129us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 3s 181us/step - loss: 0.6295 - accuracy: 0.6417\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 3s 156us/step - loss: 0.3536 - accuracy: 0.8594\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 3s 153us/step - loss: 0.2274 - accuracy: 0.9135\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 3s 154us/step - loss: 0.1544 - accuracy: 0.9431\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 3s 153us/step - loss: 0.1053 - accuracy: 0.9641\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 3s 154us/step - loss: 0.0693 - accuracy: 0.9765\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 3s 153us/step - loss: 0.0464 - accuracy: 0.9847\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 3s 159us/step - loss: 0.0321 - accuracy: 0.9885\n",
      "8333/8333 [==============================] - 1s 137us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 5s 309us/step - loss: 0.6069 - accuracy: 0.6565\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 280us/step - loss: 0.2903 - accuracy: 0.8925\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 283us/step - loss: 0.1447 - accuracy: 0.9570\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 280us/step - loss: 0.0719 - accuracy: 0.9826\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 284us/step - loss: 0.0388 - accuracy: 0.9927\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 288us/step - loss: 0.0244 - accuracy: 0.9958\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 283us/step - loss: 0.0165 - accuracy: 0.9970\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 283us/step - loss: 0.0130 - accuracy: 0.9975\n",
      "8334/8334 [==============================] - 1s 158us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 312us/step - loss: 0.6057 - accuracy: 0.6605\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 284us/step - loss: 0.2925 - accuracy: 0.8929\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 285us/step - loss: 0.1504 - accuracy: 0.9548\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 288us/step - loss: 0.0772 - accuracy: 0.9813\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 286us/step - loss: 0.0392 - accuracy: 0.9924\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 286us/step - loss: 0.0246 - accuracy: 0.9962\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 291us/step - loss: 0.0170 - accuracy: 0.9973\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 286us/step - loss: 0.0143 - accuracy: 0.9968\n",
      "8333/8333 [==============================] - 1s 162us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 316us/step - loss: 0.6248 - accuracy: 0.6414\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 287us/step - loss: 0.3044 - accuracy: 0.8867\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 284us/step - loss: 0.1563 - accuracy: 0.9529\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 284us/step - loss: 0.0744 - accuracy: 0.9815\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 292us/step - loss: 0.0396 - accuracy: 0.9926\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 288us/step - loss: 0.0264 - accuracy: 0.9949\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 288us/step - loss: 0.0162 - accuracy: 0.9977\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 295us/step - loss: 0.0137 - accuracy: 0.9973\n",
      "8333/8333 [==============================] - 1s 164us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 5s 302us/step - loss: 0.6099 - accuracy: 0.6600\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 280us/step - loss: 0.3270 - accuracy: 0.8745\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 280us/step - loss: 0.2087 - accuracy: 0.9261\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 276us/step - loss: 0.1361 - accuracy: 0.95730s - loss: 0.1357 - accuracy: 0.\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 276us/step - loss: 0.0840 - accuracy: 0.97440s - loss: 0.081\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 285us/step - loss: 0.0538 - accuracy: 0.9839\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 279us/step - loss: 0.0340 - accuracy: 0.9903\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 6s 343us/step - loss: 0.0227 - accuracy: 0.99390s - loss: 0.0228 - accuracy: 0.99 - ETA: 0s - loss: 0.022\n",
      "8334/8334 [==============================] - 1s 168us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 305us/step - loss: 0.6168 - accuracy: 0.6506\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 277us/step - loss: 0.3336 - accuracy: 0.8735\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 277us/step - loss: 0.2141 - accuracy: 0.9223\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 282us/step - loss: 0.1458 - accuracy: 0.9518\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 277us/step - loss: 0.0900 - accuracy: 0.9741\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 279us/step - loss: 0.0586 - accuracy: 0.9835\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 289us/step - loss: 0.0360 - accuracy: 0.9906\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.0232 - accuracy: 0.9930\n",
      "8333/8333 [==============================] - 1s 170us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 307us/step - loss: 0.6284 - accuracy: 0.6424\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 281us/step - loss: 0.3527 - accuracy: 0.8629\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 277us/step - loss: 0.2332 - accuracy: 0.9128\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.1584 - accuracy: 0.9465\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 282us/step - loss: 0.1032 - accuracy: 0.9674\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.0663 - accuracy: 0.97980s - loss: 0.0654 - accuracy: 0.\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.0419 - accuracy: 0.9878\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 283us/step - loss: 0.0288 - accuracy: 0.9918\n",
      "8333/8333 [==============================] - 1s 174us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 5s 322us/step - loss: 0.5930 - accuracy: 0.6618\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 292us/step - loss: 0.2592 - accuracy: 0.9030\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 295us/step - loss: 0.1131 - accuracy: 0.9663\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 291us/step - loss: 0.0470 - accuracy: 0.9897\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 293us/step - loss: 0.0240 - accuracy: 0.9957\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 301us/step - loss: 0.0149 - accuracy: 0.9972\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 295us/step - loss: 0.0111 - accuracy: 0.9977\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 295us/step - loss: 0.0096 - accuracy: 0.9978\n",
      "8334/8334 [==============================] - 1s 178us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 325us/step - loss: 0.5966 - accuracy: 0.6626\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 294us/step - loss: 0.2583 - accuracy: 0.9046\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 295us/step - loss: 0.1091 - accuracy: 0.9688\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 294us/step - loss: 0.0465 - accuracy: 0.9902\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 295us/step - loss: 0.0273 - accuracy: 0.9938\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 296us/step - loss: 0.0158 - accuracy: 0.9962\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 299us/step - loss: 0.0118 - accuracy: 0.9975\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 295us/step - loss: 0.0083 - accuracy: 0.9986\n",
      "8333/8333 [==============================] - 1s 180us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 327us/step - loss: 0.5955 - accuracy: 0.6641\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 292us/step - loss: 0.2619 - accuracy: 0.9009\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 292us/step - loss: 0.1119 - accuracy: 0.9668\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 296us/step - loss: 0.0475 - accuracy: 0.9888\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 295us/step - loss: 0.0234 - accuracy: 0.9960\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 295us/step - loss: 0.0151 - accuracy: 0.9972\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 298us/step - loss: 0.0109 - accuracy: 0.9985\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 300us/step - loss: 0.0118 - accuracy: 0.9968\n",
      "8333/8333 [==============================] - 2s 184us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 5s 317us/step - loss: 0.5913 - accuracy: 0.6749\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 312us/step - loss: 0.3044 - accuracy: 0.8786\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 288us/step - loss: 0.1872 - accuracy: 0.9302\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 289us/step - loss: 0.1137 - accuracy: 0.9612\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 307us/step - loss: 0.0660 - accuracy: 0.9793\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 299us/step - loss: 0.0371 - accuracy: 0.9885\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 300us/step - loss: 0.0213 - accuracy: 0.9934\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 301us/step - loss: 0.0151 - accuracy: 0.9956\n",
      "8334/8334 [==============================] - 2s 187us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 322us/step - loss: 0.5966 - accuracy: 0.6679\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 292us/step - loss: 0.3008 - accuracy: 0.8815\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 292us/step - loss: 0.1801 - accuracy: 0.9351\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 288us/step - loss: 0.1044 - accuracy: 0.9648\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 292us/step - loss: 0.0572 - accuracy: 0.9833\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 293us/step - loss: 0.0338 - accuracy: 0.9908\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 289us/step - loss: 0.0201 - accuracy: 0.9940\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 290us/step - loss: 0.0138 - accuracy: 0.9959\n",
      "8333/8333 [==============================] - 2s 193us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 323us/step - loss: 0.6005 - accuracy: 0.6630\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 293us/step - loss: 0.3058 - accuracy: 0.8796\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 296us/step - loss: 0.1797 - accuracy: 0.9343\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 293us/step - loss: 0.1016 - accuracy: 0.9654\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 294us/step - loss: 0.0578 - accuracy: 0.9819\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 295us/step - loss: 0.0348 - accuracy: 0.9896\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 292us/step - loss: 0.0216 - accuracy: 0.9927\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 292us/step - loss: 0.0139 - accuracy: 0.9960\n",
      "8333/8333 [==============================] - 2s 196us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 6s 343us/step - loss: 0.5773 - accuracy: 0.6763\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 308us/step - loss: 0.2407 - accuracy: 0.9086\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 312us/step - loss: 0.0957 - accuracy: 0.9719\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 314us/step - loss: 0.0412 - accuracy: 0.9891\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 310us/step - loss: 0.0207 - accuracy: 0.9955\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 310us/step - loss: 0.0132 - accuracy: 0.9973\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 319us/step - loss: 0.0111 - accuracy: 0.9971\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 313us/step - loss: 0.0113 - accuracy: 0.9968\n",
      "8334/8334 [==============================] - 2s 197us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 352us/step - loss: 0.5784 - accuracy: 0.6725\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 311us/step - loss: 0.2408 - accuracy: 0.9083\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 313us/step - loss: 0.0959 - accuracy: 0.9702\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 312us/step - loss: 0.0405 - accuracy: 0.9897\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 309us/step - loss: 0.0232 - accuracy: 0.9948\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 314us/step - loss: 0.0152 - accuracy: 0.9965\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 320us/step - loss: 0.0138 - accuracy: 0.9966\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 314us/step - loss: 0.0083 - accuracy: 0.9983\n",
      "8333/8333 [==============================] - 2s 201us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 347us/step - loss: 0.5967 - accuracy: 0.6570\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 309us/step - loss: 0.2541 - accuracy: 0.9019\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 309us/step - loss: 0.0955 - accuracy: 0.9726\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 312us/step - loss: 0.0392 - accuracy: 0.9908\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 318us/step - loss: 0.0197 - accuracy: 0.9965\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 320us/step - loss: 0.0132 - accuracy: 0.9972\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 320us/step - loss: 0.0101 - accuracy: 0.9976\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 313us/step - loss: 0.0079 - accuracy: 0.9981\n",
      "8333/8333 [==============================] - 2s 207us/step\n",
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16666/16666 [==============================] - 5s 329us/step - loss: 0.5699 - accuracy: 0.6910\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 293us/step - loss: 0.2773 - accuracy: 0.8917\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 292us/step - loss: 0.1560 - accuracy: 0.9416\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 296us/step - loss: 0.0848 - accuracy: 0.9711\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 294us/step - loss: 0.0448 - accuracy: 0.9852\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 292us/step - loss: 0.0285 - accuracy: 0.9899\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 295us/step - loss: 0.0172 - accuracy: 0.9945\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 295us/step - loss: 0.0114 - accuracy: 0.9965\n",
      "8334/8334 [==============================] - 2s 202us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 337us/step - loss: 0.5725 - accuracy: 0.6821\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 297us/step - loss: 0.2773 - accuracy: 0.8916\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 300us/step - loss: 0.1594 - accuracy: 0.9428\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 300us/step - loss: 0.0842 - accuracy: 0.9723\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 302us/step - loss: 0.0462 - accuracy: 0.9853\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 298us/step - loss: 0.0247 - accuracy: 0.9927\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 298us/step - loss: 0.0166 - accuracy: 0.9943\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 302us/step - loss: 0.0121 - accuracy: 0.9961\n",
      "8333/8333 [==============================] - 2s 206us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 332us/step - loss: 0.5845 - accuracy: 0.6788\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 302us/step - loss: 0.2893 - accuracy: 0.8833\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 298us/step - loss: 0.1626 - accuracy: 0.9422\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 298us/step - loss: 0.0908 - accuracy: 0.9690\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 302us/step - loss: 0.0499 - accuracy: 0.9841\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 298us/step - loss: 0.0292 - accuracy: 0.9913\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 299us/step - loss: 0.0210 - accuracy: 0.9932\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 302us/step - loss: 0.0139 - accuracy: 0.9954\n",
      "8333/8333 [==============================] - 2s 207us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 6s 358us/step - loss: 0.5691 - accuracy: 0.6817\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 320us/step - loss: 0.2354 - accuracy: 0.9104\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 316us/step - loss: 0.0885 - accuracy: 0.9733\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 316us/step - loss: 0.0369 - accuracy: 0.9898\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 325us/step - loss: 0.0194 - accuracy: 0.9955\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 320us/step - loss: 0.0142 - accuracy: 0.9967\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 322us/step - loss: 0.0103 - accuracy: 0.9977\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 329us/step - loss: 0.0101 - accuracy: 0.9972\n",
      "8334/8334 [==============================] - 2s 222us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 356us/step - loss: 0.5797 - accuracy: 0.6738\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 321us/step - loss: 0.2371 - accuracy: 0.9095\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 316us/step - loss: 0.0925 - accuracy: 0.9720\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 317us/step - loss: 0.0379 - accuracy: 0.9908\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 329us/step - loss: 0.0200 - accuracy: 0.9957\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 321us/step - loss: 0.0130 - accuracy: 0.9968\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 323us/step - loss: 0.0107 - accuracy: 0.9975\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 329us/step - loss: 0.0110 - accuracy: 0.9962\n",
      "8333/8333 [==============================] - 2s 220us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 357us/step - loss: 0.5723 - accuracy: 0.6787\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 322us/step - loss: 0.2373 - accuracy: 0.9102\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 315us/step - loss: 0.0876 - accuracy: 0.9742\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 315us/step - loss: 0.0364 - accuracy: 0.9897\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 6s 331us/step - loss: 0.0183 - accuracy: 0.9960\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 319us/step - loss: 0.0129 - accuracy: 0.9969\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 323us/step - loss: 0.0120 - accuracy: 0.9968\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 326us/step - loss: 0.0110 - accuracy: 0.9963\n",
      "8333/8333 [==============================] - 2s 223us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 6s 340us/step - loss: 0.5842 - accuracy: 0.6702\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 6s 331us/step - loss: 0.2741 - accuracy: 0.8936\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - ETA: 0s - loss: 0.1550 - accuracy: 0.9447 E - 5s 316us/step - loss: 0.1554 - accuracy: 0.9446\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 309us/step - loss: 0.0795 - accuracy: 0.9737\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 312us/step - loss: 0.0463 - accuracy: 0.9851\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 305us/step - loss: 0.0256 - accuracy: 0.9917\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 306us/step - loss: 0.0168 - accuracy: 0.9948\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 313us/step - loss: 0.0131 - accuracy: 0.9953\n",
      "8334/8334 [==============================] - 2s 226us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 343us/step - loss: 0.5778 - accuracy: 0.6780\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 308us/step - loss: 0.2770 - accuracy: 0.8918\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 305us/step - loss: 0.1502 - accuracy: 0.9459\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 304us/step - loss: 0.0798 - accuracy: 0.9730\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 309us/step - loss: 0.0470 - accuracy: 0.9842\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 306us/step - loss: 0.0261 - accuracy: 0.9915\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 307us/step - loss: 0.0185 - accuracy: 0.9946\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 321us/step - loss: 0.0118 - accuracy: 0.9958\n",
      "8333/8333 [==============================] - 2s 230us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 346us/step - loss: 0.5731 - accuracy: 0.6851\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 311us/step - loss: 0.2725 - accuracy: 0.8904\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 307us/step - loss: 0.1537 - accuracy: 0.9431\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 307us/step - loss: 0.0800 - accuracy: 0.9731\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 311us/step - loss: 0.0432 - accuracy: 0.9856\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 308us/step - loss: 0.0270 - accuracy: 0.9909\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 310us/step - loss: 0.0167 - accuracy: 0.9941\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 315us/step - loss: 0.0139 - accuracy: 0.9950\n",
      "8333/8333 [==============================] - 2s 234us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 5s 300us/step - loss: 0.6713 - accuracy: 0.5818\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 4s 261us/step - loss: 0.3923 - accuracy: 0.8502\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 4s 261us/step - loss: 0.2128 - accuracy: 0.9297\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 4s 259us/step - loss: 0.1251 - accuracy: 0.9671\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 4s 260us/step - loss: 0.0684 - accuracy: 0.9878\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 4s 263us/step - loss: 0.0416 - accuracy: 0.9930\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 4s 259us/step - loss: 0.0281 - accuracy: 0.9960\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 4s 259us/step - loss: 0.0206 - accuracy: 0.9970\n",
      "8334/8334 [==============================] - 2s 209us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 302us/step - loss: 0.6650 - accuracy: 0.5955\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 4s 262us/step - loss: 0.3853 - accuracy: 0.8535\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 4s 262us/step - loss: 0.2100 - accuracy: 0.9320\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 4s 267us/step - loss: 0.1217 - accuracy: 0.9699\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 4s 262us/step - loss: 0.0706 - accuracy: 0.9865\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 4s 263us/step - loss: 0.0445 - accuracy: 0.9925\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 4s 266us/step - loss: 0.0292 - accuracy: 0.9959\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 4s 263us/step - loss: 0.0201 - accuracy: 0.9966\n",
      "8333/8333 [==============================] - 2s 214us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 306us/step - loss: 0.6529 - accuracy: 0.6125\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 286us/step - loss: 0.3565 - accuracy: 0.8685\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 272us/step - loss: 0.1966 - accuracy: 0.9358\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 4s 266us/step - loss: 0.1150 - accuracy: 0.9702\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 4s 268us/step - loss: 0.0662 - accuracy: 0.9869\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 4s 266us/step - loss: 0.0417 - accuracy: 0.9938\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 4s 264us/step - loss: 0.0275 - accuracy: 0.9966\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 4s 264us/step - loss: 0.0208 - accuracy: 0.9969\n",
      "8333/8333 [==============================] - 2s 221us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 5s 297us/step - loss: 0.6319 - accuracy: 0.6415\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 4s 256us/step - loss: 0.3713 - accuracy: 0.8577\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 4s 259us/step - loss: 0.2412 - accuracy: 0.9156\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 4s 256us/step - loss: 0.1674 - accuracy: 0.9449\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 4s 255us/step - loss: 0.1122 - accuracy: 0.9669\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 4s 256us/step - loss: 0.0734 - accuracy: 0.9801\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 4s 259us/step - loss: 0.0476 - accuracy: 0.9868\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 4s 255us/step - loss: 0.0300 - accuracy: 0.9921\n",
      "8334/8334 [==============================] - 2s 221us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 300us/step - loss: 0.6515 - accuracy: 0.6179\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 4s 254us/step - loss: 0.3944 - accuracy: 0.8526\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 4s 254us/step - loss: 0.2542 - accuracy: 0.9096\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 4s 255us/step - loss: 0.1822 - accuracy: 0.9384\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 4s 256us/step - loss: 0.1251 - accuracy: 0.9605\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 4s 254us/step - loss: 0.0809 - accuracy: 0.9777\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 4s 255us/step - loss: 0.0549 - accuracy: 0.9843\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 4s 256us/step - loss: 0.0368 - accuracy: 0.9903\n",
      "8333/8333 [==============================] - 2s 226us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 299us/step - loss: 0.6507 - accuracy: 0.6209\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 4s 257us/step - loss: 0.4038 - accuracy: 0.8462\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 4s 257us/step - loss: 0.2675 - accuracy: 0.9035\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 4s 257us/step - loss: 0.1914 - accuracy: 0.9333\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 4s 259us/step - loss: 0.1341 - accuracy: 0.9553\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 4s 259us/step - loss: 0.0898 - accuracy: 0.9726\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 4s 258us/step - loss: 0.0587 - accuracy: 0.9834\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 4s 257us/step - loss: 0.0423 - accuracy: 0.9884\n",
      "8333/8333 [==============================] - 2s 225us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 5s 319us/step - loss: 0.6323 - accuracy: 0.6334\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 273us/step - loss: 0.3144 - accuracy: 0.8817\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 273us/step - loss: 0.1594 - accuracy: 0.9515\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 277us/step - loss: 0.0769 - accuracy: 0.9827\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 274us/step - loss: 0.0409 - accuracy: 0.9926\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 274us/step - loss: 0.0223 - accuracy: 0.9973\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 277us/step - loss: 0.0138 - accuracy: 0.9981\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 275us/step - loss: 0.0097 - accuracy: 0.9992\n",
      "8334/8334 [==============================] - 2s 232us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 323us/step - loss: 0.6448 - accuracy: 0.6132\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.3280 - accuracy: 0.8748\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 276us/step - loss: 0.1675 - accuracy: 0.9470\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.0795 - accuracy: 0.9817\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 283us/step - loss: 0.0407 - accuracy: 0.9933\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 277us/step - loss: 0.0237 - accuracy: 0.9974\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.0145 - accuracy: 0.9986\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.0118 - accuracy: 0.9981\n",
      "8333/8333 [==============================] - 2s 235us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 326us/step - loss: 0.6350 - accuracy: 0.6235\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 281us/step - loss: 0.3128 - accuracy: 0.8804\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.1558 - accuracy: 0.9533\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.0813 - accuracy: 0.9793\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 279us/step - loss: 0.0409 - accuracy: 0.9930\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.0243 - accuracy: 0.9968\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.0146 - accuracy: 0.9989\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.0108 - accuracy: 0.9986\n",
      "8333/8333 [==============================] - 2s 245us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 5s 316us/step - loss: 0.6296 - accuracy: 0.6390\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16666/16666 [==============================] - 5s 271us/step - loss: 0.3452 - accuracy: 0.8647\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 274us/step - loss: 0.2102 - accuracy: 0.9262\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 271us/step - loss: 0.1305 - accuracy: 0.9564\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 272us/step - loss: 0.0749 - accuracy: 0.9782\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 273us/step - loss: 0.0446 - accuracy: 0.9882\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 273us/step - loss: 0.0265 - accuracy: 0.9931\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 271us/step - loss: 0.0169 - accuracy: 0.9954\n",
      "8334/8334 [==============================] - 2s 242us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 316us/step - loss: 0.6268 - accuracy: 0.6420\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 270us/step - loss: 0.3433 - accuracy: 0.8673\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 272us/step - loss: 0.2172 - accuracy: 0.9180\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 273us/step - loss: 0.1389 - accuracy: 0.9532\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 271us/step - loss: 0.0854 - accuracy: 0.9746\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 272us/step - loss: 0.0484 - accuracy: 0.9856\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 273us/step - loss: 0.0315 - accuracy: 0.9912\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 273us/step - loss: 0.0183 - accuracy: 0.9959\n",
      "8333/8333 [==============================] - 2s 244us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 321us/step - loss: 0.6420 - accuracy: 0.6254\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 276us/step - loss: 0.3561 - accuracy: 0.8631\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 274us/step - loss: 0.2146 - accuracy: 0.9219\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 275us/step - loss: 0.1360 - accuracy: 0.9539\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 292us/step - loss: 0.0801 - accuracy: 0.9759\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 284us/step - loss: 0.0477 - accuracy: 0.9861\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 279us/step - loss: 0.0293 - accuracy: 0.9924\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 286us/step - loss: 0.0175 - accuracy: 0.9954\n",
      "8333/8333 [==============================] - 2s 250us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 6s 334us/step - loss: 0.6248 - accuracy: 0.6365\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 288us/step - loss: 0.2958 - accuracy: 0.8832\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 284us/step - loss: 0.1399 - accuracy: 0.9585\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 285us/step - loss: 0.0656 - accuracy: 0.9842\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 287us/step - loss: 0.0338 - accuracy: 0.9932\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 286us/step - loss: 0.0171 - accuracy: 0.9975\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 286us/step - loss: 0.0126 - accuracy: 0.9983\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 286us/step - loss: 0.0091 - accuracy: 0.9987\n",
      "8334/8334 [==============================] - 2s 257us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 5s 329us/step - loss: 0.6388 - accuracy: 0.6144\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 281us/step - loss: 0.3044 - accuracy: 0.8826\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.1424 - accuracy: 0.9543\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.0623 - accuracy: 0.9857\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.0333 - accuracy: 0.9934\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 281us/step - loss: 0.0194 - accuracy: 0.9973\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 277us/step - loss: 0.0124 - accuracy: 0.9984\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.0085 - accuracy: 0.9993\n",
      "8333/8333 [==============================] - 2s 257us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 332us/step - loss: 0.6356 - accuracy: 0.6180\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 283us/step - loss: 0.3029 - accuracy: 0.8842\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 286us/step - loss: 0.1374 - accuracy: 0.9578\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 281us/step - loss: 0.0648 - accuracy: 0.9845\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 282us/step - loss: 0.0329 - accuracy: 0.9932\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 284us/step - loss: 0.0181 - accuracy: 0.9976\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 283us/step - loss: 0.0127 - accuracy: 0.9978\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 282us/step - loss: 0.0092 - accuracy: 0.9988\n",
      "8333/8333 [==============================] - 2s 261us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 5s 329us/step - loss: 0.6200 - accuracy: 0.6466\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 279us/step - loss: 0.3302 - accuracy: 0.8709\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 279us/step - loss: 0.1930 - accuracy: 0.9309\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 283us/step - loss: 0.1152 - accuracy: 0.9620\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 279us/step - loss: 0.0660 - accuracy: 0.9798\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 281us/step - loss: 0.0372 - accuracy: 0.9893\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 282us/step - loss: 0.0228 - accuracy: 0.9941\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 278us/step - loss: 0.0130 - accuracy: 0.9964\n",
      "8334/8334 [==============================] - 2s 267us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 335us/step - loss: 0.6198 - accuracy: 0.6442\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 279us/step - loss: 0.3233 - accuracy: 0.8705\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.1908 - accuracy: 0.9301\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 281us/step - loss: 0.1106 - accuracy: 0.9638\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.0661 - accuracy: 0.9794\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.0377 - accuracy: 0.9887\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 281us/step - loss: 0.0218 - accuracy: 0.9935\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 283us/step - loss: 0.0151 - accuracy: 0.9957\n",
      "8333/8333 [==============================] - 2s 265us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 332us/step - loss: 0.6265 - accuracy: 0.6365\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 279us/step - loss: 0.3308 - accuracy: 0.8703\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 277us/step - loss: 0.1979 - accuracy: 0.9272\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.1163 - accuracy: 0.9611\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 281us/step - loss: 0.0672 - accuracy: 0.9785\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.0390 - accuracy: 0.9886\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 278us/step - loss: 0.0222 - accuracy: 0.9935\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 279us/step - loss: 0.0157 - accuracy: 0.9955\n",
      "8333/8333 [==============================] - 2s 273us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 6s 343us/step - loss: 0.6138 - accuracy: 0.6472\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 294us/step - loss: 0.2757 - accuracy: 0.8948\n",
      "Epoch 3/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16666/16666 [==============================] - 5s 286us/step - loss: 0.1214 - accuracy: 0.9629\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 287us/step - loss: 0.0542 - accuracy: 0.9877\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 290us/step - loss: 0.0281 - accuracy: 0.9942\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 287us/step - loss: 0.0168 - accuracy: 0.9966\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 288us/step - loss: 0.0117 - accuracy: 0.9981\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 296us/step - loss: 0.0086 - accuracy: 0.9987\n",
      "8334/8334 [==============================] - 2s 280us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 339us/step - loss: 0.6235 - accuracy: 0.6404\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 288us/step - loss: 0.2847 - accuracy: 0.8918\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 286us/step - loss: 0.1276 - accuracy: 0.9582\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 290us/step - loss: 0.0564 - accuracy: 0.9867\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 287us/step - loss: 0.0264 - accuracy: 0.9951\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 288us/step - loss: 0.0177 - accuracy: 0.9971\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 287us/step - loss: 0.0118 - accuracy: 0.9977\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 288us/step - loss: 0.0079 - accuracy: 0.9989\n",
      "8333/8333 [==============================] - 2s 283us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 7s 405us/step - loss: 0.6108 - accuracy: 0.6491\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 287us/step - loss: 0.2778 - accuracy: 0.8949\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 285us/step - loss: 0.1268 - accuracy: 0.9617\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 284us/step - loss: 0.0562 - accuracy: 0.9855\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 287us/step - loss: 0.0301 - accuracy: 0.9935\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 287us/step - loss: 0.0185 - accuracy: 0.9962\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 287us/step - loss: 0.0117 - accuracy: 0.9980\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 287us/step - loss: 0.0075 - accuracy: 0.9989\n",
      "8333/8333 [==============================] - 2s 286us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 6s 331us/step - loss: 0.6073 - accuracy: 0.6602\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 5s 279us/step - loss: 0.3148 - accuracy: 0.8766\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 5s 280us/step - loss: 0.1848 - accuracy: 0.9327\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 5s 278us/step - loss: 0.1054 - accuracy: 0.9622\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 5s 279us/step - loss: 0.0564 - accuracy: 0.9822\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 5s 281us/step - loss: 0.0347 - accuracy: 0.9894\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 5s 278us/step - loss: 0.0196 - accuracy: 0.9944\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 5s 279us/step - loss: 0.0144 - accuracy: 0.9955\n",
      "8334/8334 [==============================] - 2s 288us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 332us/step - loss: 0.6186 - accuracy: 0.6452\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.3122 - accuracy: 0.8742\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 282us/step - loss: 0.1837 - accuracy: 0.9333\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.1049 - accuracy: 0.9657\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 279us/step - loss: 0.0618 - accuracy: 0.9800\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 281us/step - loss: 0.0349 - accuracy: 0.9892\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.0202 - accuracy: 0.9940\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 279us/step - loss: 0.0142 - accuracy: 0.9958\n",
      "8333/8333 [==============================] - 2s 288us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 6s 337us/step - loss: 0.6167 - accuracy: 0.6494\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 5s 281us/step - loss: 0.3173 - accuracy: 0.8755\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 5s 282us/step - loss: 0.1879 - accuracy: 0.9315\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 5s 283us/step - loss: 0.1064 - accuracy: 0.9632\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.0629 - accuracy: 0.9792\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 5s 281us/step - loss: 0.0345 - accuracy: 0.9900\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 5s 283us/step - loss: 0.0221 - accuracy: 0.9930\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 5s 280us/step - loss: 0.0148 - accuracy: 0.9957\n",
      "8333/8333 [==============================] - 2s 286us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 8s 500us/step - loss: 0.6058 - accuracy: 0.6573\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 7s 442us/step - loss: 0.2763 - accuracy: 0.8991\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 7s 449us/step - loss: 0.1254 - accuracy: 0.9648\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 7s 444us/step - loss: 0.0554 - accuracy: 0.9894\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 7s 448us/step - loss: 0.0292 - accuracy: 0.9963\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 7s 444us/step - loss: 0.0177 - accuracy: 0.9970\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - ETA: 0s - loss: 0.0121 - accuracy: 0.99 - 7s 448us/step - loss: 0.0121 - accuracy: 0.9982\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 7s 444us/step - loss: 0.0094 - accuracy: 0.9983\n",
      "8334/8334 [==============================] - 3s 315us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 8s 504us/step - loss: 0.6274 - accuracy: 0.6327\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 450us/step - loss: 0.2963 - accuracy: 0.8869\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 7s 448us/step - loss: 0.1378 - accuracy: 0.9607\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 451us/step - loss: 0.0585 - accuracy: 0.9884\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 446us/step - loss: 0.0295 - accuracy: 0.9951\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 452us/step - loss: 0.0188 - accuracy: 0.9967\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 448us/step - loss: 0.0135 - accuracy: 0.9980\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 450us/step - loss: 0.0097 - accuracy: 0.9984\n",
      "8333/8333 [==============================] - 3s 320us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 516us/step - loss: 0.6109 - accuracy: 0.6587\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 455us/step - loss: 0.2844 - accuracy: 0.8939\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 455us/step - loss: 0.1329 - accuracy: 0.9629\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 450us/step - loss: 0.0597 - accuracy: 0.9875\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 451us/step - loss: 0.0310 - accuracy: 0.9949\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 453us/step - loss: 0.0181 - accuracy: 0.9973\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 450us/step - loss: 0.0134 - accuracy: 0.9984\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 452us/step - loss: 0.0102 - accuracy: 0.9980\n",
      "8333/8333 [==============================] - 3s 328us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 8s 496us/step - loss: 0.6012 - accuracy: 0.6666\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 7s 430us/step - loss: 0.3120 - accuracy: 0.8807\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 7s 435us/step - loss: 0.1907 - accuracy: 0.9356\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 7s 429us/step - loss: 0.1087 - accuracy: 0.9657\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 7s 433us/step - loss: 0.0643 - accuracy: 0.9817\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 7s 427us/step - loss: 0.0353 - accuracy: 0.9912\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 7s 432us/step - loss: 0.0194 - accuracy: 0.9953\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 7s 428us/step - loss: 0.0130 - accuracy: 0.9963\n",
      "8334/8334 [==============================] - 3s 332us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 8s 492us/step - loss: 0.6244 - accuracy: 0.6494\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 7s 439us/step - loss: 0.3371 - accuracy: 0.8735\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 7s 436us/step - loss: 0.2086 - accuracy: 0.9293\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 7s 433us/step - loss: 0.1242 - accuracy: 0.9602\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 438us/step - loss: 0.0683 - accuracy: 0.9806\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 7s 432us/step - loss: 0.0410 - accuracy: 0.9871\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 437us/step - loss: 0.0220 - accuracy: 0.9944\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 7s 435us/step - loss: 0.0129 - accuracy: 0.9965\n",
      "8333/8333 [==============================] - 3s 339us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 8s 492us/step - loss: 0.6074 - accuracy: 0.6603\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 7s 437us/step - loss: 0.3102 - accuracy: 0.8809\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 7s 430us/step - loss: 0.1859 - accuracy: 0.9345\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 7s 433us/step - loss: 0.1044 - accuracy: 0.9666\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 430us/step - loss: 0.0590 - accuracy: 0.9833\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 7s 434us/step - loss: 0.0320 - accuracy: 0.9916\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 432us/step - loss: 0.0210 - accuracy: 0.9935\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 7s 431us/step - loss: 0.0137 - accuracy: 0.9960\n",
      "8333/8333 [==============================] - 3s 345us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 9s 536us/step - loss: 0.5684 - accuracy: 0.6866\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 8s 469us/step - loss: 0.2300 - accuracy: 0.9146\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 8s 475us/step - loss: 0.0835 - accuracy: 0.9782\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 8s 471us/step - loss: 0.0320 - accuracy: 0.9944\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 8s 479us/step - loss: 0.0162 - accuracy: 0.9974\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 8s 474us/step - loss: 0.0101 - accuracy: 0.9990\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 8s 473us/step - loss: 0.0064 - accuracy: 0.9993\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 8s 473us/step - loss: 0.0051 - accuracy: 0.9995\n",
      "8334/8334 [==============================] - 3s 351us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 533us/step - loss: 0.5912 - accuracy: 0.6671\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 477us/step - loss: 0.2460 - accuracy: 0.9060\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 475us/step - loss: 0.0895 - accuracy: 0.9754\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 475us/step - loss: 0.0349 - accuracy: 0.9933\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 474us/step - loss: 0.0173 - accuracy: 0.9974\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 477us/step - loss: 0.0095 - accuracy: 0.9990\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 473us/step - loss: 0.0061 - accuracy: 0.9993\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 477us/step - loss: 0.0048 - accuracy: 0.9998\n",
      "8333/8333 [==============================] - 3s 352us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 538us/step - loss: 0.5835 - accuracy: 0.6762\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 475us/step - loss: 0.2423 - accuracy: 0.9080\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 477us/step - loss: 0.0942 - accuracy: 0.9737\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 474us/step - loss: 0.0331 - accuracy: 0.9934\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 476us/step - loss: 0.0179 - accuracy: 0.9973\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 474us/step - loss: 0.0096 - accuracy: 0.9992\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 477us/step - loss: 0.0070 - accuracy: 0.9993\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 474us/step - loss: 0.0047 - accuracy: 0.9995\n",
      "8333/8333 [==============================] - 3s 361us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 8s 509us/step - loss: 0.5850 - accuracy: 0.6798\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 8s 451us/step - loss: 0.2792 - accuracy: 0.8916\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 7s 447us/step - loss: 0.1475 - accuracy: 0.9492\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 7s 447us/step - loss: 0.0753 - accuracy: 0.9776\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 7s 446us/step - loss: 0.0346 - accuracy: 0.9900\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 7s 448us/step - loss: 0.0175 - accuracy: 0.9951\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 7s 449us/step - loss: 0.0112 - accuracy: 0.9965\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 8s 455us/step - loss: 0.0070 - accuracy: 0.9979\n",
      "8334/8334 [==============================] - 3s 365us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 8s 507us/step - loss: 0.5825 - accuracy: 0.6814\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 7s 444us/step - loss: 0.2755 - accuracy: 0.89362s - loss:\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 7s 448us/step - loss: 0.1438 - accuracy: 0.9507\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 7s 446us/step - loss: 0.0746 - accuracy: 0.9775\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 445us/step - loss: 0.0352 - accuracy: 0.9900\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 7s 447us/step - loss: 0.0179 - accuracy: 0.9957\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 443us/step - loss: 0.0124 - accuracy: 0.9968\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 7s 447us/step - loss: 0.0064 - accuracy: 0.9980\n",
      "8333/8333 [==============================] - 3s 372us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 514us/step - loss: 0.5917 - accuracy: 0.6720\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 7s 446us/step - loss: 0.2785 - accuracy: 0.8912\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 452us/step - loss: 0.1507 - accuracy: 0.9470\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 7s 449us/step - loss: 0.0740 - accuracy: 0.9764\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 449us/step - loss: 0.0374 - accuracy: 0.9890\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 7s 445us/step - loss: 0.0202 - accuracy: 0.9947\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 453us/step - loss: 0.0119 - accuracy: 0.9969\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 7s 449us/step - loss: 0.0082 - accuracy: 0.9974\n",
      "8333/8333 [==============================] - 3s 367us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 9s 541us/step - loss: 0.5758 - accuracy: 0.6705\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 8s 477us/step - loss: 0.2118 - accuracy: 0.9194\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 8s 475us/step - loss: 0.0713 - accuracy: 0.9810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 8s 478us/step - loss: 0.0281 - accuracy: 0.9942\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 8s 475us/step - loss: 0.0142 - accuracy: 0.9980\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 8s 479us/step - loss: 0.0092 - accuracy: 0.9986\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 8s 476us/step - loss: 0.0074 - accuracy: 0.9986\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 8s 478us/step - loss: 0.0068 - accuracy: 0.9983\n",
      "8334/8334 [==============================] - 3s 372us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 548us/step - loss: 0.5691 - accuracy: 0.6809\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 478us/step - loss: 0.2240 - accuracy: 0.9158\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 478us/step - loss: 0.0719 - accuracy: 0.9828\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 478us/step - loss: 0.0289 - accuracy: 0.9945\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 479us/step - loss: 0.0140 - accuracy: 0.9977\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 478us/step - loss: 0.0096 - accuracy: 0.9978\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 479us/step - loss: 0.0068 - accuracy: 0.9989\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 479us/step - loss: 0.0066 - accuracy: 0.9987\n",
      "8333/8333 [==============================] - 3s 381us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 553us/step - loss: 0.5724 - accuracy: 0.6787\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 480us/step - loss: 0.2273 - accuracy: 0.9147\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 483us/step - loss: 0.0736 - accuracy: 0.9792\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 480us/step - loss: 0.0282 - accuracy: 0.9948\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 483us/step - loss: 0.0145 - accuracy: 0.9972\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 481us/step - loss: 0.0081 - accuracy: 0.9989\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 485us/step - loss: 0.0069 - accuracy: 0.9989\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 485us/step - loss: 0.0051 - accuracy: 0.9990\n",
      "8333/8333 [==============================] - 3s 381us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 9s 524us/step - loss: 0.5642 - accuracy: 0.6922\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 8s 460us/step - loss: 0.2602 - accuracy: 0.9004\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 8s 458us/step - loss: 0.1348 - accuracy: 0.9531\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 8s 460us/step - loss: 0.0632 - accuracy: 0.9785\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 8s 459us/step - loss: 0.0316 - accuracy: 0.9902\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 8s 461us/step - loss: 0.0170 - accuracy: 0.9945\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 8s 459us/step - loss: 0.0098 - accuracy: 0.9968\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 8s 469us/step - loss: 0.0064 - accuracy: 0.9979\n",
      "8334/8334 [==============================] - 3s 384us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 534us/step - loss: 0.5819 - accuracy: 0.6756\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 461us/step - loss: 0.2671 - accuracy: 0.8967\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 464us/step - loss: 0.1341 - accuracy: 0.9536\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 463us/step - loss: 0.0617 - accuracy: 0.9803\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 467us/step - loss: 0.0295 - accuracy: 0.9911\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 464us/step - loss: 0.0156 - accuracy: 0.9954\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 466us/step - loss: 0.0105 - accuracy: 0.9963\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 464us/step - loss: 0.0073 - accuracy: 0.9975\n",
      "8333/8333 [==============================] - 3s 387us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 533us/step - loss: 0.5738 - accuracy: 0.6851\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 465us/step - loss: 0.2629 - accuracy: 0.8964\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 462us/step - loss: 0.1333 - accuracy: 0.9540\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 466us/step - loss: 0.0629 - accuracy: 0.9809\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 463us/step - loss: 0.0326 - accuracy: 0.9903\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 465us/step - loss: 0.0159 - accuracy: 0.9956\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 464us/step - loss: 0.0123 - accuracy: 0.9964\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 466us/step - loss: 0.0087 - accuracy: 0.9974\n",
      "8333/8333 [==============================] - 3s 390us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 10s 577us/step - loss: 0.5526 - accuracy: 0.6982\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 8s 501us/step - loss: 0.2083 - accuracy: 0.9244\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 8s 499us/step - loss: 0.0627 - accuracy: 0.9837\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 8s 501us/step - loss: 0.0244 - accuracy: 0.9950\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 8s 499us/step - loss: 0.0119 - accuracy: 0.9982\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 8s 502us/step - loss: 0.0082 - accuracy: 0.9984\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 8s 499us/step - loss: 0.0074 - accuracy: 0.9981\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 8s 503us/step - loss: 0.0069 - accuracy: 0.9983\n",
      "8334/8334 [==============================] - 3s 405us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 10s 574us/step - loss: 0.5638 - accuracy: 0.6784\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 496us/step - loss: 0.2172 - accuracy: 0.9189\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 499us/step - loss: 0.0650 - accuracy: 0.9836\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 496us/step - loss: 0.0251 - accuracy: 0.9939\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 498us/step - loss: 0.0136 - accuracy: 0.9974\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 498us/step - loss: 0.0092 - accuracy: 0.9981\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 498us/step - loss: 0.0056 - accuracy: 0.9993\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 9s 525us/step - loss: 0.0049 - accuracy: 0.9991\n",
      "8333/8333 [==============================] - 3s 417us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 10s 588us/step - loss: 0.5625 - accuracy: 0.6816\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 9s 510us/step - loss: 0.2100 - accuracy: 0.9220\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 501us/step - loss: 0.0662 - accuracy: 0.9813\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 499us/step - loss: 0.0247 - accuracy: 0.9941\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 502us/step - loss: 0.0136 - accuracy: 0.9975\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 500us/step - loss: 0.0084 - accuracy: 0.9983\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 505us/step - loss: 0.0068 - accuracy: 0.9985\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 502us/step - loss: 0.0062 - accuracy: 0.9983\n",
      "8333/8333 [==============================] - 4s 421us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 9s 551us/step - loss: 0.5660 - accuracy: 0.6879\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 8s 472us/step - loss: 0.2515 - accuracy: 0.9020\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 8s 474us/step - loss: 0.1215 - accuracy: 0.9579\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 8s 473us/step - loss: 0.0568 - accuracy: 0.9812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 8s 476us/step - loss: 0.0285 - accuracy: 0.9906\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 8s 474us/step - loss: 0.0160 - accuracy: 0.9949\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 8s 477us/step - loss: 0.0107 - accuracy: 0.9965\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 8s 472us/step - loss: 0.0097 - accuracy: 0.9966\n",
      "8334/8334 [==============================] - 3s 419us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 551us/step - loss: 0.5684 - accuracy: 0.6906\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 481us/step - loss: 0.2547 - accuracy: 0.8991\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 476us/step - loss: 0.1245 - accuracy: 0.9570\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 9s 546us/step - loss: 0.0598 - accuracy: 0.9800\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 478us/step - loss: 0.0278 - accuracy: 0.9909\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 481us/step - loss: 0.0168 - accuracy: 0.9951\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 480us/step - loss: 0.0116 - accuracy: 0.9963\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 482us/step - loss: 0.0079 - accuracy: 0.9973\n",
      "8333/8333 [==============================] - 4s 421us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 551us/step - loss: 0.5715 - accuracy: 0.6875\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 480us/step - loss: 0.2560 - accuracy: 0.9005\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 478us/step - loss: 0.1223 - accuracy: 0.9591\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 482us/step - loss: 0.0576 - accuracy: 0.9809\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 479us/step - loss: 0.0288 - accuracy: 0.9911\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 483us/step - loss: 0.0168 - accuracy: 0.9942\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 481us/step - loss: 0.0122 - accuracy: 0.9960\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 482us/step - loss: 0.0073 - accuracy: 0.9977\n",
      "8333/8333 [==============================] - 4s 422us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 8s 486us/step - loss: 0.6388 - accuracy: 0.6244\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 7s 403us/step - loss: 0.3265 - accuracy: 0.8796\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 7s 407us/step - loss: 0.1674 - accuracy: 0.9529\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 7s 406us/step - loss: 0.0875 - accuracy: 0.9809\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 7s 405us/step - loss: 0.0453 - accuracy: 0.9941\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 7s 406us/step - loss: 0.0294 - accuracy: 0.9960\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 7s 403us/step - loss: 0.0184 - accuracy: 0.9982\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 7s 408us/step - loss: 0.0139 - accuracy: 0.9984\n",
      "8334/8334 [==============================] - 3s 382us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 8s 490us/step - loss: 0.6413 - accuracy: 0.6219\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 7s 432us/step - loss: 0.3278 - accuracy: 0.8787\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 7s 422us/step - loss: 0.1720 - accuracy: 0.9494\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 17s 1ms/step - loss: 0.0901 - accuracy: 0.9812\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0465 - accuracy: 0.9932\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.0301 - accuracy: 0.9962\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.0193 - accuracy: 0.9974\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.0153 - accuracy: 0.9982\n",
      "8333/8333 [==============================] - 9s 1ms/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 24s 1ms/step - loss: 0.6521 - accuracy: 0.6140\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.3477 - accuracy: 0.8734\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.1813 - accuracy: 0.9432\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.0981 - accuracy: 0.9777\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0502 - accuracy: 0.9923\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.0307 - accuracy: 0.9962\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0213 - accuracy: 0.9976\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0146 - accuracy: 0.9983\n",
      "8333/8333 [==============================] - 9s 1ms/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 23s 1ms/step - loss: 0.6229 - accuracy: 0.6486\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 20s 1ms/step - loss: 0.3493 - accuracy: 0.8682\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 20s 1ms/step - loss: 0.2258 - accuracy: 0.9215\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 20s 1ms/step - loss: 0.1444 - accuracy: 0.9533\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 20s 1ms/step - loss: 0.0893 - accuracy: 0.9762\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 20s 1ms/step - loss: 0.0556 - accuracy: 0.9859\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 20s 1ms/step - loss: 0.0349 - accuracy: 0.9911\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 20s 1ms/step - loss: 0.0196 - accuracy: 0.9954\n",
      "8334/8334 [==============================] - 9s 1ms/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 23s 1ms/step - loss: 0.6427 - accuracy: 0.6335\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.3782 - accuracy: 0.8579\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.2377 - accuracy: 0.9154\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.1624 - accuracy: 0.9467\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.1014 - accuracy: 0.9716\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.0680 - accuracy: 0.9823\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.0398 - accuracy: 0.9901\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 19s 1ms/step - loss: 0.0233 - accuracy: 0.9941\n",
      "8333/8333 [==============================] - 9s 1ms/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 23s 1ms/step - loss: 0.6235 - accuracy: 0.6484\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.3406 - accuracy: 0.8737\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.2047 - accuracy: 0.9301\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.1264 - accuracy: 0.9610\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0757 - accuracy: 0.9797\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.0437 - accuracy: 0.9894\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.0289 - accuracy: 0.9939\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 20s 1ms/step - loss: 0.0173 - accuracy: 0.9956\n",
      "8333/8333 [==============================] - 9s 1ms/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 25s 1ms/step - loss: 0.6181 - accuracy: 0.6424\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 21s 1ms/step - loss: 0.2838 - accuracy: 0.8949\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 21s 1ms/step - loss: 0.1281 - accuracy: 0.9636\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 22s 1ms/step - loss: 0.0546 - accuracy: 0.9890\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 21s 1ms/step - loss: 0.0259 - accuracy: 0.9966\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 21s 1ms/step - loss: 0.0156 - accuracy: 0.9984\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 21s 1ms/step - loss: 0.0097 - accuracy: 0.9995\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 21s 1ms/step - loss: 0.0065 - accuracy: 0.9999\n",
      "8334/8334 [==============================] - 10s 1ms/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 24s 1ms/step - loss: 0.6312 - accuracy: 0.6245\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.3005 - accuracy: 0.8886\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.1356 - accuracy: 0.9613\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0560 - accuracy: 0.9894\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0292 - accuracy: 0.9959\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0169 - accuracy: 0.9981\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0104 - accuracy: 0.9993\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0076 - accuracy: 0.9995\n",
      "8333/8333 [==============================] - 10s 1ms/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 25s 1ms/step - loss: 0.6472 - accuracy: 0.6054\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.3110 - accuracy: 0.8840\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.1377 - accuracy: 0.9581\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0592 - accuracy: 0.9885\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0286 - accuracy: 0.9962\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0166 - accuracy: 0.9976\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0113 - accuracy: 0.9987\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0069 - accuracy: 0.9995\n",
      "8333/8333 [==============================] - 10s 1ms/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 24s 1ms/step - loss: 0.6143 - accuracy: 0.6515\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 21s 1ms/step - loss: 0.3222 - accuracy: 0.8745\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 21s 1ms/step - loss: 0.1904 - accuracy: 0.9326\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 20s 1ms/step - loss: 0.1108 - accuracy: 0.9650\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 21s 1ms/step - loss: 0.0579 - accuracy: 0.9833\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 23s 1ms/step - loss: 0.0315 - accuracy: 0.9917\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 21s 1ms/step - loss: 0.0165 - accuracy: 0.9961\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 20s 1ms/step - loss: 0.0095 - accuracy: 0.9975\n",
      "8334/8334 [==============================] - 10s 1ms/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 24s 1ms/step - loss: 0.6244 - accuracy: 0.6461\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.3295 - accuracy: 0.8742\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.1951 - accuracy: 0.9308\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.1107 - accuracy: 0.9652\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0616 - accuracy: 0.9825\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0309 - accuracy: 0.9923\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0170 - accuracy: 0.9959\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0098 - accuracy: 0.9977\n",
      "8333/8333 [==============================] - 10s 1ms/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 25s 1ms/step - loss: 0.6219 - accuracy: 0.6497\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.3279 - accuracy: 0.8740\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.1909 - accuracy: 0.9363\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.1118 - accuracy: 0.9661\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0571 - accuracy: 0.9852\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 22s 1ms/step - loss: 0.0318 - accuracy: 0.9917\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 21s 1ms/step - loss: 0.0165 - accuracy: 0.9962\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 22s 1ms/step - loss: 0.0104 - accuracy: 0.9977\n",
      "8333/8333 [==============================] - 10s 1ms/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 10s 616us/step - loss: 0.6103 - accuracy: 0.6531\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 7s 433us/step - loss: 0.2779 - accuracy: 0.8902\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 7s 430us/step - loss: 0.1153 - accuracy: 0.9671\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 7s 432us/step - loss: 0.0455 - accuracy: 0.9902\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 7s 435us/step - loss: 0.0221 - accuracy: 0.9971\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 7s 431us/step - loss: 0.0120 - accuracy: 0.9989\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 7s 434us/step - loss: 0.0078 - accuracy: 0.9993\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 7s 431us/step - loss: 0.0053 - accuracy: 0.9997\n",
      "8334/8334 [==============================] - 4s 446us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 523us/step - loss: 0.6229 - accuracy: 0.6360\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 7s 430us/step - loss: 0.2726 - accuracy: 0.8984\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 7s 432us/step - loss: 0.1130 - accuracy: 0.9677\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 7s 431us/step - loss: 0.0446 - accuracy: 0.9915\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 431us/step - loss: 0.0227 - accuracy: 0.9960\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 7s 434us/step - loss: 0.0132 - accuracy: 0.9984\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 431us/step - loss: 0.0083 - accuracy: 0.9992\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 7s 441us/step - loss: 0.0066 - accuracy: 0.9991\n",
      "8333/8333 [==============================] - 4s 464us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 525us/step - loss: 0.6247 - accuracy: 0.6354\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 7s 434us/step - loss: 0.2815 - accuracy: 0.8934\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 7s 433us/step - loss: 0.1128 - accuracy: 0.9686\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 7s 432us/step - loss: 0.0446 - accuracy: 0.9908\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 435us/step - loss: 0.0221 - accuracy: 0.9965\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 7s 433us/step - loss: 0.0131 - accuracy: 0.9983\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 435us/step - loss: 0.0082 - accuracy: 0.9991\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 7s 433us/step - loss: 0.0050 - accuracy: 0.9997\n",
      "8333/8333 [==============================] - 4s 453us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 9s 518us/step - loss: 0.6144 - accuracy: 0.6497\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 7s 424us/step - loss: 0.3121 - accuracy: 0.8779\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 7s 424us/step - loss: 0.1748 - accuracy: 0.9368\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 7s 427us/step - loss: 0.0927 - accuracy: 0.9709\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 7s 425us/step - loss: 0.0466 - accuracy: 0.9869\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 7s 427us/step - loss: 0.0239 - accuracy: 0.9935\n",
      "Epoch 7/8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16666/16666 [==============================] - 7s 420us/step - loss: 0.0133 - accuracy: 0.9962\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 7s 420us/step - loss: 0.0071 - accuracy: 0.9983\n",
      "8334/8334 [==============================] - 4s 455us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 518us/step - loss: 0.5977 - accuracy: 0.6677\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 7s 418us/step - loss: 0.2992 - accuracy: 0.8827\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 7s 420us/step - loss: 0.1670 - accuracy: 0.9409\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 7s 420us/step - loss: 0.0891 - accuracy: 0.9742\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 420us/step - loss: 0.0423 - accuracy: 0.9890\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 7s 417us/step - loss: 0.0228 - accuracy: 0.9935\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 417us/step - loss: 0.0125 - accuracy: 0.9963\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 7s 422us/step - loss: 0.0073 - accuracy: 0.9984\n",
      "8333/8333 [==============================] - 4s 453us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 513us/step - loss: 0.6004 - accuracy: 0.6659\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 7s 423us/step - loss: 0.2946 - accuracy: 0.8858\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 7s 423us/step - loss: 0.1584 - accuracy: 0.9451\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 7s 437us/step - loss: 0.0822 - accuracy: 0.9741\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 432us/step - loss: 0.0395 - accuracy: 0.9888\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 7s 426us/step - loss: 0.0245 - accuracy: 0.9933\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 426us/step - loss: 0.0132 - accuracy: 0.9965\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 7s 424us/step - loss: 0.0093 - accuracy: 0.9974\n",
      "8333/8333 [==============================] - 4s 472us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 9s 550us/step - loss: 0.6107 - accuracy: 0.6470\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 7s 444us/step - loss: 0.2645 - accuracy: 0.8991\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 7s 446us/step - loss: 0.0979 - accuracy: 0.9743\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 7s 444us/step - loss: 0.0373 - accuracy: 0.9931\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 7s 447us/step - loss: 0.0176 - accuracy: 0.9978\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 7s 445us/step - loss: 0.0098 - accuracy: 0.9990\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 7s 445us/step - loss: 0.0066 - accuracy: 0.9995\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 7s 446us/step - loss: 0.0044 - accuracy: 0.9998\n",
      "8334/8334 [==============================] - 4s 480us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 550us/step - loss: 0.6179 - accuracy: 0.6450\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 452us/step - loss: 0.2683 - accuracy: 0.8987\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 451us/step - loss: 0.1065 - accuracy: 0.9687\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 453us/step - loss: 0.0412 - accuracy: 0.9908\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 449us/step - loss: 0.0198 - accuracy: 0.9969\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 453us/step - loss: 0.0112 - accuracy: 0.9983\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 449us/step - loss: 0.0063 - accuracy: 0.9997\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 453us/step - loss: 0.0051 - accuracy: 0.9995\n",
      "8333/8333 [==============================] - 4s 476us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 557us/step - loss: 0.6153 - accuracy: 0.6477\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 8s 453us/step - loss: 0.2613 - accuracy: 0.9028\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 8s 455us/step - loss: 0.0989 - accuracy: 0.9727\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 8s 451us/step - loss: 0.0391 - accuracy: 0.9919\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 8s 456us/step - loss: 0.0196 - accuracy: 0.9969\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 8s 451us/step - loss: 0.0109 - accuracy: 0.9978\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 8s 454us/step - loss: 0.0076 - accuracy: 0.9990\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 8s 453us/step - loss: 0.0049 - accuracy: 0.9995\n",
      "8333/8333 [==============================] - 4s 494us/step\n",
      "Epoch 1/8\n",
      "16666/16666 [==============================] - 9s 539us/step - loss: 0.6020 - accuracy: 0.6637\n",
      "Epoch 2/8\n",
      "16666/16666 [==============================] - 7s 438us/step - loss: 0.2927 - accuracy: 0.8832\n",
      "Epoch 3/8\n",
      "16666/16666 [==============================] - 7s 441us/step - loss: 0.1599 - accuracy: 0.9431\n",
      "Epoch 4/8\n",
      "16666/16666 [==============================] - 7s 436us/step - loss: 0.0763 - accuracy: 0.9776\n",
      "Epoch 5/8\n",
      "16666/16666 [==============================] - 7s 439us/step - loss: 0.0384 - accuracy: 0.9898\n",
      "Epoch 6/8\n",
      "16666/16666 [==============================] - 7s 440us/step - loss: 0.0207 - accuracy: 0.9933\n",
      "Epoch 7/8\n",
      "16666/16666 [==============================] - 7s 436us/step - loss: 0.0108 - accuracy: 0.9971\n",
      "Epoch 8/8\n",
      "16666/16666 [==============================] - 7s 440us/step - loss: 0.0061 - accuracy: 0.9980\n",
      "8334/8334 [==============================] - 4s 493us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 544us/step - loss: 0.6138 - accuracy: 0.6482\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 7s 443us/step - loss: 0.3042 - accuracy: 0.8798\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 7s 443us/step - loss: 0.1575 - accuracy: 0.9435\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 7s 443us/step - loss: 0.0758 - accuracy: 0.9762\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 442us/step - loss: 0.0380 - accuracy: 0.9888\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 7s 443us/step - loss: 0.0166 - accuracy: 0.9959\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 441us/step - loss: 0.0116 - accuracy: 0.9965\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 7s 440us/step - loss: 0.0068 - accuracy: 0.9982\n",
      "8333/8333 [==============================] - 4s 492us/step\n",
      "Epoch 1/8\n",
      "16667/16667 [==============================] - 9s 560us/step - loss: 0.6137 - accuracy: 0.6545\n",
      "Epoch 2/8\n",
      "16667/16667 [==============================] - 7s 442us/step - loss: 0.2999 - accuracy: 0.8819\n",
      "Epoch 3/8\n",
      "16667/16667 [==============================] - 7s 446us/step - loss: 0.1608 - accuracy: 0.9426\n",
      "Epoch 4/8\n",
      "16667/16667 [==============================] - 7s 440us/step - loss: 0.0806 - accuracy: 0.9746\n",
      "Epoch 5/8\n",
      "16667/16667 [==============================] - 7s 444us/step - loss: 0.0383 - accuracy: 0.9881\n",
      "Epoch 6/8\n",
      "16667/16667 [==============================] - 7s 442us/step - loss: 0.0216 - accuracy: 0.9935\n",
      "Epoch 7/8\n",
      "16667/16667 [==============================] - 7s 445us/step - loss: 0.0116 - accuracy: 0.9965\n",
      "Epoch 8/8\n",
      "16667/16667 [==============================] - 7s 443us/step - loss: 0.0082 - accuracy: 0.9975\n",
      "8333/8333 [==============================] - 4s 491us/step\n",
      "Epoch 1/8\n",
      "25000/25000 [==============================] - 8s 308us/step - loss: 0.6242 - accuracy: 0.6412\n",
      "Epoch 2/8\n",
      "25000/25000 [==============================] - 6s 238us/step - loss: 0.3266 - accuracy: 0.87365s - l - ETA: 4s - loss: 0.3\n",
      "Epoch 3/8\n",
      "25000/25000 [==============================] - 6s 239us/step - loss: 0.2094 - accuracy: 0.9276TA: 3s - loss: 0.2148 - accura - ETA: 3s - loss: 0.2137 - accuracy: 0.92 - ETA: 3s - loss: 0.2148  - ETA - ETA: 1s - loss: 0.2116 - accuracy - ETA: 1s - loss: 0.2121 - ac - ETA: 0s -\n",
      "Epoch 4/8\n",
      "25000/25000 [==============================] - 6s 239us/step - loss: 0.1377 - accuracy: 0.95713s - loss: 0.138 - ETA: 3s - ETA: 2s -\n",
      "Epoch 5/8\n",
      "25000/25000 [==============================] - 6s 230us/step - loss: 0.0894 - accuracy: 0.9742\n",
      "Epoch 6/8\n",
      "25000/25000 [==============================] - 6s 234us/step - loss: 0.0638 - accuracy: 0.98303s - loss: 0.0655 - accuracy: 0.98 - ETA: 3s - loss: 0.065\n",
      "Epoch 7/8\n",
      "25000/25000 [==============================] - 6s 236us/step - loss: 0.0432 - accuracy: 0.9895\n",
      "Epoch 8/8\n",
      "25000/25000 [==============================] - 6s 234us/step - loss: 0.0345 - accuracy: 0.9904\n"
     ]
    }
   ],
   "source": [
    "#Fitting to dataset\n",
    "grid_search = grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EMBEDDING_DIM': 30, 'batch_size': 125, 'neurons': 5, 'optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "#print best hyperparameter based on above run\n",
    "best_parameters = grid_search.best_params_\n",
    "print(best_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.87368\n"
     ]
    }
   ],
   "source": [
    "#print best accuracy\n",
    "best_accuracy = grid_search.best_score_\n",
    "print(best_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final model \n",
    "This model is bulit after getting optimal hparameter from grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model buliding\n",
    "model = Sequential() #Sequential model\n",
    "\n",
    "model.add(Embedding(MAX_VOCAB, 30, input_length=SEQ_LEN))  #Init embedding layer with no pretrained wts\n",
    "#embedding layer takes input of vocabulary size i.e 10000, embedding dimension i.e 50 and input sequence i.e number of columns of dataset i.e 300\n",
    "model.add(Flatten()) #Use flatten layer\n",
    "model.add(Dropout(0.5)) #use dropout for regularization\n",
    "model.add(Dense(5)) #Hidden layer\n",
    "model.add(Dropout(0.3)) #use dropout for regularization\n",
    "model.add(Dense(1, activation='sigmoid')) #Output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TxNDNhrseCzA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 300, 30)           300000    \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 9000)              0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 9000)              0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 5)                 45005     \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 6         \n",
      "=================================================================\n",
      "Total params: 345,011\n",
      "Trainable params: 345,011\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#To see what our model have, number of layer , output shape ,etc\n",
    "#model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L3CSVVPPeCzD"
   },
   "outputs": [],
   "source": [
    "#Compile model with optimizer adam, loss as binary cross entropy and metric is accuracy\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 300)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross check step to see shape of input to our model\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define early stop and reduced lr\n",
    "stop = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=1e-6, verbose=1, mode=\"min\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 4s 160us/step - loss: 0.6175 - accuracy: 0.6468 - val_loss: 0.4197 - val_accuracy: 0.8282\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 0.3155 - accuracy: 0.8812 - val_loss: 0.3153 - val_accuracy: 0.8675\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 4s 161us/step - loss: 0.2062 - accuracy: 0.9272 - val_loss: 0.3119 - val_accuracy: 0.8716\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 4s 155us/step - loss: 0.1387 - accuracy: 0.9565 - val_loss: 0.3361 - val_accuracy: 0.8670\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 4s 156us/step - loss: 0.0941 - accuracy: 0.9727 - val_loss: 0.3674 - val_accuracy: 0.8657\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 4s 158us/step - loss: 0.0629 - accuracy: 0.9826 - val_loss: 0.4043 - val_accuracy: 0.8644\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 4s 156us/step - loss: 0.0460 - accuracy: 0.9880 - val_loss: 0.4493 - val_accuracy: 0.8597\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 0.0383 - accuracy: 0.9894 - val_loss: 0.4808 - val_accuracy: 0.8620\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 0.0278 - accuracy: 0.9925 - val_loss: 0.5205 - val_accuracy: 0.8589\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 4s 153us/step - loss: 0.0226 - accuracy: 0.9939 - val_loss: 0.5578 - val_accuracy: 0.8586\n"
     ]
    }
   ],
   "source": [
    "#Fitting model to xtrain and ytrain with defined epochs and batch size\n",
    "#Here I have used requlaization and model performance tech such as reduced lr and early stop\n",
    "r = model.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  batch_size=125,\n",
    "  epochs=EPOCHS,\n",
    " # callbacks=[reduce_lr, stop],\n",
    "  validation_data=(X_test,y_test)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 30us/step\n",
      "Test accuracy:  0.8585600256919861\n"
     ]
    }
   ],
   "source": [
    "#Evaluate test set and then print accuracy\n",
    "results = model.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 30us/step\n",
      "Test accuracy:  0.9999200105667114\n"
     ]
    }
   ],
   "source": [
    "#Evaluate train set and then print accuracy\n",
    "results = model.evaluate(X_train, y_train)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting prediction of test set and train set\n",
    "prediction_train = model.predict(X_train)\n",
    "prediction_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00270584]\n",
      " [1.        ]\n",
      " [0.9998786 ]\n",
      " ...\n",
      " [0.00501606]\n",
      " [0.41706505]\n",
      " [0.5904646 ]]\n"
     ]
    }
   ],
   "source": [
    "#print prediction\n",
    "print(prediction_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set threshold\n",
    "#we want the value to be either 0 or 1\n",
    "pred=np.zeros((25000,))\n",
    "count=0\n",
    "for i in prediction_test:\n",
    "    if i>0.5:  #Got .5 after many hit and try\n",
    "        pred[count]=1\n",
    "    \n",
    "    else:\n",
    "        pred[count]=0\n",
    "    \n",
    "    count+=1\n",
    "\n",
    "#Threshold is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 1.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print value after threshold\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.8586\n",
      "F1 score : 0.8548\n",
      "precision score : 0.8784\n",
      "recall score : 0.8324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86     12500\n",
      "           1       0.88      0.83      0.85     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print accuracy score, f1 score, precision, recall and classification report\n",
    "print(\"accuracy : {0:.4f}\".format(accuracy_score(y_test,pred)))\n",
    "print(\"F1 score : {0:.4f}\".format(f1_score(y_test,pred)))\n",
    "print(\"precision score : {0:.4f}\".format(precision_score(y_test,pred)))\n",
    "print(\"recall score : {0:.4f}\".format(recall_score(y_test,pred)))\n",
    "print(metrics.classification_report(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.41967814072966575, 0.31534422516822813, 0.31188236258924007, 0.3360690650343895, 0.3673982647806406, 0.40433042369782923, 0.44931577399373057, 0.4807850980758667, 0.5205187710374594, 0.5577785823494196], 'val_accuracy': [0.8281999826431274, 0.8674799799919128, 0.8715599775314331, 0.8670399785041809, 0.8656799793243408, 0.8644400238990784, 0.8596799969673157, 0.8620399832725525, 0.8589199781417847, 0.8585600256919861], 'loss': [0.6174829252064228, 0.3154584526270628, 0.20619698014110327, 0.1386769899353385, 0.09410732259973884, 0.0628730616811663, 0.04597682327032089, 0.03829919936135411, 0.027760785922873767, 0.022568860789760947], 'accuracy': [0.6468, 0.88124, 0.9272, 0.95652, 0.97268, 0.98264, 0.988, 0.98936, 0.99252, 0.99392]}\n"
     ]
    }
   ],
   "source": [
    "#To see what out model stores info so that these can be plotted\n",
    "print(r.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8GearUAAAgAElEQVR4nO3deXxU9bnH8c8zk8m+EEI2EiABQtgCogFEBAEBkSC4C26IW627Xq21Vtta21rt1fbeWluviqIg4FJFglC1VFwQCFvCDoYtCyFsCRCy/+4fJ2KIAQZIOLM879fLVzJnzpw8jPCdX55zzu8nxhiUUkp5P4fdBSillGoZGuhKKeUjNNCVUspHaKArpZSP0EBXSikfEWDXD27Xrp1JSUmx68crpZRXWr58+R5jTGxzz9kW6CkpKeTk5Nj145VSyiuJyPbjPactF6WU8hEa6Eop5SM00JVSykdooCullI/QQFdKKR+hga6UUj5CA10ppXyE1wX6yh37+eP8DXaXoZRSHsfrAn1NYRkv/+c7Nu46aHcpSinlUbwu0Mf0TsQhkJ1bZHcpSinlUbwu0GMjgji/cwxz84rR1ZaUUuoHbgW6iIwRkY0iskVEfn6cfa4VkXUislZEZrRsmcfK6pNIfulh1hdr20Uppb530kAXESfwEnAp0BOYJCI9m+yTBjwODDbG9AIebIVajxrTK8Fqu+Rp20Uppb7nzgh9ALDFGJNvjKkGZgITmuxzB/CSMWY/gDFmd8uWeayY8CAu6NKO7Fxtuyil1PfcCfQkYGejxwUN2xrrBnQTka9F5FsRGdPcgUTkThHJEZGc0tLS06u4wbg+iWzbW8HaovIzOo5SSvkKdwJdmtnWdFgcAKQBw4BJwKsi0uZHLzLmFWNMpjEmMza22fnZ3XZJrwScDmFubvEZHUcppXyFO4FeAHRo9DgZaNq8LgA+MsbUGGO2AhuxAr7VRIcFMrhrO7LzirTtopRSuBfoy4A0EUkVkUBgIjCnyT4fAsMBRKQdVgsmvyULbc64Pons3HeEvMKy1v5RSinl8U4a6MaYWuBeYAGwHphtjFkrIk+LyPiG3RYAe0VkHbAQeNQYs7e1iv7eJT0TcDmFbG27KKUUYle7IjMz07TEmqJTpi5lU8khvnpsOCLNtfuVUsp3iMhyY0xmc8953Z2iTWX1aU/hgSOs2nnA7lKUUspWXh/oo3rGE+h0aNtFKeX3vD7Qo0JcDO3Wjnl5xdTX69UuSin/5fWBDtbcLkVllazcud/uUpRSyjY+Eegje8QTGODQm4yUUn7NJwI9ItjFsG6x2nZRSvk1nwh0sNouJeVV5GzXtotSyj/5TKBf3COeoACHrmSklPJMdTWwcT68ewvs+LZVfoTPBHp4UAAjuscxb80u6rTtopTyBMZA0Sr45Ofw393hnetg6yIoK2iVHxfQKke1SVafRD5Zs4tl2/ZxfucYu8tRSvmrskLImw2rZ0LpBnAGQvql0HcSdB0JTler/FifCvQR3eMIdjmYm1ukga6UOruqDsH6j2H1O9YoHAMdzodxf4Zel0NIdKuX4FOBHhoYwMXd45m/Zhe/vqwXAU6f6SgppTxRfR1s/QJWz4L1c6CmAqJTYNjPoc+10LbzWS3HpwIdrCl1s/OKWbp1Hxd0bWd3OUopX1SyDnJnQu5sOFgMwVFWgPedBB0Ggk0TBfpcoA9LjyM00MncvGINdKVUyzm0G/Les1oqu3LBEQBdR8GYZ6HbGHAF212h7wV6SKCTi3tYbZenx2vbRSl1BmqOwMZ51snNLZ+DqYP2/eDS56D3VRDmWYNGnwt0gKyMRD5eXcTi/L0MSTuztUuVUn6mvh52LLZG4us+gqpyiEyCwQ9A34kQm253hcflk4E+LD2WsEAn2bnFGuhKKffs/c4aiefOhAM7IDAceoy3QjxlCDg8/7d9nwz0YJeTUT3jmb92F7+9vDcubbsopZpTsQ/WfmAFecEyEAd0HgYjnoTuWRAYZneFp8QnAx2slYw+XFXE11v2MCw9zu5ylFKeorYaNv/LaqlsWgD1NRDXE0b9FjKugchEuys8bT4b6EO7tSMiKIDs3GINdKX8nTFQuMIK8TXvwZH9EBYHA+60WioJGbZdatiSfDbQgwKcjOoVz4K1u/jdFRkEBmjbRSm/U7EPcmfBirdg91oICLZaKX0nQefh4PStCPStP00T4/ok8sGKQr7esofh3XWUrpRfqK+HbYtgxTTrVvy6amh/rnULfu8rrZuAfJRPB/qFXWOJDA7g49wiDXSlfF15Eayabo3GD2yH4DZw3hQ49yarpeIHfDrQAwMcXNIrgflrdlFVW0dQgNPukpRSLamuxjrBuWKa9dXUQ+pQuPgp6D7OI+7ePJvcCnQRGQP8BXACrxpjnm3y/C3A80Bhw6a/GmNebcE6T1tWn0TeXV7Al5v2MLJnvN3lKKVawt7vYOVbsGoGHCqB8AS48CHod+NZnxDLk5w00EXECbwEjAIKgGUiMscYs67JrrOMMfe2Qo1nZHDXdkSFuJibW6SBrpQ3qzli9cRXTINtX4I4odslcO7N1pwqPnaC83S48w4MALYYY/IBRGQmMAFoGugeyeV0MKZXAnNzi6isqSPYpW0XpbzKrjwrxHNnQWUZRKdaLZW+13v1NeOtwZ1ATwJ2NnpcAAxsZr+rRGQosAl4yBizs+kOInIncCdAx44dT73a0zSubyKzcnbyxaZSLumVcNZ+rlLqNFWWWTMbrnwLilaCMwh6jrdG450u9Irb8O3gTqA3d7V900U7PwbeMcZUichdwJvAiB+9yJhXgFcAMjMzz9rCn4M6xxAd6iI7t1gDXSlPZYy1ePKKabD2n1B7BOJ7w6XPQ8bVENrW7go9njuBXgB0aPQ4GShqvIMxZm+jh/8H/PHMS2s5AU4HY3on8tGqQo5U1xESqG0XpTzGoVLrDs4V02DvZgiMsO7ePPdma6paH7iD82xxJ9CXAWkikop1FctE4PrGO4hIojGmuOHheGB9i1bZAi7rk8g7S3fwn427uTRD+25K2aq+Dr5bCCvetOYbr6+11t+88CFr/U0vmxTLU5w00I0xtSJyL7AA67LF140xa0XkaSDHGDMHuF9ExgO1wD7gllas+bQMSG1Lu/BA5uYVa6ArZZcDO2DldFj5NpQXQGgMDLzLGo178Dzj3sKt63yMMfOAeU22PdXo+8eBx1u2tJZltV0SeH95IRXVtYQG6iVOSp0VtdXWKHzFNPju39a2LiPgkt9B+lgICLS3Ph/iV6mWldGet7/dwb837GZcn/Z2l6OU76osg82fWkG++dOGVX+S4aLHoN8N0ObsXeXmT/wq0AektiU2Iojs3GINdKVaWlkBbPwENmTDtq+secbDYqHnBKsv3nk4OPSChNbkV4HudAhjeycwc9lODlfVEhbkV398pVqWMVCy1grwjdlQvNraHpMGg+6G9CxIztQQP4v8LtGy+rTnzcXb+XzDbsb31VG6UqekrhZ2fAMb5lkhfmAHIJDcH0b+2grx2G42F+m//C7QMztFEx8ZxNzVRRroSrmj6hB897k1Et+0ACoPWHdudhkOQx6B9EshXKen9gR+F+gOhzA2I5HpS3ZwsLKGiGCX3SUp5XkO7rL64RvnQf4XUFcFIdFWeKePta5SCQq3u0rVhN8FOlgrGU39ehufr9/N5f2S7C5HKfsZA6UbrTbKhnlQmGNtj06B/rdD97HWjT86o6FH88v/O/06RJMYFczc3CINdOW/6utg59KGEM+GffnW9vb9YPgvrbU343rorfdexC8D3eEQsjISmbZ4O2VHaogK0baL8hPVFZC/0BqFb5oPFXvA4YLUITDoHuh2KUTpIMdb+WWgg7WS0atfbeWzdSVcdV6y3eUo1XoO77HCe0O2NX9K7REIioK0UVYrpetIn1442Z/4baCf06ENSW1CyM4r1kBXvqfmiLUgxOqZsHOJtdZmZLK1YHL6WOg0WG+590F+G+giQlafRKZ+vZWyihqiQrXtonzAwRJY9n+Q8zpU7IXYHjD0USvEE/tqP9zH+W2gg3W1yyuL8lmwbhfXZnY4+QuU8lTFufDt36xVfuprrcsLz78bUi7UEPcjfh3oGUlRdGgbQnZusQa68j719bB5ASx+yVo02RUGmVOs6WhjuthdnbKBXwe6iJCV0Z5Xv8xn/+FqosO0p6i8QPVhWDUDvn0Z9n0HkUkw8jdw3mTr5h/lt/w60MFqu/z9i+9YsHYXEwfolJ7Kg5UVwtJXYPkb1u33SefBVa9Zsxk69RyQ0kCnV/tIUmJCyc4r1kBXnqlwOSz+G6z70LpapcdlcP490GGA9sfVMfw+0L+/2uXvX+Sz91AVMeFBdpeklHUX54a5VpDv/NZaOHnAT2Dgndbt+Eo1w+8DHayVjF5a+B0L1pZw/UAdpSsbVZZb620uedmamrZNR7jkD9DvRgiOtLs65eG8M9Dr61p00vweiRF0jg1jbm6RBrqyx/7tsOQf1rqb1QetibBGPwPdx+kCEcpt3hfo6z6Cr16EGz+A0LYtckgRYVxGIn9duIXSg1XERmjbRZ0FxliTYy3+q9VeQaDXFdZqP0nn2V2d8kIOuws4ZYHhULIO3rocjhxoscNm9WlPvYH5a3e12DGValZdjXUD0KsXw+ujYesXcMH98GAeXP2ahrk6bd4X6F0vhuvetkL97ausnmML6BYfTte4cOauLmqR4yn1I0f2w1d/hr/0hfdvswYkY/8ED6+HUb/RWQ7VGfO+QAfoNhqufROKV8H0a6wlss6QiDCuTyJLt+1jd3llCxSpVIO938G8R+GFXvDZr6BtZ5g0E+7NgQF3QGCY3RUqH+FWoIvIGBHZKCJbROTnJ9jvahExIpLZciUeR/cs66aKgmUw4zprnuczlJWRiDHwyRptu6gzZAxs/RLemQT/ex7kTLVuAPrJl3DLXGuuFYd3jqeU5zrpSVERcQIvAaOAAmCZiMwxxqxrsl8EcD+wpDUKbVavy61+5D/vhJmTYNIscAWf9uHS4iNIj48gO7eYyRektFydyn9U7IM171tXq+zKhZC2MPQRaxm3iAS7q1M+zp2rXAYAW4wx+QAiMhOYAKxrst9vgeeAR1q0wpPpcw3U18CHd8OsG2HidAg4/atUsvok8uJnm9hVVklC1Ol/OCg/UltlLSCxehZs/pf19zGuF1z2F+hzHbhC7K5Q+Ql3fudLAnY2elzQsO0oEekHdDDGzG3B2tx3zvXWP54tn8LsyVBbfdqHyupjtV3m5RW3YIHK5xgD27+Bjx+AP6XB7JuthZUH/sRqq/z0azjvFg1zdVa5M0JvbrIIc/RJEQfwInDLSQ8kcidwJ0DHji18A895k6GuGuY9Au/fCldPPa0Ji7rEhtMjMZLsvGJuvTC1ZWtU3m/PFsidaa0GdGAHuEKtuVX6XAupw8Dpfbd2KN/hzt++AqDxZOHJQONr+yKA3sB/xJooKAGYIyLjjTE5jQ9kjHkFeAUgMzPT0NIG3GH11Bc8Dv/8CVz5f6d1l924Pok8v2AjRQeO0L6NjrD83uG9Vl88d6Y1UZY4IPUiGP6EdSdnULjdFSoFuBfoy4A0EUkFCoGJwPXfP2mMKQPaff9YRP4DPNI0zM+aQXdbI/XPfmWtZn7530451LMyrECfl1fM7UM6t1KhyqPVVMKmT6y++JZPrVWA4nvDqN9CxjUQmWh3hUr9yEkD3RhTKyL3AgsAJ/C6MWatiDwN5Bhj5rR2kafswgetkfrCZ6y2y2X/c0qXiKW0C6N3UiRzczXQ/Up9Pez4xmqnrP0IqsogItFayq3PdZDQ2+4KlTohtxp+xph5wLwm2546zr7DzrysFnDRo9ZIfdFzVqhnvXBKc0dnZbTnj/M3sHNfBR3ahrZiocp2pZsa+uLvQtkOaym3nuOtEE8dqpNjKa/h22dwhv/CCvWv/wzOQBjzrNuhnpWRyB/nb+CTNcXcOVTXZ/Q5h0p/6IsXrbT64p2Hw8VPWjet6d2bygv5dqCLwMhfW6H+7d+skfqo37oV6h1jQumbHMXcXA10n1FzBDbOa+iLfwamDhIyYPTvIONqvfFHeT3fDnSwwvuS31uh/s3/WiP1EU+6FepZfRL5/bwN7NhbQccYbbt4pfp62P6V1RdfNweqyq1FlS+4z2qpxPe0u0KlWozvBzpY4X3p89aJ0i//G5xBMOyxk75sbIYV6Nl5xfx0mI7SvcruDT/0xcsLrGmXe06wQjzlQu2LK5/kH4EO1lUu4/5sXX72n99b7ZchD5/wJcnRoZzToQ3ZeUUa6N7gwA5Y/7E1Gi9eDeKELiOsqWnTx0Kg/palfJv/BDpYoT7+f632y+e/sdovF9x7wpeM65PIM9nr2bbnMCnt9ESZRzHGCu6N82DDPCjJs7YnnmOdAO99FYTH2VujUmeRfwU6WL9qX/53K9T/9YQV6gPvPO7uYzOsQM/OK+ae4V3PYqGqWbXVsO1LK8Q3fgLlhdYVKh3Ot054d8+CGP1tSvkn/wt0sObbuOo1qKuFTx612i+ZU5rdtX2bEM7rFM3cXA102xw5AJs/hY3ZsPkzaxFlV6jVThn+BHS7BMLanfw4Svk4/wx0sEL8mqnWlLtzH7Qe97ux2V3H9UnkNx+v47vSQ3SJ1Xk7zooDO6wR+IZs2P61de4jLA56XwHpWdD5Ip3JUKkm/DfQwZo3/dq34J2J8NG91twvfa/70W6X9k7k6bnryM4t5v6L02wo1A8crx/eLh0G3Wu1UpIydZUfpU7AvwMdrBWOJs6AGdfCh3dZI/XeVx6zS0JUMP07tdVAb2nH7YcP1H64UqdBAx2sy9munwVvXw3v326Feo/Ljtklq08iv5qzls0lB0mLj7CpUB9w5IB1l+aGbOtrVbn2w5VqIRro3wsMgxtmw1tXwLtT4Lq3IX3M0acvzUjg1x+vZW5uMQ+N0kA/Jc32w2OtG326Z0HnYdoPV6oFaKA3FhQBN7wHb10Os2+CSe9A15EAxEUEMzC1Ldl5xTw4Mg05hZkb/c5x++HdtB+uVCvSQG8qpA3c+AFMGw8zb7BaMZ2HAZDVpz1PfriGTSWHSE/QUfoxTtYPTx8L7fSyT6VakwZ6c0Lbwk0fwZvjYMZEuPF9SBnMmF4J/OqjNWTnFpGekG53lfaqrrAWRd6+2FoUYucyqDkMASEN/fBfQLcx2g9X6izSQD+esBi4+SN4IwumXwM3/ZPYjgMZ1CWmoY/ezb/aLhX7YMe3VnhvXwzFq6xeOALxveCc66HrxdoPV8pGGugnEh4Hkz+GqWNh+tVw04dkZbTnF//MY33xQXq2j7S7wtZzYCfsWAzbv7GCvHS9td0ZCO3Ptaaf7XgBdBhgtamUUrbTQD+ZiAQr1N8YC29fQda17/OkQ8jOK/KdQK+vhz0bGwJ8sfW1bKf1XGAEdBwIGVdZAZ50nnXtvlLK42iguyMq6ehIPerda7iuwx/Izi3mkdHp3tl2qauxrkLZ/o0V3ju+hSP7rOfC4qDTIOtqlE6DrJXude5wpbyCBrq72nSEyXNgahZP7f8F4w7+nLVF59I7Kcruyk6u6hAULGsI78VQkAM1FdZzbTtbV6B0GgQdB1mPvfFDSimlgX5K2naGyR8TOPVS3gn8HdnfxNE76yIICLb+85Trqg/vaTiB2dADL15trZ8pDmvEfe7N0PF8K8B1HU2lfIYYY2z5wZmZmSYnJ8eWn33GSjdS/vJoIusPHLvdGWQFuyv4h5B3BVuX8h2zLcSaGKzZ7U1eFxD0w/bmXucIsO7EPHoCczHs2fRDPUnnNYy+L4AO/SHYC36jUEodl4gsN8ZkNvecjtBPR2w6+25YwP+8/TrhzhpuG5hIREAt1FZCTSXUHoHaKmuV+dpK67/qCqjY27C9YZ+ahufqa06/FnGAqbe+D4qyTmD2nQSdLoD2/awPAKWUX9BAP00pXboz4bYnmPR/3zI3L5jZPxlE27DA0ztYXcOHQW3VsUFfW3nsh0JzHxY1RyAi0RqFx/XUE5hK+TG3Wi4iMgb4C+AEXjXGPNvk+buAe4A64BBwpzFm3YmO6dUtl0YWf7eXyVOX0iMhgul3nE94kH5GKqVaz4laLic9iyciTuAl4FKgJzBJRHo22W2GMSbDGHMO8BzwwhnW7DUGdYnhb9efy5qicu54M4fKmjq7S1JK+Sl3LssYAGwxxuQbY6qBmcCExjsYY8obPQwD7DnTapORPeP50zV9WJy/l/veWUltXb3dJSml/JA7gZ4E7Gz0uKBh2zFE5B4R+Q5rhH5/cwcSkTtFJEdEckpLS0+nXo91Rb9kfjO+F5+uK+Fn7+dSX+9Xn2lKKQ/gTqA3d5fJj9LKGPOSMaYL8Bjwy+YOZIx5xRiTaYzJjI2NPbVKvcDkC1J4eFQ3PlhRyNNz12HXJaFKKf/kzhm8AqBDo8fJQNEJ9p8JvHwmRXmz+0Z0pexIDa99tZU2oS4eHNnN7pKUUn7CnUBfBqSJSCpQCEwErm+8g4ikGWM2NzzMAjbjp0SEJ8b2oOxIDX/+bDORwS5uvTDV7rKUUn7gpIFujKkVkXuBBViXLb5ujFkrIk8DOcaYOcC9IjISqAH2A5Nbs2hP53AIz16ZwcHKGp6eu46oEBdXnZdsd1lKKR+nt/63osqaOm57cxnf5u/j5RvOZXQvnTdFKXVmzug6dHX6gl1O/nFTJr2Torh3xkq+2bLH7pKUUj5MA72VhQcF8MYt/UlpF8od03JYtfPAyV+klFKnQQP9LIgOC+St2wbSNjyQW6YuZXPJQbtLUkr5IA30syQ+Mpi3bxuIy+ngxteWsHNfhd0lKaV8jAb6WdQpJoy3bhtAZU09N762hN0HK+0uSSnlQzTQz7LuCZFMndKf3eVV3PzaUsoqzmAudKWUakQD3QbndozmlZvPI7/0MFPeWEpFda3dJSmlfIAGuk2GpMXyP5POYdXOA/zkreVU1eq0u0qpM6OBbqMxvRN59so+fLl5Dw/NWkWdztColDoDuryOza7t34HyyhqeyV5PRFAez16VgUhzE1wqpdSJaaB7gNuHdOZARQ1/XbiFqFAXj1/aXUNdKXXKNNA9xH+N7kbZkRpeWZRPm1AXdw/randJSikvo4HuIUSE34zvRXllDc/N30hksIsbz+9kd1lKKS+ige5BHA7hT9f05WBlLU9+tIbIEBfj+7a3uyyllJfQq1w8jMvp4G83nEv/lLY8PGsVCzfstrskpZSX0ED3QMEuJ69OzqR7YgR3vb2cpVv32V2SUsoLaKB7qMhgF29OGUBSdAi3vbGMNYVldpeklPJwGugeLCY8iLduG0hEcACTX19Kfukhu0tSSnkwDXQPl9QmhLduHwjAja8uoejAEZsrUkp5Kg10L9AlNpw3bx3AwcpabnxtCXsPVdldklLKA2mge4neSVG8OjmTwv1HmDx1KeWVOu2uUupYGuheZGDnGF6+8Vw2FB/k9jdzqKzRGRqVUj/QQPcyI7rH89/X9mXZtn3cPX0FNXX1dpeklPIQGuheaMI5STw9oTf/3rCbR95dTb1Ou6uUws1AF5ExIrJRRLaIyM+bef5hEVknIrki8rmI6CQkreym8zvx6CXpfLSqiAdmrdKl7JRSJw90EXECLwGXAj2BSSLSs8luK4FMY0wf4D3guZYuVP3Y3cO68MjobszLK2b0n7/g8/UldpeklLKROyP0AcAWY0y+MaYamAlMaLyDMWahMaai4eG3QHLLlqmaIyLcOyKND+8eTJuQQG57M4eHZ63iQEW13aUppWzgTqAnATsbPS5o2HY8twGfNPeEiNwpIjkiklNaWup+leqEMpKj+Pi+C7l/RFfmrC5i1IuL+HSdjtaV8jfuBHpzS+c0exZORG4EMoHnm3veGPOKMSbTGJMZGxvrfpXqpAIDHDw8Op0P7xlMu/Ag7piWw4MzV7L/sI7WlfIX7gR6AdCh0eNkoKjpTiIyEngCGG+M0VsZbdI7KYqP7hnMgyPTmJtbzKgXFzF/zS67y1JKnQXuBPoyIE1EUkUkEJgIzGm8g4j0A/6BFeY6gbfNAgMcPDiyGx/dO5i4iCDuens5972zkn06WlfKp5000I0xtcC9wAJgPTDbGLNWRJ4WkfENuz0PhAPvisgqEZlznMOps6hX+yg+uncwD4/qxvw1xYx64Qs+ySu2uyylVCsRY+y5KSUzM9Pk5OTY8rP90frich59bzVrCsvJ6pPI0+N7ERMeZHdZSqlTJCLLjTGZzT2nd4r6iR6Jkfzz7sE8ekk6/1q7i1EvLiI7V0frSvkSDXQ/4nI6uGd4V+beN4Tk6BDumbGCn769nNKDeg5bKV+gge6H0hMi+OCnF/CzMel8vn43o1/8gjmri7Cr/aaUahka6H4qwOng7mFdyb7/QjrGhHH/Oyu56+3l7D5YaXdpSqnTpIHu59LiI3j/rkE8fml3Fm4sZfSLi/hwZaGO1pXyQhroigCng59c1IV59w8htV0YD85axR3TlrO7XEfrSnkTDXR1VNe4cN676wKeGNuDLzeXMvKFL/hgRYGO1pXyEhro6hhOh3DH0M7Me2AIafERPDx7Nbe/mUOJjtaV8nga6KpZXWLDmf2TQTw5ridff7eHUS98wXvLdbSulCfTQFfH5XQIt12YyicPDCU9IYJH3l3NlDeWUVx2xO7SlFLN0EBXJ5XaLoxZdw7iV5f1ZEn+Pka/sIjZy3bqaF0pD6OBrtzicAhTBqcy/8Eh9Gwfyc/ez2Xy1GUUHdDRulKeQgNdnZJOMWG8c8f5PD2hFznb9jH6xUW8s3SHjtaV8gAa6OqUORzCzYNSmP/AUDKSonj8gzxufn0pW3YftLs0pfyaBro6bR1jQpl++0Ceubw3K7bvZ+QLi5j8+lK+2FSqI3albKDzoasWsfdQFTOW7GDat9spPVhF17hwpgxO4cp+yYQEOu0uTymfcaL50DXQVYuqrq0nO6+I177ayprCctqEupg0oCM3D+pEYlSI3eUp5fU00NVZZ4whZ/t+Xv9qKwvW7kJEGJuRyK2DU+jXMdru8pTyWicK9ICzXYzyDyJC/5S29E9py859Fbz5zTZmLdvJx6uLOLdjG269MJUxvRIIcOppHKVairw9BMEAABEGSURBVI7Q1VlzqKqW93J2MvWbbWzfW0H7qGBuviCFif070CY00O7ylPIK2nJRHqWu3rBww25e/3or33y3lxCXk6vOS+KWC1LpGhdud3lKeTQNdOWx1heXM/XrrXy4qojq2nqGpcdy6+BUhqS1Q0TsLk8pj6OBrjzenkNVTP92B299u509h6pIiwvn1gtTuaJfEsEuvexRqe9poCuvUVVbx9zVxbz+9VbWFpUTHeri+oEduen8FBKigu0uTynbnXGgi8gY4C+AE3jVGPNsk+eHAn8G+gATjTHvneyYzQV6TU0NBQUFVFbqYgonEhwcTHJyMi6Xy+5SWo0xhqVb9/H611v517oSnCJk9Unk1sGp9O3Qxu7ylLLNGV22KCJO4CVgFFAALBOROcaYdY122wHcAjxyJoUWFBQQERFBSkqK9k+PwxjD3r17KSgoIDU11e5yWo2IMLBzDAM7x7BjbwVvLrYue/xoVRHndYrm1sGpXNIrXi97VKoRd/41DAC2GGPyjTHVwExgQuMdjDHbjDG5QP2ZFFNZWUlMTIyG+QmICDExMX71W0zHmFCeHNeTxY+P4FeX9aT0YBX3zFjBRc//h1cWfUdZRY3dJSrlEdwJ9CRgZ6PHBQ3bTpmI3CkiOSKSU1paerx9TufQfsVf36OIYBdTBqey8JFhvHLTeXRoG8Lv521g0LOf89RHa8gvPWR3iUrZyp07RZtLj9M6k2qMeQV4Bawe+ukcQymnQxjdK4HRvRJYW1TG1K+3MXPpTqYt3s7w9FgmnJPE8PQ4okJ99xyDUs1xJ9ALgA6NHicDRa1Tjv3Cw8M5dEhHet6iV/so/nRNXx4b053pS7YzY8kOFm4sxekQBqS0ZVTPeEb1jKdD21C7S1Wq1bkT6MuANBFJBQqBicD1rVqVUqcoNiKIB0d24/4RaawuOMCn60r4dF0JT89dx9Nz19E9IYJRPeMZ2SOejKQoHA7/bFsp33bSQDfG1IrIvcACrMsWXzfGrBWRp4EcY8wcEekP/BOIBi4Tkd8YY3qdSWG/+Xgt64rKz+QQP9KzfSS/usy9sowx/OxnP+OTTz5BRPjlL3/JddddR3FxMddddx3l5eXU1tby8ssvc8EFF3DbbbeRk5ODiHDrrbfy0EMPtWjtyj0Oh9CvYzT9OkbzszHd2bbnMJ+tt8L9pYVb+N9/byE+MoiLe1gj90GdY/TGJeUz3Jpt0RgzD5jXZNtTjb5fhtWK8RkffPABq1atYvXq1ezZs4f+/fszdOhQZsyYwSWXXMITTzxBXV0dFRUVrFq1isLCQtasWQPAgQMHbK5efS+lXRi3D+nM7UM6s/9wNQs37ubTdSV8uLKQGUt2EBboZGi3WEb1jGd4ehzRYTpJmPJeHjt9rrsj6dby1VdfMWnSJJxOJ/Hx8Vx00UUsW7aM/v37c+utt1JTU8Pll1/OOeecQ+fOncnPz+e+++4jKyuL0aNH21q7al50WCBXnpvMlecmU1lTx+L8vXy6roTP1pXwyZpdOB1CZqfoo333TjFhdpes1CnRuzKO43h30A4dOpRFixaRlJTETTfdxLRp04iOjmb16tUMGzaMl156idtvv/0sV6tOVbDLyfD0OH5/RQbfPn4xH90zmLuHdaHsSA3PZK/nouf/w6gXvuC5+RtYuWM/9fV6UZbyfB47Qrfb0KFD+cc//sHkyZPZt28fixYt4vnnn2f79u0kJSVxxx13cPjwYVasWMHYsWMJDAzkqquuokuXLtxyyy12l69OgcMh9O3Qhr4d2vBfo9PZsbfiaN/9H4vy+dt/viM2IoiRPeIY2SOewV3bad9deSQN9OO44oorWLx4MX379kVEeO6550hISODNN9/k+eefx+VyER4ezrRp0ygsLGTKlCnU11s3yv7hD3+wuXp1JjrGhHLrhancemEqZRU1R/vuH68u5p2lOwlxORmS1o5RPeO5uEc8bbXvrjyER822uH79enr06GFLPd5G36uzr6q2jm/z9/HZuhI+W19CcVklDoHzGvruI3vE0zlWF+hQrUvXFFWqBQQFOLmoWywXdYvl6Qm9WFtUzr8arnf//bwN/H7eBrrEhjGyZzwj0uPo0T6SyGC9W1WdPRroSp0GEaF3UhS9k6J4eFQ3CvZXNIzcd/Pal1v5xxf5ACRGBdM1Lpxu8RGkxYWTFh9BWny4Br1qFRroSrWA5OhQbhmcyi2DUyk7UsPSrfvYvPsgm0sOsXn3QaYv2U5lzQ+TkSZEBpMWH05anBXw3eLD6RoXQVSIBr06fRroSrWwqBDX0WvZv1dXbyjcf4RNJQfZvPsQmxu+zlh6bNDHRwbRLT7i2FF9XIRONKbcooGu1FngdAgdY0LpGBPKyEZBX19vKDzwQ9BvKjnIlt2HmLl0J0dq6o7uFxfRJOjjw+mmQa+a0EBXykYOh9ChbSgd2oZycY8fB/33bZtNJYfYsvsgs3N2UlH9Q9DHRgTRrVHrJi0ugm7x4bQJ1Usp/ZEGulIeqHHQj+h+bNAXlR052pvfVHKIzbsP8W7OTg43Cvp24VbQd4oJo22Yi+jQQOu/xt+HBhIRHKAzT/oQDfQzcKK507dt28a4ceOOTtilVEtwOITk6FCSo0MZ3j3u6HZjDEVllVbLpuTQ0RbOv9bu4sCRGuqOM3WB0yG0CXHRJrQh6MMCiW7yfZuG8G8bZn3fJsSla7l6KM8N9E9+DrvyWvaYCRlw6bMte0ylPICIkNQmhKQ2IQxPjzvmufp6w8GqWvYfrmZ/RTUHKmrY1/j7imoOVFSz/3ANO/dVkFtQzf6KGqprj79EcGRwANFhgbQJDaRtwweAFfyuhg+CY79vGxZIYIB+CLQ2zw10Gzz22GN06tSJu+++G4Bf//rXiAiLFi1i//791NTU8MwzzzBhwoSTHOlYlZWV/PSnPyUnJ4eAgABeeOEFhg8fztq1a5kyZQrV1dXU19fz/vvv0759e6699loKCgqoq6vjySef5LrrrmuNP67yEw6HEBXiIirERQruzSBpjOFITR37Dluhv7+i+pjvrQ8H6/vSQ1VsKjnEgYrqY9o+TcWEBRIXGUxCZBDxkcFH/0uICiIuIpiEqGDahgZqC+gMeG6g2zCSnjhxIg8++ODRQJ89ezbz58/noYceIjIykj179nD++eczfvz4U1qo+aWXXgIgLy+PDRs2MHr0aDZt2sTf//53HnjgAW644Qaqq6upq6tj3rx5tG/fnuzsbADKyspa/g+q1EmICKGBAYQGBpAc7f7rqmrrGoV+w9eKavYcrGZXeSW7yyvZVV5JXmE5ew9X0XTmEZdTiIsIJi4yiIRGoR/f8Dgu0gr+8CDPjS476bvSSL9+/di9ezdFRUWUlpYSHR1NYmIiDz30EIsWLcLhcFBYWEhJSQkJCQluH/err77ivvvuA6B79+506tSJTZs2MWjQIH73u99RUFDAlVdeSVpaGhkZGTzyyCM89thjjBs3jiFDhrTWH1epFhcU4CQ+0kl8ZPBJ962pq6f0YNUPQV9WScnBKkrKKik5aJ0P+GrzHg5W1f7otWGBTuKjgolvGNk39wEQFxHsd20eDfQmrr76at577z127drFxIkTmT59OqWlpSxfvhyXy0VKSgqVlZWndMzjTYB2/fXXM3DgQLKzs7nkkkt49dVXGTFiBMuXL2fevHk8/vjjjB49mqeeeqrZ1yvlzVxOB+3bhNC+TcgJ9ztcVUtJw8h+d7n1AVDS6Ptl2/axu7yK6rof9/xjwgJ/GOFHBRMVEkh4kJOwoADCggIIP/rVSXiQi7Ag59FtLi888auB3sTEiRO544472LNnD1988QWzZ88mLi4Ol8vFwoUL2b59+ykfc+jQoUyfPp0RI0awadMmduzYQXp6Ovn5+XTu3Jn777+f/Px8cnNz6d69O23btuXGG28kPDycN954o+X/kEp5kbCgADrHhp9wJktjDPsrahpG+ZXWKL+8qkmbp4zyI7XNBn9zAgMcDeHuJCywcfgHNAS/q9kPh8YfCt9/DXU5z8q5AQ30Jnr16sXBgwdJSkoiMTGRG264gcsuu4zMzEzOOeccunfvfsrHvPvuu7nrrrvIyMggICCAN954g6CgIGbNmsXbb7+Ny+UiISGBp556imXLlvHoo4/icDhwuVy8/PLLrfCnVMq3iAhtw6yraXoSecJ9q2rrOFxVx+GqWg5V1Tb62mRbtfX1cFXd0W0HKqrZub/i6PbD1bU/Og/QfH0QFtjw4RAUwIMjuzG+b/sW+tM3+jk6H7p30vdKKfvV11tXAzX+UPg+/A9XN/7AaNin0vqgmNi/A0PSYk/rZ+p86Eop1QocDjnacok7+e6tTgP9DOXl5XHTTTcdsy0oKIglS5bYVJFSyl95XKAbY07pGm+7ZWRksGrVqrP6M+1qkymlPJtb1+WIyBgR2SgiW0Tk5808HyQisxqeXyIiKadTTHBwMHv37tXAOgFjDHv37iU4+OTX+Sql/MtJR+gi4gReAkYBBcAyEZljjFnXaLfbgP3GmK4iMhH4I3DK96snJydTUFBAaWnpqb7UrwQHB5OcnGx3GUopD+NOy2UAsMUYkw8gIjOBCUDjQJ8A/Lrh+/eAv4qImFMcartcLlJTU0/lJUoppRq403JJAnY2elzQsK3ZfYwxtUAZENP0QCJyp4jkiEiOjsKVUqpluRPozZ2hbDrydmcfjDGvGGMyjTGZsbGndw2mUkqp5rkT6AVAh0aPk4Gi4+0jIgFAFLCvJQpUSinlHnd66MuANBFJBQqBicD1TfaZA0wGFgNXA/8+Wf98+fLle0Tk1CdGsbQD9pzma32Rvh/H0vfjB/peHMsX3o9Ox3vipIFujKkVkXuBBYATeN0Ys1ZEngZyjDFzgNeAt0RkC9bIfKIbxz3tnouI5Bzv1ld/pO/HsfT9+IG+F8fy9ffDrRuLjDHzgHlNtj3V6PtK4JqWLU0ppdSp8L4Jf5VSSjXLWwP9FbsL8DD6fhxL348f6HtxLJ9+P2ybPlcppVTL8tYRulJKqSY00JVSykd4XaCfbOZHfyEiHURkoYisF5G1IvKA3TV5AhFxishKEZlrdy12E5E2IvKeiGxo+HsyyO6a7CIiDzX8O1kjIu+IiE9OV+pVgd5o5sdLgZ7AJBHpaW9VtqkF/ssY0wM4H7jHj9+Lxh4A1ttdhIf4CzDfGNMd6Iufvi8ikgTcD2QaY3pj3U9z0ntlvJFXBTqNZn40xlQD38/86HeMMcXGmBUN3x/E+sfadNI0vyIiyUAW8KrdtdhNRCKBoVg3/WGMqTbGHLC3KlsFACENU5OE8uPpS3yCtwW6OzM/+p2GBUX6Af6+7t2fgZ8B9XYX4gE6A6XA1IYW1KsiEmZ3UXYwxhQCfwJ2AMVAmTHmX/ZW1Tq8LdDdmtXRn4hIOPA+8KAxptzueuwiIuOA3caY5XbX4iECgHOBl40x/YDDgF+ecxKRaKzf5FOB9kCYiNxob1Wtw9sC3Z2ZH/2GiLiwwny6MeYDu+ux2WBgvIhsw2rFjRCRt+0tyVYFQIEx5vvf2t7DCnh/NBLYaowpNcbUAB8AF9hcU6vwtkA/OvOjiARindiYY3NNthBrJe3XgPXGmBfsrsduxpjHjTHJxpgUrL8X/zbG+OQozB3GmF3AThFJb9h0MceuMuZPdgDni0how7+bi/HRE8RuTc7lKY4386PNZdllMHATkCciqxq2/aJhIjWlAO4DpjcMfvKBKTbXYwtjzBIReQ9YgXV12Ep8dAoAvfVfKaV8hLe1XJRSSh2HBrpSSvkIDXSllPIRGuhKKeUjNNCVUspHaKArpZSP0EBXSikf8f99IfBiG7DN+wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot model loss\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot model accuracies\n",
    "plt.plot(r.history['accuracy'], label='acc')\n",
    "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999995520000001\n"
     ]
    }
   ],
   "source": [
    "#print AUC for train set\n",
    "aucs = []\n",
    "for j in range(6):\n",
    "    auc = roc_auc_score(y_train, prediction_train)\n",
    "    aucs.append(auc)\n",
    "print(np.mean(aucs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9339280992\n"
     ]
    }
   ],
   "source": [
    "#print AUC for test set\n",
    "aucs = []\n",
    "for j in range(6):\n",
    "    auc = roc_auc_score(y_test, prediction_test)\n",
    "    aucs.append(auc)\n",
    "print(np.mean(aucs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init value for that prediction\n",
    "n=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape =  (25000, 300)\n",
      "Shape of x =  (300,)\n",
      "New Shape of x =  (1, 300)\n",
      "\n",
      "\n",
      "Predicted review =  1\n",
      "Actual value =  1\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test shape = \",X_test.shape) #To see shape of X_test\n",
    "\n",
    "x=X_test[n]  #Put test value in x\n",
    "print(\"Shape of x = \",x.shape)\n",
    "#AS you can see it is out of shape when comared to x_test because it is a single value therefore we need to reshape it\n",
    "\n",
    "x2=x.reshape(1,SEQ_LEN) #Reshape to (1,300) so to put in our model if not done this step it will throw error as our model accept this shape only\n",
    "\n",
    "print(\"New Shape of x = \",x2.shape) #Cross check step to see our new shape\n",
    "\n",
    "P=model.predict(x2) #predict x based on our model\n",
    "\n",
    "P2 = 1 if P>0.5 else 0  #Setting threshold if value less than 0.5 it is -ve review otherwise poistive\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Predicted review = \",P2) #predicted review\n",
    "print(\"Actual value = \",y_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> like some other people wrote i'm a die hard mario fan and i loved this game br br this game starts slightly boring but trust me it's worth it as soon as you start your hooked the levels are fun and <UNK> they will hook you <UNK> your mind turns to <UNK> i'm not kidding this game is also <UNK> and is beautifully done br br to keep this spoiler free i have to keep my mouth shut about details but please try this game it'll be worth it br br story 9 9 action 10 1 it's that good <UNK> 10 attention <UNK> 10 average 10 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "#This returns the decoded value by using above decoded function\n",
    "k=decode(X_test[n])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note:\n",
    "The model is better in performance but again it tends to overfit. Lets use LSTM for this task and see what is the performance of that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM WOTHOUT PRE-TRAINED WTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model buliding\n",
    "model2 = Sequential()  #Sequential model\n",
    "model2.add(Embedding(MAX_VOCAB, EMBEDDING_DIM, input_length=SEQ_LEN)) #Init embedding layer with no pretrained wts\n",
    "#embedding layer takes input of vocabulary size i.e 10000, embedding dimension i.e 50 and input sequence i.e number of columns of dataset i.e 300\n",
    "#Here I am not using pretrained glove vector\n",
    "model2.add(Dropout(0.3)) #use dropout for regularization\n",
    "model2.add(LSTM(100,return_sequences=True, dropout=0.3))  #Hidden layer with dropout Here return_sequences=true as I want to get output from all layer than apply global max pool over all the output so that I dont miss out important info\n",
    "model2.add(GlobalMaxPool1D()) #This is to get values from all states and not missing important info\n",
    "model2.add(Dense(1, activation=\"sigmoid\")) #Final dense layer \n",
    "\n",
    "#Note:GlobalMaxPool1D requires output from all lSTMs therfore reurn_sequences = True, this returns output from all LSTM not just final LSTM \n",
    "#ALso GlobalMaxPool1D takes max of all output in very special way this step is done so that every words wether at start or at he end is given importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 300, 50)           500000    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 300, 50)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 300, 100)          60400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 560,501\n",
      "Trainable params: 560,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#To see what our model have, number of layer , output shape ,etc\n",
    "#model summary\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model with optimizer adam, loss as binary cross entropy and metric is accuracy\n",
    "model2.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 102s 4ms/step - loss: 0.6099 - accuracy: 0.6868 - val_loss: 0.3755 - val_accuracy: 0.8482\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 101s 4ms/step - loss: 0.3049 - accuracy: 0.8752 - val_loss: 0.3984 - val_accuracy: 0.8370\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 103s 4ms/step - loss: 0.2254 - accuracy: 0.9136 - val_loss: 0.3031 - val_accuracy: 0.8720\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 102s 4ms/step - loss: 0.1829 - accuracy: 0.9315 - val_loss: 0.3496 - val_accuracy: 0.8636\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 102s 4ms/step - loss: 0.1544 - accuracy: 0.9450 - val_loss: 0.3445 - val_accuracy: 0.8596\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 103s 4ms/step - loss: 0.1336 - accuracy: 0.9532 - val_loss: 0.3732 - val_accuracy: 0.8585\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 103s 4ms/step - loss: 0.1171 - accuracy: 0.9600 - val_loss: 0.3746 - val_accuracy: 0.8587\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 122s 5ms/step - loss: 0.1010 - accuracy: 0.9658 - val_loss: 0.4519 - val_accuracy: 0.8568\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 110s 4ms/step - loss: 0.0853 - accuracy: 0.9704 - val_loss: 0.4761 - val_accuracy: 0.8561\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 103s 4ms/step - loss: 0.0750 - accuracy: 0.9752 - val_loss: 0.4876 - val_accuracy: 0.8523\n"
     ]
    }
   ],
   "source": [
    "#Fitting model to xtrain and ytrain with defined epochs and batch size\n",
    "r = model2.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_data=(X_test,y_test),\n",
    "  verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 31s 1ms/step\n",
      "Test accuracy:  0.8523200154304504\n"
     ]
    }
   ],
   "source": [
    "#Evaluate test set and then print accuracy\n",
    "results = model2.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 35s 1ms/step\n",
      "Test accuracy:  0.9905200004577637\n"
     ]
    }
   ],
   "source": [
    "#Evaluate train set and then print accuracy\n",
    "results = model2.evaluate(X_train, y_train)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE:\n",
    "This LSTM without pretrained wts is also overfitting and results are also not that good lets use LSTM with glove vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM WITH PRE-TRAINED GLOVE VECTOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load glove vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word vectors...\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#load Pretrained glove vector from file\n",
    "#Note the pretrained are of 100 dim long as I am using it from 'glove.6B.100d. This embed each word with 100 dimensions\n",
    "\n",
    "print('Loading word vectors...')\n",
    "word2vec = {} #This dict contain all wrod to vec of golve model\n",
    "with open(os.path.join(os.getcwd()+'\\\\glove.6B.100d.txt'),encoding=\"utf8\") as f: #import data from file\n",
    "    for line in f:\n",
    "        values = line.split() \n",
    "        word2 = values[0]  #Store value in the variable\n",
    "        vec = np.asarray(values[1:], dtype='float32') #store vector of that word to different variable\n",
    "        word2vec[word2] = vec\n",
    "        #print(values)\n",
    "        \n",
    "print('Found %s word vectors.' % len(word2vec)) #print lenth of the dict that stored value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling pre-trained embeddings...\n"
     ]
    }
   ],
   "source": [
    "#this is that embedding from word2vec that are in word2idx in our dataset\n",
    "#This steps store only those vectors that are there in our dictionary word2idx\n",
    "#Only those word's vectors that are there in our vocabulary\n",
    "EMBEDDING_DIM=100 #As we are using GLOVE 100\n",
    "print('Filling pre-trained embeddings...')\n",
    "num_words = min(MAX_VOCAB, len(word2idx) + 1) #Set threshold\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM)) #init array to store embedding\n",
    "for word, i in word2idx.items():\n",
    "    if i < MAX_VOCAB:\n",
    "        embedding_vector = word2vec.get(word) #Get pretrained glove vector from word2vec that are there in our vocalbulary and stroe in some other variable one by one\n",
    "        #print(embedding_vector)\n",
    "        if embedding_vector is not None:\n",
    "      # words not found in embedding index will be all zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "#Note the embedding_matrix is nothing but the pretrained glove vector from file 'glove.6B.100d.txt' but has only those words that are there in our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "(10000, 100)\n"
     ]
    }
   ],
   "source": [
    "#cross check step\n",
    "#To see if everthing is done properly\n",
    "\n",
    "print(num_words) #To see threshold \n",
    "\n",
    "print(embedding_matrix.shape)\n",
    "\n",
    "#AS you can see the output of embedding_matrix which is 10000 in length and 50 in dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Cross check to see a dummy value\n",
    "embedding_matrix[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Building LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model buliding\n",
    "model2 = Sequential()  #Sequential model\n",
    "model2.add(Embedding(MAX_VOCAB, EMBEDDING_DIM, input_length=SEQ_LEN, weights=[embedding_matrix], trainable=True)) #Init embedding layer with no pretrained wts\n",
    "#embedding layer takes input of vocabulary size i.e 10000, embedding dimension i.e 50 and input sequence i.e number of columns of dataset i.e 300\n",
    "#Here I am using pretrained glove vector stored in variable \n",
    "model2.add(Dropout(0.5)) #use dropout for regularization embedding_matrix\n",
    "model2.add(LSTM(100,return_sequences=True, dropout=0.4))  #Hidden layer with dropout Here return_sequences=true as I want to get output from all layer than apply global max pool over all the output so that I dont miss out important info\n",
    "model2.add(GlobalMaxPool1D()) #This is to get values from all states and not missing important info\n",
    "model2.add(Dense(1, activation=\"sigmoid\")) #Final dense layer \n",
    "#Note:GlobalMaxPool1D requires output from all lSTMs therfore reurn_sequences = True, this returns output from all LSTM not just final LSTM \n",
    "#ALso GlobalMaxPool1D takes max of all output in very special way this step is done so that every words wether at start or at he end is given importance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 300, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 300, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 300, 100)          80400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#To see what our model have, number of layer , output shape ,etc\n",
    "#model summary\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compile model with optimizer adam, loss as binary cross entropy and metric is accuracy\n",
    "model2.compile(\n",
    "  loss='binary_crossentropy',\n",
    "  #optimizer=Adam(lr=0.01),  #best\n",
    "  optimizer='adam',\n",
    "  metrics=['acc']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define checkpoint, early stop and reduced lr\n",
    "stop = EarlyStopping(monitor=\"val_loss\", patience=5, mode=\"min\")\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=1e-6, verbose=1, mode=\"min\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 173s 7ms/step - loss: 0.6027 - acc: 0.6603 - val_loss: 0.4449 - val_acc: 0.7908\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 175s 7ms/step - loss: 0.4264 - acc: 0.8055 - val_loss: 0.3397 - val_acc: 0.8537\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 174s 7ms/step - loss: 0.3593 - acc: 0.8464 - val_loss: 0.3648 - val_acc: 0.8523\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 174s 7ms/step - loss: 0.3220 - acc: 0.8634 - val_loss: 0.3266 - val_acc: 0.8714\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 175s 7ms/step - loss: 0.2985 - acc: 0.8749 - val_loss: 0.2887 - val_acc: 0.8834\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 175s 7ms/step - loss: 0.2777 - acc: 0.8850 - val_loss: 0.2818 - val_acc: 0.8842\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 176s 7ms/step - loss: 0.2590 - acc: 0.8927 - val_loss: 0.2766 - val_acc: 0.8894\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 175s 7ms/step - loss: 0.2435 - acc: 0.9019 - val_loss: 0.2689 - val_acc: 0.8927\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 176s 7ms/step - loss: 0.2343 - acc: 0.9030 - val_loss: 0.2709 - val_acc: 0.8946\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 175s 7ms/step - loss: 0.2229 - acc: 0.9114 - val_loss: 0.2657 - val_acc: 0.8956\n"
     ]
    }
   ],
   "source": [
    "#Fitting model to xtrain and ytrain with defined epochs and batch size\n",
    "#Here I have used requlaization and model performance tech such as reduced lr, check point and early stop\n",
    "EPOCHS=10\n",
    "r = model2.fit(\n",
    "  X_train,\n",
    "  y_train,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  epochs=EPOCHS,\n",
    "  validation_data=(X_test,y_test),\n",
    "  callbacks=[reduce_lr, stop],\n",
    "  verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 36s 1ms/step\n",
      "Test accuracy:  0.8956400156021118\n"
     ]
    }
   ],
   "source": [
    "#Evaluate test set and then print accuracy\n",
    "results = model2.evaluate(X_test, y_test)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 36s 1ms/step\n",
      "Test accuracy:  0.9401999711990356\n"
     ]
    }
   ],
   "source": [
    "#Evaluate train set and then print accuracy\n",
    "results = model2.evaluate(X_train, y_train)\n",
    "print('Test accuracy: ', results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting prediction of test set and train set\n",
    "prediction_train2 = model2.predict(X_train)\n",
    "prediction_test2 = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01150385]\n",
      " [0.9953797 ]\n",
      " [0.9725683 ]\n",
      " ...\n",
      " [0.0026525 ]\n",
      " [0.03272676]\n",
      " [0.15806133]]\n"
     ]
    }
   ],
   "source": [
    "#print prediction\n",
    "print(prediction_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set threshold\n",
    "#we want the value to be either 0 or 1\n",
    "pred2=np.zeros((25000,))\n",
    "count=0\n",
    "for i in prediction_test2:\n",
    "    if i>0.5:  #Got .5 after many hit and try\n",
    "        pred2[count]=1\n",
    "    \n",
    "    else:\n",
    "        pred2[count]=0\n",
    "    \n",
    "    count+=1\n",
    "        \n",
    "#threshold is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print value after threshold\n",
    "pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy : 0.8956\n",
      "F1 score : 0.8966\n",
      "precision score : 0.8888\n",
      "recall score : 0.9045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.89     12500\n",
      "           1       0.89      0.90      0.90     12500\n",
      "\n",
      "    accuracy                           0.90     25000\n",
      "   macro avg       0.90      0.90      0.90     25000\n",
      "weighted avg       0.90      0.90      0.90     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#print accuracy score, f1 score, precision, recall and classification report\n",
    "print(\"accuracy : {0:.4f}\".format(accuracy_score(y_test,pred2)))\n",
    "print(\"F1 score : {0:.4f}\".format(f1_score(y_test,pred2)))\n",
    "print(\"precision score : {0:.4f}\".format(precision_score(y_test,pred2)))\n",
    "print(\"recall score : {0:.4f}\".format(recall_score(y_test,pred2)))\n",
    "print(metrics.classification_report(y_test,pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'val_loss': [0.4449035955667496, 0.3396933563947678, 0.3648205891251564, 0.326638181656599, 0.2887175160348415, 0.28176495325565337, 0.2765772679448128, 0.26885333260893823, 0.2709451336562633, 0.26571866223216056], 'val_acc': [0.7908400297164917, 0.8537200093269348, 0.8523200154304504, 0.8713600039482117, 0.8833600282669067, 0.8842399716377258, 0.8893600106239319, 0.8927199840545654, 0.894599974155426, 0.8956400156021118], 'loss': [0.602748440861702, 0.4264164352416992, 0.35930150574445724, 0.322038579583168, 0.29847083616256714, 0.27765644431114195, 0.25904917052388193, 0.24345125898718833, 0.23433869951963424, 0.22287024840712547], 'acc': [0.66032, 0.80552, 0.84636, 0.86336, 0.87488, 0.885, 0.89272, 0.90192, 0.903, 0.9114], 'lr': [0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001]}\n"
     ]
    }
   ],
   "source": [
    "#To see what out model stores info so that these can be plotted\n",
    "print(r.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot model loss\n",
    "plt.plot(r.history['loss'], label='loss')\n",
    "plt.plot(r.history['val_loss'], label='val_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot model accuracies\n",
    "plt.plot(r.history['acc'], label='acc')\n",
    "plt.plot(r.history['val_acc'], label='val_acc')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.984149616\n"
     ]
    }
   ],
   "source": [
    "#print AUC for train set\n",
    "aucs2 = []\n",
    "for j in range(6):\n",
    "    auc2 = roc_auc_score(y_train, prediction_train2)\n",
    "    aucs2.append(auc2)\n",
    "print(np.mean(aucs2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9623555615999999\n"
     ]
    }
   ],
   "source": [
    "#print AUC for test set\n",
    "aucs2 = []\n",
    "for j in range(6):\n",
    "    auc2 = roc_auc_score(y_test, prediction_test2)\n",
    "    aucs2.append(auc2)\n",
    "print(np.mean(aucs2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Init value for that prediction\n",
    "n=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape =  (25000, 300)\n",
      "Shape of x =  (300,)\n",
      "New Shape of x =  (1, 300)\n",
      "\n",
      "\n",
      "Predicted review =  1\n",
      "Actual value =  1\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test shape = \",X_test.shape) #To see shape of X_test\n",
    "\n",
    "x=X_test[n]  #Put test value in x\n",
    "print(\"Shape of x = \",x.shape)\n",
    "#AS you can see it is out of shape when comared to x_test because it is a single value therefore we need to reshape it\n",
    "\n",
    "x2=x.reshape(1,SEQ_LEN) #Reshape to (1,300) so to put in our model if not done this step it will throw error as our model accept this shape only\n",
    "\n",
    "print(\"New Shape of x = \",x2.shape) #Cross check step to see our new shape\n",
    "\n",
    "P=model2.predict(x2) #predict x based on our model\n",
    "\n",
    "P2 = 1 if P>0.5 else 0  #Setting threshold if value less than 0.5 it is -ve review otherwise poistive\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Predicted review = \",P2) #predicted review\n",
    "print(\"Actual value = \",y_test[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> like some other people wrote i'm a die hard mario fan and i loved this game br br this game starts slightly boring but trust me it's worth it as soon as you start your hooked the levels are fun and <UNK> they will hook you <UNK> your mind turns to <UNK> i'm not kidding this game is also <UNK> and is beautifully done br br to keep this spoiler free i have to keep my mouth shut about details but please try this game it'll be worth it br br story 9 9 action 10 1 it's that good <UNK> 10 attention <UNK> 10 average 10 <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "#This returns the decoded value by using above decoded function\n",
    "k=decode(X_test[n])\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:\n",
    "This model of LSTM with wts is best till now it has best performance without overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Igq8Qm8GeCzG"
   },
   "source": [
    "# Retrive the output of each layer in keras for a given single test sample from the trained model you built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT FOR NORMAL SEQUENTIAL MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_2/embedding_lookup/Identity_1:0\", shape=(?, 300, 50), dtype=float32)\n",
      "Tensor(\"flatten_2/Reshape:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"dropout_3/cond/Merge:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"dense_3/BiasAdd:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"dropout_4/cond/Merge:0\", shape=(?, 10), dtype=float32)\n",
      "Tensor(\"dense_4/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Here you can see what all layers are used, shape of each layer an dtype\n",
    "print(model.layers[0].output)\n",
    "print(model.layers[1].output)\n",
    "print(model.layers[2].output)\n",
    "print(model.layers[3].output)\n",
    "print(model.layers[4].output)\n",
    "print(model.layers[5].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape =  (25000, 300)\n",
      "Shape of x =  (300,)\n",
      "New Shape of x =  (1, 300)\n",
      "Predicted review 1\n",
      "Actual review 1\n"
     ]
    }
   ],
   "source": [
    "print(\"X_test shape = \",X_test.shape) #To see shape of X_test\n",
    "\n",
    "x=X_test[n]  #Put test value in x\n",
    "print(\"Shape of x = \",x.shape)\n",
    "#AS you can see it is out of shape when comared to x_test because it is a single value therefore we need to reshape it\n",
    "\n",
    "x2=x.reshape(1,SEQ_LEN) #Reshape to (1,300) so to put in our model if not done this step it will throw error as our model accept this shape only\n",
    "\n",
    "print(\"New Shape of x = \",x2.shape) #Cross check step to see our new shape\n",
    "\n",
    "P=model.predict(x2) #predict x based on our model\n",
    "\n",
    "P2 = 1 if P>0.5 else 0  #Setting threshold if value less than 0.5 it is -ve review otherwise poistive\n",
    "print(\"Predicted review\",P2) #predicted review\n",
    "print(\"Actual review\",y_test[n]) #To compare pridicted result with actual result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 300, 50)           500000    \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 15000)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                150010    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 650,021\n",
      "Trainable params: 650,021\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#use this step to cross check output shape\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here you can see I made use of keras backend to initliaze, It returns the output of each layer when invoked \n",
    "#As the model is sequential I have to provide value to first layer and it will give output till final layer \n",
    "#Here inside function you can see model.layer which are model layer followed by index which mean that particular layer, followed by the input which is the input to that layer or out is we will get output from that layer\n",
    "#This is done for all layers that are in my model\n",
    "get_embed_out0 = keras.backend.function(\n",
    "    [model.layers[0].input],\n",
    "    [model.layers[0].output])\n",
    "\n",
    "get_embed_out1 = keras.backend.function(\n",
    "    [model.layers[0].input],\n",
    "    [model.layers[1].output])\n",
    "\n",
    "get_embed_out2 = keras.backend.function(\n",
    "    [model.layers[0].input],\n",
    "    [model.layers[2].output])\n",
    "\n",
    "get_embed_out3 = keras.backend.function(\n",
    "    [model.layers[0].input],\n",
    "    [model.layers[3].output])\n",
    "\n",
    "get_embed_out4 = keras.backend.function(\n",
    "    [model.layers[0].input],\n",
    "    [model.layers[4].output])\n",
    "\n",
    "get_embed_out5 = keras.backend.function(\n",
    "    [model.layers[0].input],\n",
    "    [model.layers[5].output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for layer 1st i.e Flatten layer =    <class 'list'> 1 (1, 300, 50)\n",
      "for layer 2nd i.e Flatten layer =    <class 'list'> 1 (1, 15000)\n",
      "for layer 3rd i.e 1st Dropout layer =    <class 'list'> 1 (1, 15000)\n",
      "for layer 4th i.e Dense layer =    <class 'list'> 1 (1, 10)\n",
      "for layer 5th i.e 2nd Dropout layer =    <class 'list'> 1 (1, 10)\n",
      "for layer 6th i.e dense layer =    <class 'list'> 1 (1, 1)\n"
     ]
    }
   ],
   "source": [
    "#Shape, type and length of each layer from embedding till dense final layer\n",
    "#Here input is the x2(reshaped of x)\n",
    "#This step gives us important details about the what shape each layers gives out\n",
    "layer_output0 = get_embed_out0([x2]) #Init our input to above function and creating object of it similarly for rest\n",
    "layer_output1 = get_embed_out1([x2])\n",
    "layer_output2 = get_embed_out2([x2])\n",
    "layer_output3 = get_embed_out3([x2])\n",
    "layer_output4 = get_embed_out4([x2])\n",
    "layer_output5 = get_embed_out5([x2])\n",
    "\n",
    "print(\"for layer 1st i.e Flatten layer =   \", type(layer_output0), len(layer_output0), layer_output0[0].shape)\n",
    "print(\"for layer 2nd i.e Flatten layer =   \", type(layer_output1), len(layer_output1), layer_output1[0].shape)\n",
    "print(\"for layer 3rd i.e 1st Dropout layer =   \",type(layer_output2), len(layer_output2), layer_output2[0].shape)\n",
    "print(\"for layer 4th i.e Dense layer =   \",type(layer_output3), len(layer_output3), layer_output3[0].shape)\n",
    "print(\"for layer 5th i.e 2nd Dropout layer =   \",type(layer_output4), len(layer_output4), layer_output4[0].shape)\n",
    "print(\"for layer 6th i.e dense layer =   \",type(layer_output5), len(layer_output5), layer_output5[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For layer 1 i.e Embedding layer\n",
      "[array([[[ 5.66664312e-05, -1.79680102e-02,  8.28240719e-03, ...,\n",
      "          3.40049528e-03, -2.71884049e-03,  4.18313732e-03],\n",
      "        [-7.95939714e-02,  1.58535223e-02, -1.65786128e-02, ...,\n",
      "          7.43049458e-02, -2.50634663e-02,  4.92246076e-02],\n",
      "        [-1.31525407e-02, -1.58749130e-02,  4.36166413e-02, ...,\n",
      "          1.32223743e-03, -1.60870922e-03, -2.37565897e-02],\n",
      "        ...,\n",
      "        [ 6.48019984e-02,  6.91158175e-02, -4.78430912e-02, ...,\n",
      "          6.13603331e-02,  3.37494463e-02,  1.21757602e-02],\n",
      "        [-2.71052518e-03,  2.82813944e-02, -4.88941781e-02, ...,\n",
      "         -1.83668882e-02,  1.02251165e-01, -3.11347526e-02],\n",
      "        [-7.95939714e-02,  1.58535223e-02, -1.65786128e-02, ...,\n",
      "          7.43049458e-02, -2.50634663e-02,  4.92246076e-02]]],\n",
      "      dtype=float32)]\n",
      "\n",
      "\n",
      " For layer 2 i.e Flatten layer\n",
      "[array([[ 5.6666431e-05, -1.7968010e-02,  8.2824072e-03, ...,\n",
      "         7.4304946e-02, -2.5063466e-02,  4.9224608e-02]], dtype=float32)]\n",
      "\n",
      "\n",
      " For layer 3 i.e 1st Dropout layer\n",
      "[array([[ 5.6666431e-05, -1.7968010e-02,  8.2824072e-03, ...,\n",
      "         7.4304946e-02, -2.5063466e-02,  4.9224608e-02]], dtype=float32)]\n",
      "\n",
      "\n",
      " For layer 4 i.e Dense layer\n",
      "[array([[ 1.4233767, -1.454182 , -1.2959054,  1.4426725, -1.083765 ,\n",
      "         1.1942524,  1.4471122,  1.2649356,  0.784304 , -1.1838924]],\n",
      "      dtype=float32)]\n",
      "\n",
      "\n",
      " For layer 5 i.e 2nd Dropout layer\n",
      "[array([[ 1.4233767, -1.454182 , -1.2959054,  1.4426725, -1.083765 ,\n",
      "         1.1942524,  1.4471122,  1.2649356,  0.784304 , -1.1838924]],\n",
      "      dtype=float32)]\n",
      "\n",
      "\n",
      " For layer 6 i.e dense layer\n",
      "[array([[0.99999976]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Output for all layers from embedding till dense final layer\n",
    "#Here I have show each layer output based on our input value x2(reshaped of x)\n",
    "print(\"For layer 1 i.e Embedding layer\")\n",
    "print(layer_output0)\n",
    "\n",
    "print(\"\\n\\n For layer 2 i.e Flatten layer\")\n",
    "print(layer_output1)\n",
    "\n",
    "print(\"\\n\\n For layer 3 i.e 1st Dropout layer\")\n",
    "print(layer_output2)\n",
    "\n",
    "print(\"\\n\\n For layer 4 i.e Dense layer\")\n",
    "print(layer_output3)\n",
    "\n",
    "print(\"\\n\\n For layer 5 i.e 2nd Dropout layer\")\n",
    "print(layer_output4)\n",
    "\n",
    "print(\"\\n\\n For layer 6 i.e dense layer\")\n",
    "print(layer_output5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-dUDSg7VeCzM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OUTPUT FOR LSTM MODEL (with pretrained weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"embedding_5/embedding_lookup/Identity_1:0\", shape=(?, 300, 100), dtype=float32)\n",
      "Tensor(\"dropout_7/cond/Merge:0\", shape=(?, 300, 100), dtype=float32)\n",
      "Tensor(\"lstm_3/transpose_1:0\", shape=(?, 300, 100), dtype=float32)\n",
      "Tensor(\"global_max_pooling1d_3/Max:0\", shape=(?, 100), dtype=float32)\n",
      "Tensor(\"dense_7/Sigmoid:0\", shape=(?, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#Here you can see what all layers are used, shape of each layer an dtype\n",
    "print(model2.layers[0].output)\n",
    "print(model2.layers[1].output)\n",
    "print(model2.layers[2].output)\n",
    "print(model2.layers[3].output)\n",
    "print(model2.layers[4].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test shape =  (25000, 300)\n",
      "Shape of x =  (300,)\n",
      "New Shape of x =  (1, 300)\n",
      "Predicted review 1\n",
      "Actual review 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"X_test shape = \",X_test.shape) #To see shape of X_test\n",
    "\n",
    "x=X_test[n]  #Put test value in x\n",
    "print(\"Shape of x = \",x.shape)\n",
    "#AS you can see it is out of shape when comared to x_test because it is a single value therefore we need to reshape it\n",
    "\n",
    "x2=x.reshape(1,SEQ_LEN) #Reshape to (1,300) so to put in our model if not done this step it will throw error as our model accept this shape only\n",
    "\n",
    "print(\"New Shape of x = \",x2.shape) #Cross check step to see our new shape\n",
    "\n",
    "P=model2.predict(x2) #predict x based on our model\n",
    "\n",
    "P2 = 1 if P>0.5 else 0  #Setting threshold if value less than 0.5 it is -ve review otherwise poistive\n",
    "print(\"Predicted review\",P2) #predicted review\n",
    "print(\"Actual review\",y_test[n]) #To compare pridicted result with actual result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 300, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 300, 100)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 300, 100)          80400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 1,080,501\n",
      "Trainable params: 1,080,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#use this step to cross check output shape \n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here you can see I made use of keras backend to initliaze, It returns the output of each layer when invoked \n",
    "#As the model is sequential I have to provide value to first layer and it will give output till final layer \n",
    "#Here inside function you can see model.layer which are model layer followed by index which mean that particular layer, followed by the input which is the input to that layer or out is we will get output from that layer\n",
    "#This is done for all layers that are in my model\n",
    "get_embed_out0 = keras.backend.function(\n",
    "    [model2.layers[0].input],\n",
    "    [model2.layers[0].output])\n",
    "\n",
    "get_embed_out1 = keras.backend.function(\n",
    "    [model2.layers[0].input],\n",
    "    [model2.layers[1].output])\n",
    "\n",
    "get_embed_out2 = keras.backend.function(\n",
    "    [model2.layers[0].input],\n",
    "    [model2.layers[2].output])\n",
    "\n",
    "get_embed_out3 = keras.backend.function(\n",
    "    [model2.layers[0].input],\n",
    "    [model2.layers[3].output])\n",
    "\n",
    "get_embed_out4 = keras.backend.function(\n",
    "    [model2.layers[0].input],\n",
    "    [model2.layers[4].output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for layer 1st i.e Embedding layer =    <class 'list'> 1 (1, 300, 100)\n",
      "for layer 2nd i.e Dropout layer =    <class 'list'> 1 (1, 300, 100)\n",
      "for layer 3rd i.e LSTM layer =    <class 'list'> 1 (1, 300, 100)\n",
      "for layer 4th i.e global_max_pooling1d =    <class 'list'> 1 (1, 100)\n",
      "for layer 5th i.e Dense layer =    <class 'list'> 1 (1, 1)\n"
     ]
    }
   ],
   "source": [
    "#Shape, type and length of each layer from embedding till dense final layer\n",
    "#Here input is the x2(reshaped of x)\n",
    "#This step gives us important details about the what shape each layers gives out\n",
    "layer_output0 = get_embed_out0([x2]) #Init our input to above function and creating object of it similarly for rest \n",
    "layer_output1 = get_embed_out1([x2])\n",
    "layer_output2 = get_embed_out2([x2])\n",
    "layer_output3 = get_embed_out3([x2])\n",
    "layer_output4 = get_embed_out4([x2])\n",
    "\n",
    "\n",
    "print(\"for layer 1st i.e Embedding layer =   \", type(layer_output0), len(layer_output0), layer_output0[0].shape)\n",
    "print(\"for layer 2nd i.e Dropout layer =   \", type(layer_output1), len(layer_output1), layer_output1[0].shape)\n",
    "print(\"for layer 3rd i.e LSTM layer =   \",type(layer_output2), len(layer_output2), layer_output2[0].shape)\n",
    "print(\"for layer 4th i.e global_max_pooling1d =   \",type(layer_output3), len(layer_output3), layer_output3[0].shape)\n",
    "print(\"for layer 5th i.e Dense layer =   \",type(layer_output4), len(layer_output4), layer_output4[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For layer 1 i.e Embedding layer\n",
      "[array([[[ 0.04729347,  0.04208159,  0.04265385, ...,  0.06871387,\n",
      "          0.05492609, -0.00377948],\n",
      "        [-0.30647343,  0.78726465, -0.20184797, ..., -0.06453719,\n",
      "          0.58388066,  0.5944857 ],\n",
      "        [ 0.4920982 ,  0.21363993, -0.03415488, ..., -0.521059  ,\n",
      "          0.84319484,  0.11655941],\n",
      "        ...,\n",
      "        [-0.18036468,  0.071647  ,  0.197794  , ..., -0.5258234 ,\n",
      "          0.648752  ,  0.05700096],\n",
      "        [-0.42847362,  0.41960958,  0.5360627 , ..., -0.20482133,\n",
      "         -0.10294092,  0.2667207 ],\n",
      "        [-0.30647343,  0.78726465, -0.20184797, ..., -0.06453719,\n",
      "          0.58388066,  0.5944857 ]]], dtype=float32)]\n",
      "\n",
      "\n",
      " For layer 2 i.e Dropout layer\n",
      "[array([[[ 0.04729347,  0.04208159,  0.04265385, ...,  0.06871387,\n",
      "          0.05492609, -0.00377948],\n",
      "        [-0.30647343,  0.78726465, -0.20184797, ..., -0.06453719,\n",
      "          0.58388066,  0.5944857 ],\n",
      "        [ 0.4920982 ,  0.21363993, -0.03415488, ..., -0.521059  ,\n",
      "          0.84319484,  0.11655941],\n",
      "        ...,\n",
      "        [-0.18036468,  0.071647  ,  0.197794  , ..., -0.5258234 ,\n",
      "          0.648752  ,  0.05700096],\n",
      "        [-0.42847362,  0.41960958,  0.5360627 , ..., -0.20482133,\n",
      "         -0.10294092,  0.2667207 ],\n",
      "        [-0.30647343,  0.78726465, -0.20184797, ..., -0.06453719,\n",
      "          0.58388066,  0.5944857 ]]], dtype=float32)]\n",
      "\n",
      "\n",
      " For layer 3 i.e LSTM layer\n",
      "[array([[[ 0.00058593, -0.04326791, -0.00521626, ..., -0.02140813,\n",
      "         -0.00674273, -0.01120373],\n",
      "        [ 0.00579395, -0.05400041, -0.17517124, ..., -0.08464328,\n",
      "         -0.08379809,  0.0207662 ],\n",
      "        [ 0.13990936, -0.07573938, -0.2226643 , ..., -0.1520586 ,\n",
      "          0.16098736,  0.0728052 ],\n",
      "        ...,\n",
      "        [-0.06268351, -0.03837064, -0.29780003, ..., -0.07628812,\n",
      "         -0.2502994 , -0.06532285],\n",
      "        [-0.04537094, -0.03457071, -0.13634764, ..., -0.05128488,\n",
      "         -0.18292272, -0.04940049],\n",
      "        [-0.05200253, -0.05093808, -0.28842887, ..., -0.13301402,\n",
      "         -0.23665042, -0.02541872]]], dtype=float32)]\n",
      "\n",
      "\n",
      " For layer 4 i.e global_max_pooling1d\n",
      "[array([[ 0.41327313,  0.03306616, -0.00521626,  0.15293595,  0.02784829,\n",
      "         0.08256047,  0.5155799 , -0.02874551,  0.15717655, -0.02785311,\n",
      "         0.01718166,  0.22874159,  0.30958277,  0.63073975,  0.4774018 ,\n",
      "         0.5875051 ,  0.47188714,  0.609735  ,  0.00229769, -0.01533988,\n",
      "        -0.01855648, -0.02612569, -0.02534891,  0.49740484,  0.02291668,\n",
      "         0.46703544,  0.35225967, -0.02427171,  0.1638305 ,  0.00984725,\n",
      "         0.28630912,  0.076772  ,  0.7755257 ,  0.2206463 ,  0.16526225,\n",
      "         0.0246842 ,  0.05382042,  0.4097593 ,  0.15773004,  0.02800423,\n",
      "         0.34634495,  0.7284284 ,  0.44941214,  0.86647594,  0.00835111,\n",
      "         0.19448186,  0.47351173,  0.1584568 ,  0.07903234,  0.12237645,\n",
      "         0.5128136 ,  0.1948194 ,  0.6489978 ,  0.63034415,  0.38938418,\n",
      "         0.01978613,  0.38939694,  0.41222757,  0.15158711,  0.47917774,\n",
      "         0.55224514, -0.00558464, -0.0038351 ,  0.01778492,  0.6903438 ,\n",
      "         0.06080435,  0.6128018 ,  0.33289215,  0.53571373, -0.01980456,\n",
      "         0.00205751, -0.03734275,  0.14059062,  0.5400655 ,  0.04458598,\n",
      "         0.09469576,  0.4717455 ,  0.5476319 ,  0.6835195 ,  0.84632736,\n",
      "         0.1065859 ,  0.06415015,  0.33098432, -0.02998896,  0.0112371 ,\n",
      "         0.22641869,  0.02402317,  0.2876454 ,  0.8096525 ,  0.45610118,\n",
      "         0.0177958 , -0.01430814,  0.05492949,  0.7739554 ,  0.76029444,\n",
      "         0.59583926, -0.0079925 ,  0.28649294,  0.43417087,  0.0728052 ]],\n",
      "      dtype=float32)]\n",
      "\n",
      "\n",
      " For layer 5 i.e Dense layer\n",
      "[array([[0.97256833]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "#Output for all layers from embedding till dense final layer\n",
    "#Here I have show each layer output based on our input value x2(reshaped of x)\n",
    "print(\"For layer 1 i.e Embedding layer\")\n",
    "print(layer_output0)\n",
    "\n",
    "print(\"\\n\\n For layer 2 i.e Dropout layer\")\n",
    "print(layer_output1)\n",
    "\n",
    "print(\"\\n\\n For layer 3 i.e LSTM layer\")\n",
    "print(layer_output2)\n",
    "\n",
    "print(\"\\n\\n For layer 4 i.e global_max_pooling1d\")\n",
    "print(layer_output3)\n",
    "\n",
    "print(\"\\n\\n For layer 5 i.e Dense layer\")\n",
    "print(layer_output4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "\n",
    "I have performed various important steps in this project starting from EDA which helped me to know my dataset proper than I Done embedding step which include various sub step like creating different dictionary eg word2idx and idx2word also in in this I have created a function which maps the sequences back to word. Then next step comes is Model biluding step here I have created various model strating from normal Neural network dense model with sequence length of 20 then cearted sam emodel with 300 sequence length then performed grid search on that and prepared final model with improved hyperparameter with 300 sequence length but again the model was overfitting, Then I decided to make LSTM model with and withour pretrained word embeddings, the one with pretrained wts performed very well and I have also printed its results and predictions alsong with performance graph. At the end I have shown each layers output and output shape for lstm with pretrained wts and normal dense neural network with improved hyyperparameter. Also now I would finally conclude in production I would use LSTM with pretrained wts because it was giving great results without overfitting   "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "SeqNLP_Project1_Questions.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
